{"path":"papers/empowerment/papers/2024 Growing Q-Networks.pdf","text":"Proceedings of Machine Learning Research vol vvv:1–15, 2024 6th Annual Conference on Learning for Dynamics and Control Growing Q-Networks: Solving Continuous Control Tasks with Adaptive Control Resolution Tim Seyde TSEYDE@MIT.EDU MIT CSAIL Peter Werner WERNERPE@MIT.EDU MIT CSAIL Wilko Schwarting WILKO@ISEE.AI ISEE AI Markus Wulfmeier* MWULFMEIER@GOOGLE.COM Google DeepMind Daniela Rus* RUS@CSAIL.MIT.EDU MIT CSAIL Editors: A. Abate, K. Margellos, A. Papachristodoulou Abstract Recent reinforcement learning approaches have shown surprisingly strong capabilities of bang- bang policies for solving continuous control benchmarks. The underlying coarse action space discretizations often yield favourable exploration characteristics while final performance does not visibly suffer in the absence of action penalization in line with optimal control theory. In robotics applications, smooth control signals are commonly preferred to reduce system wear and energy efficiency, but action costs can be detrimental to exploration during early training. In this work, we aim to bridge this performance gap by growing discrete action spaces from coarse to fine con- trol resolution, taking advantage of recent results in decoupled Q-learning to scale our approach to high-dimensional action spaces up to dim(A) = 38. Our work indicates that an adaptive control resolution in combination with value decomposition yields simple critic-only algorithms that yield surprisingly strong performance on continuous control tasks. Keywords: Continuous Control; Q-learning; Value Decomposition; Growing resolution 1. Introduction Reinforcement learning for continuous control applications commonly leverages policies parame- terized via continuous distributions. Recent works have shown surprisingly strong performance of discrete policies both in the actor-critic and critic-only setting (Tang and Agrawal, 2020; Tavakoli et al., 2021; Seyde et al., 2021). While discrete critic-only methods promise simpler controller designs than their continuous actor-critic counterparts, applications such as robot control tend to favor smooth control signals to maintain stability and prevent system wear (Hodel, 2018). It has previously been noted that coarse action discretization can provide exploration benefits early during training (Czarnecki et al., 2018; Farquhar et al., 2020), while converged policies should increasingly prioritize controller smoothness (Bohez et al., 2019). Our work aims to bridge the gap between these two objectives while maintaining algorithm simplicity. We introduce Growing Q-Networks (GQN), a simple discrete critic-only agent that © 2024 T. Seyde, P. Werner, W. Schwarting, M. Wulfmeier* & D. Rus*.arXiv:2404.04253v1 [cs.LG] 5 Apr 2024 SEYDE WERNER SCHWARTING WULFMEIER* RUS* combines the scalability benefits of fully decoupled Q-learning (Seyde et al., 2022b) with the ex- ploration benefits of dynamic control resolution (Czarnecki et al., 2018; Farquhar et al., 2020). Introducing an adaptive action masking mechanism into a value-decomposed Q-Network, the agent can autonomously decide when to increase control resolution. This approach enhances learning effi- ciency and balances the exploration-exploitation trade-off more effectively, improving convergence speed and solution smoothness. The primary contributions of this paper are threefold: • Framework for adaptive control resolution: we adaptively grow control resolution from coarse to fine within decoupled Q-learning. This reconciles coarse exploration during early training with smooth control at convergence, while retaining scalability of decoupled control. • Insights into scalability of discretized control: our research provides valuable insights into overcoming exploration challenges in soft-contrained continuous control settings via simple discrete Q-learning methods, studying applicability in challenging control scenarios. • Comprehensive experimental validation: we validate the effectiveness of our GQN algo- rithm on a diverse set of continuous control tasks, highlighting benefits of adaptive control resolution over static DQN variations as well as recent continuous actor-critic methods. The remainder of the paper is organized as follows: Section 2 reviews related work, Section 3 introduces preliminaries, Section 4 details the proposed GQN methodology, Section 4 presents experimental results, and Section 5 concludes with a discussion on future research directions. 2. Related Works In the following, we discuss several key related works grouped by their primary research thrust. Discretized Control Learning continuous control tasks typically relies on policies with con- tinuous support, primarily Gaussians with diagonal covariance matrices (Schulman et al., 2017; Haarnoja et al., 2018; Abdolmaleki et al., 2018a; Hafner et al., 2020; Wulfmeier et al., 2020). Recent works have shown that competitive performance is often attainable via discrete policies (Tavakoli et al., 2018; Neunert et al., 2020; Tang and Agrawal, 2020; Seyde et al., 2022a) with bang-bang control at the extreme (Seyde et al., 2021). Bang-bang controllers have been extensively investi- gated in optimal control research (Sonneborn and Van Vleck, 1964; Bellman et al., 1956; LaSalle, 1959; Maurer et al., 2005) as well as early works in reinforcement learning (Waltz and Fu, 1965; Lambert and Levine, 1970; Anderson, 1988), while the extreme switching behavior was often ob- served to naturally emerge even under continuous policy distributions (Huang et al., 2019; Novati and Koumoutsakos, 2019; Thuruthel et al., 2019). The direct application of discrete action-space algorithms then harbors potential benefits for reducing model complexity (Metz et al., 2017; Sharma et al., 2017; Tavakoli, 2021; Watkins and Dayan, 1992), although control resolution trade-offs and scalability may require computational overhead (Van de Wiele et al., 2020). Scalability Scalability of Q-learning based approaches has been studied extensively in the context of mitigating coordination challenges and system non-stationarity (Tan, 1993; Claus and Boutilier, 1998; Matignon et al., 2012; Lauer and Riedmiller, 2000; Matignon et al., 2007; Foerster et al., 2017; Busoniu et al., 2006; B¨ohmer et al., 2019). Exponential coupling can be avoided by information- sharing (Schneider et al., 1999; Russell and Zimdars, 2003; Yang et al., 2018), composition of local utility functions (Sunehag et al., 2017; Rashid et al., 2018; Son et al., 2019; Wang et al., 2020; Su 2 GROWING Q-NETWORKS et al., 2021; Peng et al., 2021), and considering different levels of interaction (Guestrin et al., 2002; Kok and Vlassis, 2006). Centralization can further be facilitated via high degrees of parameter- sharing (Gupta et al., 2017; B¨ohmer et al., 2020; Christianos et al., 2021; Van Seijen et al., 2017; Chu and Ye, 2017)). Decoupled control via Q-learning was proposed for Atari (Sharma et al., 2017) and extended to mixing across higher-order action subspaces (Tavakoli et al., 2021), with decoupled bang-bang control displaying strong performance on continuous control tasks (Seyde et al., 2022b). While coarse discretization can be beneficial for exploration, particularly in the presence of action penalties, they may also reduce steady-state performance. Conversely, fine discretization can exac- erbate coordination challenges (Seyde et al., 2022b; Ireland and Montana, 2024). Here, we consider adapting the control resolution over the course of training to achieve the best of both worlds. Expanding Action Spaces Smith et al. (2023) presents an adaptive policy regularization approach that introduces soft constraints on feasible action regions, growing continuous regions linearly over the course of training with adjustments based on dynamics uncertainty. They focus on learning quadrupedal locomotion on hardware and expand locally around joint angles of a stable initial pose. In discrete action spaces, one can instead leverage iterative resolution refinement. Czarnecki et al. (2018) considers DeepMind Lab navigation tasks (Beattie et al., 2016) with a natively discrete action space that avoids reasoning about system dynamics stability. Their policy-based method formulates a mixture policy that is optimized under a distillation objective to facilitate knowledge transfer, adjusting the mixing weights via Population Based Training (PBT) (Jaderberg et al., 2017). Similarly, Synnaeve et al. (2019) considers multi-agent coordination in StarCraft and adjusts spatial command resolution via PBT. Farquhar et al. (2020) grow action resolution under a linear growth schedule, while showing limited application to simple continuous control tasks, as they enumerate the action space and do not consider decoupled optimization. Beyond control applications, Yang et al. (2023) demonstrate adaptive mesh refinement strategies that reduce the errors in finite element simulations. Their refinement policy recursively adds finer elements, expanding the action space. Constrained Optimization Reward-optimal bang-bang policies may not be desirable for real- world applications as they can be less energy efficient and increase wear and tear on physical sys- tems, e.g. Hodel (2018). In the past, this behavior was generally avoided by employing penalty functions as soft constraints at the cost of potentially hindering exploration or enabling reward hack- ing Skalse et al. (2022). The rewards and costs are automatically re-balanced to combat this issue in Bohez et al. (2019). Similarly, undesirable behaviors are avoided by automatically balancing soft chance constraints with the primary rewards in Roy et al. (2021). Here, we do not assume access to explicit penalty terms and efficiently learn controllers directly based on environment reward. 3. Preliminaries We formulate the learning control problem as a Markov Decision Process (MDP) described by the tuple {S, A, T , R, γ}, where S ⊂ RN and A ⊂ RM denote the state and action space, respectively, T : S × A → S the transition distribution, R : S × A → R the reward function, and γ ∈ [0, 1) the discount factor. Let st and at denote the state and action at time t, where actions are sampled from policy π(at|st). We define the discounted infinite horizon return as Gt = ∑∞ τ =t γτ −tR(sτ , aτ ), where st+1 ∼ T (·|st, at) and at ∼ π(·|st). Our objective is to learn the optimal policy that max- imizes the expected infinite horizon return E[Gt] under unknown dynamics and reward mappings. Conventional algorithms for continuous control settings leverage actor-critic designs with a con- 3 SEYDE WERNER SCHWARTING WULFMEIER* RUS* tinuous policy πϕ(at|st) maximizing expected returns from a value estimator Qθ(st, at) or Vθ(st). Recent studies have shown strong results with simpler methods employing discretized actors (Tang and Agrawal, 2020; Seyde et al., 2021) or critic-only formulations (Tavakoli et al., 2018, 2021; Seyde et al., 2022b). Here, we focus on the light-weight critic-only setting and increase control resolution over the course of training to bridge the gap between discrete and continuous control. 3.1. Deep Q-Networks We consider the general framework of Deep Q-Networks (DQN) (Mnih et al., 2013), where the state action value function Qθ(st, at) is represented by a neural network with parameters θ. The parameters are updated to minimize the temporal-difference (TD) error, where we leverage sev- eral performance enhancements based on the Rainbow agent (Hessel et al., 2018). These include target networks to improve stability in combination with double Q-learning to mitigate overesti- mation (Mnih et al., 2015; Van Hasselt et al., 2016), prioritized experience replay (PER) to focus sampling on more informative transitions (Schaul et al., 2015), and multi-step returns to improve stability of Bellman backups (Sutton and Barto, 2018). The resulting objective function is given by L(θ) = B∑ b=1 Lδ(yt − Qθ(st, at)), (1) where action evaluation employs the target yt = ∑n−1 j=0 γjr(st+j, at+j) + γnQθ−(st+n, a∗ t+n), action selection uses a∗ t+1 = arg maxa Qθ(st+1, a), Lδ(·) is the Huber loss and the batch size is B. 3.2. Decoupled Q-Networks Traditional DQN-based agents enumerate the entire action space and do therefore not scale well to high dimensional control problems. Decoupled representations address scalability issues by treat- ing subsets of action dimensions as separate agents and coordinating joint behavior in expecta- tion (Sharma et al., 2017; Sunehag et al., 2017; Rashid et al., 2018; Tavakoli et al., 2021; Seyde et al., 2022b). The Decoupled Q-Networks (DecQN) agent introduced in Seyde et al. (2022b) em- ploys a full decomposition with the critic predicting univariate utilities for each action dimension aj conditioned on the global state s. The corresponding state-action value function is recovered as Qθ(st, at) = M∑ j=1 Qj θ(st, a j t ) M , (2) where the objective is analogous to Eq. 1, enabling centralized training with decentralized execution. 4. Growing Q-Networks Discrete control algorithms have demonstrated competitive performance on continuous control bench- marks (Tang and Agrawal, 2020; Tavakoli et al., 2018; Seyde et al., 2021). One potential benefit of these methods is the intrinsic coarse exploration that can accelerate the generation of informative environment feedback. In robot control, we typically prefer smooth controllers at convergence to limit hardware stress. Our objective is to bridge the gap between coarse exploration capabilities and smooth control performance while retaining sample-efficient learning. We leverage insights from 4 GROWING Q-NETWORKS Figure 1: Schematic of a GQN agent with decoupled 5-bin discretization and 3-bin active subspace. The available actions are highlighted in green while the masked actions are depicted in gray. The predicted state-action values Q(s, a0, ..., aM ) are computed via linear compo- sition of the univariate utilities Q(s, aj) by selecting one action per dimension (red). the growing action space literature (Czarnecki et al., 2018; Farquhar et al., 2020) and consider a decoupled critic that increases its control resolution over the course of training. To this end, we define the discrete action sub-space at iteration g as Ag ⊂ A and modify the TD target to yield yt = n−1∑ j=0 γjr(st+j, at+j) + γn M∑ j=1 max a j t+1∈Ag Q j θ−(st+n, a j t+n) M , (3) where ϵ-greedy action sampling is analogously constrained to Ag. The network architecture ac- commodates the full discretized action space from the start and constrains the active set via action masking, enabling masked action combinations to still profit from information propagation in the shared torso (Van Seijen et al., 2017). A schematic depiction of a decoupled agent with 5-bin dis- cretization and active 3-bin subspace is provided in Figure 1. In order to deploy such an agent we require a schedule for when to expand the active action space Ag → Ag+1. Here, we consider two simple variations to limit engineering effort. First, we consider a linear schedule that doubles con- trol resolution every 1 N +1 of total training episodes, where N indicates the number of subspaces Ag. Second, we formulate an adaptive schedule based on an upper confidence bound inspired threshold over the moving average returns Gthreshold,t = (1.00 − 0.05 sgn µ G MA,t−1)µ G MA,t−1 + 0.90σG MA,t−1, (4) where µMA and σMA are the moving average mean and standard deviation of the evaluation returns, respectively. The objective underestimates the mean by 5% and expands the action space whenever the current mean return falls below the threshold µG t < Gthreshold,t, signifying performance stagna- tion. This parameterization can avoid pre-mature expansion when exploring under sparse rewards, but alternative formulations are also applicable. A qualitative example of our approach is provided in Figure 2, where we visualize the state-action value function over the course of training on a pen- dulum swing-up task. We consider a GQN agent with discretization 2 → 9 (meaning {2, 3, 5, 9}) and provide learned values for each action bin starting at initialization and adding a row every time the action space is grown (top to bottom). The active bins are framed in green, where we observe accurate representation of the state-action value function for active bins, while the inactive bins still provide structured output due to high degree of weight sharing provided by our architecture. In the following section, we provide quantitative results on a range of challenging continuous control tasks. We use the same set of hyperparameters throughout all experiments, unless otherwise indicated, following the general parameterization of Seyde et al. (2022b) with a simple multi-layer 5 SEYDE WERNER SCHWARTING WULFMEIER* RUS* Figure 2: State-action values for a pendulum swing-up task over the course of training (top to bot- tom). The active bins are outlined in green. The value predictions transition from random at initialization to structured upon activation. Inactive bins profit from the emergent struc- ture within the shared network torso to warm-start their optimization. perceptron architecture and dimensionality [512, 512]. We evaluate mean performance with stan- dard deviation across 4 seeds and 10 evaluation episode for each task. 5. Experiments We evaluate our approach on a selection of tasks from the DeepMind Control Suite (Tunyasuvu- nakool et al., 2020), MetaWorld (Yu et al., 2020), and MyoSuite (Vittorio et al., 2022). The former two benchmarks generally do not consider action penalties and have previously been solved with bang-bang control (Seyde et al., 2022b). We therefore focus on action-penalized task variations to encourage smooth control and highlight exploration challenges in the presence of penalty terms. We first evaluate performance on tasks from the DeepMind Control Suite with action dimen- sionality up to dim(A) = 38. We consider 2 penalty weights ca ∈ {0.1, 0.5}, such that rewards are computed as rt = ro t − ca ∑M j=1 aj t 2 from original reward ro t . We consider GQN agents that grow their action space discretization from 2 to 9 bins in each action dimension, where we evaluate both the linear and adaptive growing schedules discussed in Section 4. We compare performance against the state-of-the-art continuous control D4PG (Barth-Maron et al., 2018) and DMPO (Abdolmaleki et al., 2018b) agents, while providing two discrete control DecQN agents with stationary action space discretization of 2 or 9 for reference. The results in Figures 3 and 4 indicate the strong perfor- 6 GROWING Q-NETWORKS Figure 3: Performance on tasks from the DeepMind Control Suite with action penalty −0.1|a|2. Our GQN agent grows its action space from a 2 bin to a 9 bin discretization, where the linear and adaptive expansion schedules yield similar results. The GQN agent performs competitive to the discrete DecQN as well as the continuous D4PG and DMPO baselines, achieving noticeable improvements on the Humanoid Stand and Walk tasks. mance of GQN agents, with the adaptive schedule improving upon the linear schedule in terms of convergence rate and variance. Growing control resolution further provides a clear advantage over the stationary DecQN agents both in terms of final performance (vs. DecQN 2) and exploration abilities (vs. DecQN 9). These observations mirror findings by Czarnecki et al. (2018), where coarse control resolution was beneficial for early exploration, a characteristic that is amplified by the presence of action penalties. We further observe the strong performance of discrete GQN agents compared to the continuous D4PG and DMPO agents. In order to provide additional quantitative motivation for the presence of action penalties, we compare smoothness of the converged policies in Figure 5. We consider the adaptive GQN agent with action penalties ca ∈ {0.1, 0.5} and the continuous D4PG agent with action penalty ca = 0.5. The metrics we consider are original non-penalized task performance, R, incurred action penalty, P , action magnitude, |a|, instantaneous action change, |∆a|, and the Fast Fourier Transform (FFT) based smoothness metric from Mysore et al. (2021), SM. All metrics are normalized by the corre- sponding value achieved by the unconstrained GQN agent with ca = 0.0. The results indicate that increasing the action penalty yields noticeably smoother control signals while only having minor impact on the original task performance as measured by the unconstrained reward, R. We further find that smoothness of the discrete GQN agent is at least as good as for the continuous D4PG agent on the tasks considered (note that D4PG is unable to solve the Humanoid task variations, R ≈ 0). We next extend our study to velocity-level control tasks for the Sawyer robot in MetaWorld. While acceleration-level control often provides sufficient filtering to interact favourably with highly discretized bang-bang exploration, velocity-level control tends to require more fine-grained inputs. We therefore investigate scalability of growing action spaces within decoupled Q-learning represen- tations. To this end, we consider GQN agents with 2 → 9 and 9 → 65 (meaning {9, 17, 33, 65}) discretization as well as a stationary DecQN agent with 9 bins. The results in Figure 6 indicate that initial bang-bang action selection is not well-suited for generating velocity-level actions, with 7 SEYDE WERNER SCHWARTING WULFMEIER* RUS* Figure 4: Performance on tasks from the DeepMind Control Suite with action penalty −0.5|a|2. Our GQN agent grows its action space from a 2 bin to a 9 bin discretization, where we observe benefits of the adaptive variant over the linear schedule. The GQN agent yields performance improvements over the discrete DecQN as well as the continuous D4PG and DMPO baselines, with particularly strong deltas on the Humanoid and Finger tasks. Figure 5: Comparison of control smoothness and reward performance, relative to GQN without action penalties. Increasing the action penalty coefficient yields smoother control while only minor impact on the original task performance as measured by unconstrained reward R. The discrete GQN further improves upon the continuous D4PG agent. the agent achieving good performance once transitioning to more fine-grained discretization (GQN 2 → 9). Interestingly, considering a larger growing action space with GQN 9 → 65 can surpass the performance of a stationary DecQN 9 agent, despite the non-stationary optimization objective induced by the addition of finer action discretizations over the course of training. Performance of GQN 9 → 65 is furthermore competitive with the continuous D4PG agent on average. Lastly, we stress-test our approach by considering a selection of tasks from the MyoSuite bench- mark. The tasks require control of biomechanical models that aim to be physiologically accurate with dim(A) = 39 and up to dim(O) = 115, and should constrain applicability of simple de- coupled Q-learning approaches such as GQN. Indeed, we find that the agent capacity becomes a limiting factor yielding overestimation errors that are further exacerbated by the large magnitude reward signals. We therefore extend the network capacity to [512, 512] → [2048, 2048] and lower 8 GROWING Q-NETWORKS Figure 6: Performance on manipulation tasks from MetaWorld with action penalty −0.5|a|2. These tasks require control at the velocity level and are therefore more challenging to solve with extremely coarse discretization. We therefore investigate the scalability of our GQN agent and consider growing discretizations from 9 up to 65 bins. The resulting policy achieves stable learning and performs competitively with the continuous D4PG baseline while improving on the stationary 9 bins DecQN agent. Figure 7: Performance for controlling biomechanical models from the MyoSuite as measured by task success at termination. These continuous control tasks stress test growing decoupled discrete action spaces, due to their dimensionality and inherent complexity. Increasing the network capacity and adjusting the discount factor to mitigate overestimation, we observe strong performance for growing action spaces up to a discretization of 65 bins. the discount factor γ = 0.99 → 0.95 (alternatively, increasing multi-step returns 3 → 5 worked similarly well). With these parameter adjustments, we observe good performance as measured by task success at the final step of an episode. This further underlines the surprising effectiveness that decoupled discrete control can yield in continuous control settings and the benefit of adaptive control resolution change over the course of training. 6. Conclusion In this work, we investigate the application of growing action spaces within the context of decoupled Q-learning to efficiently solve continuous control tasks. Our Growing Q-Networks (GQN) agent leverages a linear value decomposition along actuators to retain scalability in high-dimensional ac- tion spaces and adaptively increases control resolution over the course of training. This enables coarse exploration early during training without reduced control smoothness and accuracy at con- vergence. The resulting agent is robust and performs well even for very fine control resolutions 9 SEYDE WERNER SCHWARTING WULFMEIER* RUS* despite inherent non-smoothness in the optimization objective arising at the transition between res- olution levels. While GQN as a critic-only method displays very strong performance compared to recent continuous actor-critic methods on the tasks considered, we also investigate scenarios that prove challenging for decoupled discrete controllers as exemplified by velocity-level control of simulated manipulators or applications to control of biomechanical models. Interesting avenues for future work include addressing coordination challenges in increasingly high-dimensional action spaces and mitigating overestimation bias. Generally, GQN provides a simple yet capable agent that efficiently bridges the gap between between coarse exploration and solution smoothness through adaptive control resolution refinement. Acknowledgments Tim Seyde, Peter Werner, Wilko Schwarting, and Daniela Rus were supported in part by the Office of Naval Research (ONR) Grant N00014-18-1-2830, Qualcomm, and the United States Air Force Research Laboratory and the Department of the Air Force Artificial Intelligence Accelerator under Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Department of the Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwith- standing any copyright notation herein. The authors further would like to acknowledge the MIT SuperCloud and Lincoln Laboratory Supercomputing Center for providing HPC resources. References Abbas Abdolmaleki, Jost Tobias Springenberg, Jonas Degrave, Steven Bohez, Yuval Tassa, Dan Belov, Nicolas Heess, and Martin Riedmiller. Relative entropy regularized policy iteration. arXiv preprint arXiv:1812.02256, 2018a. Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Mar- tin Riedmiller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920, 2018b. Charles W. Anderson. Learning to Control an Inverted Pendulum with Connectionist Networks. In Proceedings of the American Control Conference (ACC), 1988. Gabriel Barth-Maron, Matthew W Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva Tb, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic policy gradients. arXiv preprint arXiv:1804.08617, 2018. Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K¨uttler, Andrew Lefrancq, Simon Green, V´ıctor Vald´es, Amir Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016. R. Bellman, I. Glicksberg, and O. Gross. On the “bang-bang” control problem. Quarterly of Applied Mathematics, 14(1), 1956. Steven Bohez, Abbas Abdolmaleki, Michael Neunert, Jonas Buchli, Nicolas Heess, and Raia Had- sell. Value constrained model-free continuous control. arXiv preprint arXiv:1902.04623, 2019. 10 GROWING Q-NETWORKS Wendelin B¨ohmer, Tabish Rashid, and Shimon Whiteson. Exploration with unreliable intrinsic reward in multi-agent reinforcement learning. arXiv preprint arXiv:1906.02138, 2019. Wendelin B¨ohmer, Vitaly Kurin, and Shimon Whiteson. Deep coordination graphs. In International Conference on Machine Learning, pages 980–991. PMLR, 2020. Lucian Busoniu, Bart De Schutter, and Robert Babuska. Decentralized reinforcement learning con- trol of a robotic manipulator. In 2006 9th International Conference on Control, Automation, Robotics and Vision, pages 1–6. IEEE, 2006. Filippos Christianos, Georgios Papoudakis, Muhammad A Rahman, and Stefano V Albrecht. Scal- ing multi-agent reinforcement learning with selective parameter sharing. In International Con- ference on Machine Learning, pages 1989–1998. PMLR, 2021. Xiangxiang Chu and Hangjun Ye. Parameter sharing deep deterministic policy gradient for cooper- ative multi-agent reinforcement learning. arXiv preprint arXiv:1710.00336, 2017. Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multia- gent systems. AAAI/IAAI, 1998:2, 1998. Wojciech Czarnecki, Siddhant Jayakumar, Max Jaderberg, Leonard Hasenclever, Yee Whye Teh, Nicolas Heess, Simon Osindero, and Razvan Pascanu. Mix & match agent curricula for reinforce- ment learning. In International Conference on Machine Learning, pages 1087–1095. PMLR, 2018. Gregory Farquhar, Laura Gustafson, Zeming Lin, Shimon Whiteson, Nicolas Usunier, and Gabriel Synnaeve. Growing action spaces. In International Conference on Machine Learning, pages 3040–3051. PMLR, 2020. Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras, Philip HS Torr, Pushmeet Kohli, and Shimon Whiteson. Stabilising experience replay for deep multi-agent reinforcement learning. In International conference on machine learning, pages 1146–1155. PMLR, 2017. Carlos Guestrin, Michail Lagoudakis, and Ronald Parr. Coordinated reinforcement learning. In ICML, volume 2, pages 227–234. Citeseer, 2002. Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using deep reinforcement learning. In International conference on autonomous agents and multiagent systems, pages 66–83. Springer, 2017. Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and appli- cations. arXiv preprint arXiv:1812.05905, 2018. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with dis- crete world models. arXiv preprint arXiv:2010.02193, 2020. Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In Thirty-second AAAI conference on artificial intelligence, 2018. 11 SEYDE WERNER SCHWARTING WULFMEIER* RUS* Benjamin J. Hodel. Learning to Operate an Excavator via Policy Optimization. Procedia Computer Science, 140, 2018. Sandy H. Huang, Martina Zambelli, Jackie Kay, Murilo F. Martins, Yuval Tassa, Patrick M. Pilarski, and Raia Hadsell. Learning Gentle Object Manipulation with Curiosity-Driven Deep Reinforce- ment Learning. arXiv:1903.08542, 2019. David Ireland and Giovanni Montana. Revalued: Regularised ensemble value-decomposition for factorisable markov decision processes. arXiv preprint arXiv:2401.08850, 2024. Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population based train- ing of neural networks. arXiv preprint arXiv:1711.09846, 2017. Jelle R Kok and Nikos Vlassis. Collaborative multiagent reinforcement learning by payoff propa- gation. Journal of Machine Learning Research, 7:1789–1828, 2006. J. Lambert and M. Levine. A two-stage learning control system. Trans. on Automatic Control, 15 (3), 1970. J. P. LaSalle. Time Optimal Control Systems. Proceedings of the National Academy of Sciences, 45 (4), 1959. Martin Lauer and Martin Riedmiller. An algorithm for distributed reinforcement learning in coop- erative multi-agent systems. In In Proceedings of the Seventeenth International Conference on Machine Learning. Citeseer, 2000. La¨etitia Matignon, Guillaume J Laurent, and Nadine Le Fort-Piat. Hysteretic q-learning: an al- gorithm for decentralized reinforcement learning in cooperative multi-agent teams. In 2007 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 64–69. IEEE, 2007. Laetitia Matignon, Guillaume J Laurent, and Nadine Le Fort-Piat. Independent reinforcement learn- ers in cooperative markov games: a survey regarding coordination problems. The Knowledge Engineering Review, 27:1–31, 2012. H. Maurer, C. B¨uskens, J.-H. R. Kim, and C. Y. Kaya. Optimization methods for the verification of second order sufficient conditions for bang–bang controls. Optimal Control Applications and Methods, 26(3), 2005. Luke Metz, Julian Ibarz, Navdeep Jaitly, and James Davidson. Discrete sequential prediction of continuous actions for deep rl. arXiv preprint arXiv:1705.05035, 2017. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier- stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle- mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518:529–533, 2015. 12 GROWING Q-NETWORKS Siddharth Mysore, Bassel Mabsout, Renato Mancuso, and Kate Saenko. Regularizing action poli- cies for smooth control with reinforcement learning. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 1810–1816. IEEE, 2021. Michael Neunert, Abbas Abdolmaleki, Markus Wulfmeier, Thomas Lampe, Tobias Springen- berg, Roland Hafner, Francesco Romano, Jonas Buchli, Nicolas Heess, and Martin Riedmiller. Continuous-discrete reinforcement learning for hybrid control in robotics. In Conference on Robot Learning, pages 735–751. PMLR, 2020. Guido Novati and Petros Koumoutsakos. Remember and Forget for Experience Replay. In Interna- tional Conference on Machine Learning (ICML), 2019. Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr, Wendelin B¨ohmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised policy gra- dients. Advances in Neural Information Processing Systems, 34, 2021. Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforce- ment learning. In International Conference on Machine Learning, pages 4295–4304. PMLR, 2018. Julien Roy, Roger Girgis, Joshua Romoff, Pierre-Luc Bacon, and Christopher Pal. Direct behavior specification via constrained reinforcement learning. arXiv preprint arXiv:2112.12228, 2021. Stuart J Russell and Andrew Zimdars. Q-decomposition for reinforcement learning agents. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pages 656– 663, 2003. Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015. Jeff G Schneider, Weng-Keen Wong, Andrew W Moore, and Martin A Riedmiller. Distributed value functions. In ICML, 1999. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Tim Seyde, Igor Gilitschenski, Wilko Schwarting, Bartolomeo Stellato, Martin Riedmiller, Markus Wulfmeier, and Daniela Rus. Is bang-bang control all you need? solving continuous control with bernoulli policies. Advances in Neural Information Processing Systems, 34, 2021. Tim Seyde, Wilko Schwarting, Igor Gilitschenski, Markus Wulfmeier, and Daniela Rus. Strength through diversity: Robust behavior learning via mixture policies. In Conference on Robot Learn- ing, pages 1144–1155. PMLR, 2022a. Tim Seyde, Peter Werner, Wilko Schwarting, Igor Gilitschenski, Martin Riedmiller, Daniela Rus, and Markus Wulfmeier. Solving continuous control via q-learning. In The Eleventh International Conference on Learning Representations, 2022b. 13 SEYDE WERNER SCHWARTING WULFMEIER* RUS* Sahil Sharma, Aravind Suresh, Rahul Ramesh, and Balaraman Ravindran. Learning to factor poli- cies and action-value functions: Factored action space representations for deep reinforcement learning. arXiv preprint arXiv:1705.07269, 2017. Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and charac- terizing reward gaming. Advances in Neural Information Processing Systems, 35:9460–9471, 2022. Laura Smith, Yunhao Cao, and Sergey Levine. Grow your limits: Continuous improvement with real-world rl for robotic locomotion. arXiv preprint arXiv:2310.17634, 2023. Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In Interna- tional Conference on Machine Learning, pages 5887–5896. PMLR, 2019. L. M. Sonneborn and F. S. Van Vleck. The Bang-Bang Principle for Linear Control Systems. Journal of the Society for Industrial and Applied Mathematics Series A Control, 2(2), 1964. Jianyu Su, Stephen Adams, and Peter A Beling. Value-decomposition multi-agent actor-critics. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 11352–11360, 2021. Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition networks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296, 2017. Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. 2018. Gabriel Synnaeve, Jonas Gehring, Zeming Lin, Daniel Haziza, Nicolas Usunier, Danielle Rother- mel, Vegard Mella, Da Ju, Nicolas Carion, Laura Gustafson, et al. Growing up together: Struc- tured exploration for large action spaces. 2019. Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings of the tenth international conference on machine learning, pages 330–337, 1993. Yunhao Tang and Shipra Agrawal. Discretizing continuous action space for on-policy optimization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 5981–5988, 2020. Arash Tavakoli. On structural and temporal credit assignment in reinforcement learning. PhD thesis, Imperial College London, 2021. Arash Tavakoli, Fabio Pardo, and Petar Kormushev. Action branching architectures for deep rein- forcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018. Arash Tavakoli, Mehdi Fatemi, and Petar Kormushev. Learning to represent action values as a hypergraph on the action vertices. In International Conference on Learning Representations, 2021. 14 GROWING Q-NETWORKS Thomas George Thuruthel, Egidio Falotico, Federico Renda, and Cecilia Laschi. Model-Based Reinforcement Learning for Closed-Loop Dynamic Control of Soft Robotic Manipulators. IEEE T-RO, 35(1), 2019. Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh Merel, Tom Erez, Timothy Lillicrap, Nicolas Heess, and Yuval Tassa. dm control: Software and tasks for continuous control. Software Impacts, 6:100022, 2020. Tom Van de Wiele, David Warde-Farley, Andriy Mnih, and Volodymyr Mnih. Q-learning in enor- mous action spaces via amortized approximate maximization. arXiv preprint arXiv:2001.08116, 2020. Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q- learning. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016. Harm Van Seijen, Mehdi Fatemi, Joshua Romoff, Romain Laroche, Tavian Barnes, and Jeffrey Tsang. Hybrid reward architecture for reinforcement learning. Advances in Neural Information Processing Systems, 30, 2017. Caggiano Vittorio, Wang Huawei, Durandau Guillaume, Sartori Massimo, and Kumar Vikash. Myosuite – a contact-rich simulation suite for musculoskeletal motor control. https:// github.com/myohub/myosuite, 2022. URL https://arxiv.org/abs/2205. 13600. M. Waltz and K. Fu. A heuristic approach to reinforcement learning control systems. IEEE TACON, 10(4), 1965. Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Dop: Off-policy multi-agent decomposed policy gradients. In International Conference on Learning Representa- tions, 2020. Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8:279–292, 1992. Markus Wulfmeier, Abbas Abdolmaleki, Roland Hafner, Jost Tobias Springenberg, Michael Ne- unert, Noah Siegel, Tim Hertweck, Thomas Lampe, Nicolas Heess, and Martin Riedmiller. Com- positional Transfer in Hierarchical Reinforcement Learning. In Robotics: Science and Systems (RSS), 2020. Jiachen Yang, Tarik Dzanic, Brenden Petersen, Jun Kudo, Ketan Mittal, Vladimir Tomov, Jean- Sylvain Camier, Tuo Zhao, Hongyuan Zha, Tzanio Kolev, et al. Reinforcement learning for adaptive mesh refinement. In International Conference on Artificial Intelligence and Statistics, pages 5997–6014. PMLR, 2023. Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean field multi- agent reinforcement learning. In International Conference on Machine Learning, pages 5571– 5580. PMLR, 2018. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learn- ing. In Conference on Robot Learning, pages 1094–1100. PMLR, 2020. 15","libVersion":"0.3.2","langs":""}