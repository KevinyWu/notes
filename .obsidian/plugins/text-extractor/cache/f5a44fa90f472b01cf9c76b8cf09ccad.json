{"path":"papers/deep-reinforcement-learning/slides/1 Introduction.pdf","text":"? Option 1: Understand the problem, design a solution Option 2: Set it up as a machine learning problem data supervised learning data reinforcement learning 4 What are some recent advances we’ve seen in AI? image credit: UW IPD 5 What’s the main idea behind this? 6 What does reinforcement learning do differently? Evolved Virtual Creatures. Karl Sims, 1994 Model-Predictive Control with iLQG. Yuval Tassa, 2012 evolutionary algorithms, controls, optimization classical reinforcement learning deep reinforcement learning 7 Reinforcement learning can discover new solutions “Move 37” in Lee Sedol AlphaGo match: reinforcement learning “discovers” a move that surprises everyone Impressive because it looks like something a person might draw! Impressive because no person had thought of it!Deep Reinforcement Learning, Decision Making, and Control CS 285 Instructor: Sergey Levine UC Berkeley Course logistics Class Information & Resources • Course website: http://rail.eecs.berkeley.edu/deeprlcourse • Ed: CS285 Deep Reinforcement Learning: https://edstem.org/us/join/c9XPxK • Gradescope: CS285 Deep Reinforcement Learning: https://www.gradescope.com/courses/571673 • Office hours: check course website, mine are after class right here (starting next week) • Links to lecture videos, etc.: always posted on Ed (pinned post) Sergey Levine Instructor Kyle Stachowicz Head GSI Joey Hong GSI Vivek Myers GSI Kevin Black GSI Prerequisites & Enrollment • All enrolled students must have taken CS182, CS189, CS289, CS281A, CS282, or an equivalent course at your home institution • Please contact Sergey Levine if you haven’t • If you are not eligible to enroll directly into the class, fill out the enrollment application form (do this today): http://rail.eecs.berkeley.edu/deeprlcourse/ • We will enroll subject to availability based on responses to this form • We will not use the official CalCentral wait list! • Fill out an application before the end of this week (8/25/23)! Class format • Lectures • Lectures will be posted over the weekend on YouTube • The YouTube playlist link will be posted on Ed and on the course website: • https://youtube.com/playlist?list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps • Some lectures from past years, some new • Watch the lectures in advance • Post questions on Ed thread • Upvote questions on Ed thread • Every lecture has a quiz Class format • Every lecture has a quiz • Complete on Gradescope prior to class • Due at 5 pm on the day of the corresponding lecture • Except today’s quiz, which is due 5 pm next wk Mon • Should never take more than 5-10 min if you watched the lecture and understood it • Check your understanding, lightweight & quick • Also serves to summarize the key concepts you should know • If you don’t like your grade you can actually take it again (second time)! • We release your grades automatically on the due date at 5 pm • We release a “second try” quiz that you can take if you don’t like your grade (within 48 hours) • Answers will be released with “second try” quiz Class format • In class • Attend class live: meant to be interactive • Discuss questions: submit & upvote Ed questions to have a fun class! • More in-depth derivations: we’ll go through some whiteboard derivations • Participate and ask questions: plenty of time for Q&A, bring your questions! • Length: Class will be about 50 min, followed by about 30 min of OH, since you are already watching 50-70 minutes of online lecture • Office hours: My office hours will take place immediately after class • As soon as we cover all discussion, we will switch to “office hours” – use this time to ask questions about final projects, etc. • Exception is today, since we have a “classic” lecture, and presumably you don’t have many OH questions on first day of class :) What you should know • Assignments will require training neural networks with standard automatic differentiation packages (PyTorch) • Review Section • We will have a review section to cover PyTorch next week on Wednesday • Optional, no quiz, but very useful if you are unfamiliar with it! • If you are unsure about background, try HW1 as soon as it comes out (next Monday) and if you’re having trouble, come to the review section Course content What we’ll cover 1. From supervised learning to decision making 2. Model-free algorithms: Q-learning, policy gradients, actor-critic 3. Model-based algorithms: planning, sequence models, etc. 4. Exploration 5. Offline reinforcement learning 6. Inverse reinforcement learning 7. Advanced topics, research talks, and invited lectures Assignments 1. Homework 1: Imitation learning (control via supervised learning) 2. Homework 2: Policy gradients 3. Homework 3: Q-learning and actor-critic algorithms 4. Homework 4: Model-based reinforcement learning 5. Homework 5: Offline reinforcement learning 6. Final project: Research-level project of your choice (form a group of up to 2-3 students, you’re welcome to start early!) Grading: 50% homework (10% each), 40% project, 10% quizzes 5 late days total for homeworks (does not apply to quizzes, proposal, milestone report, or final project report) Your “Homework” Today 1. Make sure you are signed up for Ed (UC Berkeley CS285) 2. Start forming your final project groups, unless you want to work alone, which is fine 3. Take the lecture 1 quiz • it should be super quick if you watched lecture 1, mostly to familiarize yourself with Gradescope interface What is reinforcement learning? What is reinforcement learning? Mathematical formalism for learning-based decision making Approach for learning decision making and control from experience How is this different from other machine learning topics? Standard (supervised) machine learning: Usually assumes: • i.i.d. data • known ground truth outputs in training Reinforcement learning: • Data is not i.i.d.: previous outputs influence future inputs! • Ground truth answer is not known, only know if we succeeded or failed • more generally, we know the reward data supervised learning data reinforcement learning What is reinforcement learning? supervised learning reinforcement learning someone gives this to you pick your own actions decisions (actions) consequences observations rewards Actions: muscle contractions Observations: sight, smell Rewards: food Actions: motor current or torque Observations: camera images Rewards: task success measure (e.g., running speed) Actions: what to purchase Observations: inventory levels Rewards: profit (states) Complex physical tasks Rajeswaran, et al. 2018 Really complex physical tasks! Smith et al., “Learning and Adapting Agility Skills by Transferring Experience.” 2022. Really really complex physical tasks! Rudin et al., “Learning Locomotion and Local Navigation End-to-End.” 2022 Unexpected solutions Mnih, et al. 2015 At scale in the real world Herzog et al. Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators. 2023. Not just games and robots! Cathy Wu Reinforcement learning with language models Source: https://huggingface.co/blog/rlhf Reinforcement learning with image generation “a dolphin riding a bike” DDPO (RL) DDPO (RL) DDPO (RL) DDPO (RL) Kevin Black*, Michael Janner*, Yilun Du, Ilya Kostrikov, Sergey Levine. Training Diffusion Models with Reinforcement Learning. 2023. Reinforcement learning for chip design Source: https://ai.googleblog.com/2020/04/chip-design-with-deep-reinforcement.html Why should we study deep reinforcement learning? 35 Reinforcement learning can discover new solutions “Move 37” in Lee Sedol AlphaGo match: reinforcement learning “discovers” a move that surprises everyone Impressive because it looks like something a person might draw! Impressive because no person had thought of it! 36 How does data-driven AI work? 37 Data-Driven AI Reinforcement Learning So where does that leave us? + learns about the real world from data - doesn’t try to do better than the data + optimizes a goal with emergent behavior - but need to figure out how to use at scale! Data without optimization doesn’t allow us to solve new problems in new ways All about using data All about optimization 38 A bitter but misunderstood lesson Richard Sutton We have to learn the bitter lesson that building in how we think we think does not work in the long run. The two methods that seem to scale arbitrarily … are learning and search. - “The Bitter Lesson”, Sutton 2019 You mean we should shovel more data into the GPU, right? xkcd Learning Search optimization use data to extract patterns use computation to extract inferences allows us to understand the world leverages that understanding for emergence some optimization process that uses (typically iterative) computation to make rational decisions Data without optimization doesn’t allow us to solve new problems in new ways Optimization without data is hard to apply to the real world outside of simulators A bit of philosophy 39 Why do we need machine learning anyway? Aside: why do we need brains anyway? Daniel Wolpert (knows quite a lot about brains) “We have a brain for one reason and one reason only -- that's to produce adaptable and complex movements. Movement is the only way we have affecting the world around us… I believe that to understand movement is to understand the whole brain.” A postulate: We need machine learning for one reason and one reason only – that’s to produce adaptable and complex decisions. Decision: how do I move my joints? Decision: how do I steer the car? What is the decision? The image label? What happens with that label afterwards? How do we build intelligent machines?Why should we study this now? big end-to-end trained models work quite well! we have RL algorithms that we can feasibly combine with deep networks and yet learning-based control in truly real-world settings remains a major open problem! What other problems do we need to solve to enable real-world sequential decision making? Beyond learning from reward • Basic reinforcement learning deals with maximizing rewards • This is not the only problem that matters for sequential decision making! • We will cover more advanced topics • Learning reward functions from example (inverse reinforcement learning) • Transferring knowledge between domains (transfer learning, meta-learning) • Learning to predict and using prediction to act Where do rewards come from?Are there other forms of supervision? • Learning from demonstrations • Directly copying observed behavior • Inferring rewards from observed behavior (inverse reinforcement learning) • Learning from observing the world • Learning to predict • Unsupervised learning • Learning from other tasks • Transfer learning • Meta-learning: learning to learn Imitation learning Bojarski et al. 2016 More than imitation: inferring intentions Warneken & Tomasello Inverse RL examples Finn et al. 2016 Prediction Ebert et al. 2017 Prediction for real-world control Xie et al. 2019 Using tools with predictive models Predictive models have come a long way! Voleti et al. 2022 Predictive models have come a long way! Voleti et al. 2022 Leveraging advances in pretrained models RT-2: Vision-Language-Action Models: https://robotics-transformer2.github.io/ Leveraging advances in pretrained models RT-2: Vision-Language-Action Models: https://robotics-transformer2.github.io/ Leveraging advances in pretrained models RT-2: Vision-Language-Action Models: https://robotics-transformer2.github.io/ How do we build intelligent machines? How do we build intelligent machines? • Imagine you have to build an intelligent machine, where do you start? Learning as the basis of intelligence • Some things we can all do (e.g. walking) • Some things we can only learn (e.g. driving a car) • We can learn a huge variety of things, including very difficult things • Therefore our learning mechanism(s) are likely powerful enough to do everything we associate with intelligence • But it may still be very convenient to “hard-code” a few really important bits A single algorithm? [BrainPort; Martinez et al; Roe et al.] Seeing with your tongue Auditory Cortex adapted from A. Ng • An algorithm for each “module”? • Or a single flexible algorithm? What must that single algorithm do? • Interpret rich sensory inputs • Choose complex actions Why deep reinforcement learning? • Deep = scalable learning from large, complex datasets • Reinforcement learning = optimization Learning Search optimization use data to extract patterns use computation to extract inferences allows us to understand the world leverages that understanding for emergence Some evidence in favor of deep learningSome evidence for reinforcement learning • Percepts that anticipate reward become associated with similar firing patterns as the reward itself • Basal ganglia appears to be related to reward system • Model-free RL-like adaptation is often a good fit for experimental data of animal adaptation • But not always… What challenges still remain? • We have great methods that can learn from huge amounts of data • We have great optimization methods for RL • We don’t (yet) have amazing methods that both use data and RL • Humans can learn incredibly quickly, deep RL methods are usually slow • Humans reuse past knowledge, transfer learning in RL is an open problem • Not clear what the reward function should be • Not clear what the role of prediction should be Instead of trying to produce a program to simulate the adult mind, why not rather try to produce one which simulates the child's? If this were then subjected to an appropriate course of education one would obtain the adult brain. - Alan Turing general learning algorithm environmentobservationsactions","libVersion":"0.3.2","langs":""}