{"path":"papers/foundation/figures/octo.png","text":"Task Tokens Task Observation Readout Observation Readout Observation Put he e, onhe plat Emme s ) oo )=o) v [ e a Octo Transformer Q s i v Wy PT—O0—0—0—0 Socoa T =) =) Observation Tokens T Pre—Training -»a ->a BERE | ricuning L | '.7 ! {0 - Gy (o - @ a Octo Transformer p coeeT '+{ New Action Head |~ Figure 2: Model architecture. Left: Octo tokenizes task descriptions (green) and input obser- vations (blue) using pretrained language models and CNNs respectively. Top: The transformer backbone processes the sequence of task and observation tokens and produces readout tokens (purple) that get passed to output heads to produce actions. Bottom: The block-wise attention structure of the transformer backbone allows to flexibly add and remove inputs and outputs during finetuning and e.g., add new observations (blue, dashed) or action spaces (purple, dashed) during finetuning.","libVersion":"0.3.2","langs":"eng"}