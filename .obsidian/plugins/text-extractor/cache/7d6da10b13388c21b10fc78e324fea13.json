{"path":"papers/deep-reinforcement-learning/slides/6 Actor-Critic Algorithms.pdf","text":"Actor-Critic Algorithms CS 285 Instructor: Sergey Levine UC Berkeley Recap: policy gradients generate samples (i.e. run the policy) fit a model to estimate return improve the policy “reward to go” Improving the policy gradient “reward to go” What about the baseline?State & state-action value functions the better this estimate, the lower the variance unbiased, but high variance single-sample estimate generate samples (i.e. run the policy) fit a model to estimate return improve the policy Value function fitting generate samples (i.e. run the policy) fit a model to estimate return improve the policy Policy evaluation generate samples (i.e. run the policy) fit a model to estimate return improve the policy Monte Carlo evaluation with function approximation the same function should fit multiple samples! Can we do better?Policy evaluation examples TD-Gammon, Gerald Tesauro 1992 AlphaGo, Silver et al. 2016 From Evaluation to Actor Critic An actor-critic algorithm generate samples (i.e. run the policy) fit a model to estimate return improve the policy Aside: discount factors episodic tasks continuous/cyclical tasks Aside: discount factors for policy gradientsWhich version is the right one? Further reading: Philip Thomas, Bias in natural actor-critic algorithms. ICML 2014 Actor-critic algorithms (with discount) Actor-Critic Design Decisions Architecture design two network design + simple & stable - no shared features between actor & critic shared network design Online actor-critic in practice works best with a batch (e.g., parallel workers) synchronized parallel actor-critic asynchronous parallel actor-critic Can we remove the on-policy assumption entirely? off-policy actor-critic replay buffer transitions that we saw in prior time steps form a batch by using old previously seen transitions Let’s see what that looks like replay buffer batch size This algorithm is broken! Can you spot the problems? Fixing the value functionFixing the policy updateWhat else is left? Is there any remaining problem? Some implementation details could also use reparameterization trick to better estimate the integral lots of fancier ways to fit Q-functions (more on this in next two lectures) Example practical algorithm: Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine. Soft Actor-Critic: Off- Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. 2018. We’ll also learn about algorithms that do this with deterministic policies later! Critics as Baselines Critics as state-dependent baselines + no bias - higher variance (because single-sample estimate) + lower variance (due to critic) - not unbiased (if the critic is not perfect) + no bias + lower variance (baseline is closer to rewards) Control variates: action-dependent baselines + no bias - higher variance (because single-sample estimate) + goes to zero in expectation if critic is correct! - not correct use a critic without the bias (still unbiased), provided second term can be evaluated Gu et al. 2016 (Q-Prop) Eligibility traces & n-step returns + lower variance - higher bias if value is wrong (it always is) + no bias - higher variance (because single-sample estimate) Can we combine these two, to control bias/variance tradeoff? Generalized advantage estimation Schulman, Moritz, Levine, Jordan, Abbeel ‘16 Do we have to choose just one n? Cut everywhere all at once! weighted combination of n-step returns How to weight? Mostly prefer cutting earlier (less variance) exponential falloff similar effect as discount! remember this? discount = variance reduction! Review, Examples, and Additional Readings Review • Actor-critic algorithms: • Actor: the policy • Critic: value function • Reduce variance of policy gradient • Policy evaluation • Fitting value function to policy • Discount factors • Carpe diem Mr. Robot • …but also a variance reduction trick • Actor-critic algorithm design • One network (with two heads) or two networks • Batch-mode, or online (+ parallel) • State-dependent baselines • Another way to use the critic • Can combine: n-step returns or GAE generate samples (i.e. run the policy) fit a model to estimate return improve the policy Actor-critic examples • High dimensional continuous control with generalized advantage estimation (Schulman, Moritz, L., Jordan, Abbeel ‘16) • Batch-mode actor-critic • Blends Monte Carlo and function approximator estimators (GAE) Actor-critic examples • Asynchronous methods for deep reinforcement learning (Mnih, Badia, Mirza, Graves, Lillicrap, Harley, Silver, Kavukcuoglu ‘16) • Online actor-critic, parallelized batch • N-step returns with N = 4 • Single network for actor and critic Actor-critic suggested readings • Classic papers • Sutton, McAllester, Singh, Mansour (1999). Policy gradient methods for reinforcement learning with function approximation: actor-critic algorithms with value function approximation • Deep reinforcement learning actor-critic papers • Mnih, Badia, Mirza, Graves, Lillicrap, Harley, Silver, Kavukcuoglu (2016). Asynchronous methods for deep reinforcement learning: A3C -- parallel online actor-critic • Schulman, Moritz, L., Jordan, Abbeel (2016). High-dimensional continuous control using generalized advantage estimation: batch-mode actor-critic with blended Monte Carlo and function approximator returns • Gu, Lillicrap, Ghahramani, Turner, L. (2017). Q-Prop: sample-efficient policy- gradient with an off-policy critic: policy gradient with Q-function control variate","libVersion":"0.3.2","langs":""}