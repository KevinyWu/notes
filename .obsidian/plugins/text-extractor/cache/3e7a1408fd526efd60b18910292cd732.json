{"path":"video2reward/papers/2024 Data Augmentation.pdf","text":"Published as a conference paper at ICLR 2024 REVISITING DATA AUGMENTATION IN DEEP REINFORCEMENT LEARNING Jianshu Hu, Yunpeng Jiang UM-SJTU Joint Institute Shanghai Jiao Tong University Shanghai, China {hjs1998,jyp9961}@sjtu.edu.cn Paul Weng Data Science Research Center Duke Kunshan University Kunshan, Jiangsu, China paul.weng@duke.edu ABSTRACT Various data augmentation techniques have been recently proposed in image- based deep reinforcement learning (DRL). Although they empirically demonstrate the effectiveness of data augmentation for improving sample efficiency or gener- alization, which technique should be preferred is not always clear. To tackle this question, we analyze existing methods to better understand them and to uncover how they are connected. Notably, by expressing the variance of the Q-targets and that of the empirical actor/critic losses of these methods, we can analyze the ef- fects of their different components and compare them. We furthermore formulate an explanation about how these methods may be affected by choosing different data augmentation transformations in calculating the target Q-values. This anal- ysis suggests recommendations on how to exploit data augmentation in a more principled way. In addition, we include a regularization term called tangent prop, previously proposed in computer vision, but whose adaptation to DRL is novel to the best of our knowledge. We evaluate our proposition 1 and validate our analysis in several domains. Compared to different relevant baselines, we demonstrate that it achieves state-of-the-art performance in most environments and shows higher sample efficiency and better generalization ability in some complex environments. 1 INTRODUCTION Although Deep Reinforcement Learning (DRL) has shown its effectiveness in various tasks, like playing video games (Mnih et al., 2013) and solving control tasks (Li et al., 2019), it still suffers from low sample efficiency and poor generalization ability. To tackle those two issues, data aug- mentation, a proven simple and efficient technique in computer vision (Kumar et al., 2023), starts to be actively studied in image-based DRL where it has been implemented in various ways (Laskin et al., 2020; Kostrikov et al., 2020; Raileanu et al., 2021). However, most such work has been mainly experimental and a more principled comparison between these state-of-the-art methods is lacking. In this paper, we study data augmentation techniques in image-based online DRL. We formulate a general actor-critic scheme integrating data augmentation. We then show that current data augmen- tation methods, categorized as explicit/implicit regularization, are instances of this general scheme. In explicit regularization, image transformations are used in regularization terms calculated with augmented samples to explicitly enforce invariance in the actor and critic. By contrast, image trans- formations are directly applied on the observations during training in implicit regularization. Following the analysis about implicit and explicit regularization, we propose a principled data aug- mentation method in DRL and further justify its design. We start the justification with a discussion about applying different image transformations in calculating the target Q-values. Hansen et al. (2021) propose to avoid using complex image transformations in calculating the targets to stabilize the training. We provide further analysis on why some image transformations are complex and not suitable for calculating the target and how to judge whether an image transformation is complex or not. 1The source code of our method: https://github.com/Jianshu-Hu/drqv2 1arXiv:2402.12181v1 [cs.LG] 19 Feb 2024 Published as a conference paper at ICLR 2024 In addition, we justify other components of our method (e.g., KL regularization in policy) by an- alyzing the variance of the actor/critic losses and the variance of the Q-estimation under image transformation, which is important to control since applying random image transformations neces- sarily increase the variance of those statistics. The analysis also reveals the importance of learning the invariance in critic for stabilizing the training. This latter observation motivates us to include an adaption of tangent prop (Simard et al., 1991) regularization in the training of the critic. Contributions: (1) We analyze existing state-of-the-art data augmentation methods in image- based DRL and show how they are related. (2) We provide an empirical and theoretical analysis for the different components in these methods to justify their effectiveness. (3) Based on our anal- ysis, we propose a principled data augmentation actor-critic scheme, which also includes tangent prop, which is novel in DRL. (4) We empirically validate our analysis and evaluate our method. 2 RELATED WORK Many data augmentation techniques for DRL have been proposed, mostly for image-based DRL (Ma et al., 2022), although the fully-observable setting has also been considered Lin et al. (2020). Data augmentation can be used to generate artificial observations, transitions, or trajectories for an existing DRL algorithm or for improving representation learning. Most propositions investigate the usual online DRL training, but recently data augmentation with self-supervised learning in DRL (Srinivas et al., 2020; Schwarzer et al., 2021) has become more active following its success in com- puter vision (He et al., 2020; Grill et al., 2020). For space reasons, we focus our discussion on the most relevant methods for our work: data augmentation for image-based online DRL. The first methods to leverage data augmentation in DRL apply transformations directly on obser- vations to generate artificial ones to train the RL agent, which can lead to better generalization (Cobbe et al., 2018). In particular, Reinforcement learning with Augmented Data (RAD) (Laskin et al., 2020) extends Soft Actor-Critic (SAC) (Haarnoja et al., 2018) and Proximal Policy Opti- mization (PPO) (Schulman et al., 2017) to directly train with augmented observations. Instead, Data-regularized Q (DrQ) (Kostrikov et al., 2020) introduces in SAC the idea of using an aver- aged Q-target by leveraging more augmented samples. Using Deep Deterministic Policy Gradient (DDPG) (Lillicrap et al., 2015), DrQ-v2 (Yarats et al., 2021) provides various implementation and hyperparameter optimizations and gives up the idea of averaged Q-target. SVEA (Hansen et al., 2021) avoids using complex image transformation when calculating the target values because doing so increases the variance of the target value. Raileanu et al. (2021) argue that simply applying image transformations on the observations in PPO may lead to a wrong estimation of the actor loss and instead propose adding regularization terms in the actor and critic losses. They also propose some methods of automatically choosing image transformation. In recent work like (Liu et al., 2023; Yuan et al., 2022), they investigate problems of using a trainable image transformation which is orthogonal to ours. 3 BACKGROUND In this section, we define notations, recall invariant transformations in DRL, formulate a generic actor-critic scheme exploiting data augmentation, and explain how existing methods fit this scheme according to how data augmentation is applied in the actor and critic losses. Notations For any set X , ∆(X ) denotes the set of probability distributions over X . For a random variable X, E[X] (resp. V[X]) denotes its expectation (resp. variance). For any function ϕ and i.i.d. samples x1, . . . , xN of X, ˆE[ϕ(X)] = 1 N ∑N i=1 ϕ(xi) is an empirical mean estimating E[ϕ(X)]. A Markov Decision Process (MDP) M = (S, A, r, T, ρ0) is composed of a set of state S, a set of action A, a reward function r : S × A → R, a transition function T : S × A → ∆(S), and a probability distribution over initial states ρ0 ∈ ∆(S). In reinforcement learning (RL), the agent is trained by interacting with the environment to learn a policy π(· | s) ∈ ∆(A) such that the expected return Eπ[∑∞ t=0 γtrt | s0 ∼ ρ0] is maximized. 2 Published as a conference paper at ICLR 2024 Algorithm 1 Data-Augmented Off-policy Actor-Critic Scheme Hyperparameters: total number of training steps T , mini-batch size N , policy update frequency κ, image transformation set FT = {fτ | τ ∈ T }, distributions P, B ∈ ∆(T ), random variables ν ∼ P, µ ∼ B to transform states. 1: Initialize critic Qϕ(s, a), actor πθ(s), and empty replay buffer D. 2: Start with initial state s0. 3: for t = 0 . . . T do 4: Interact with the environment using action from current policy at ∼ π(· | st). 5: Save transition (st, at, rt, st+1) in replay buffer D. 6: if episode ends then reset st+1 to an initial state end if 7: // Actor-critic update 8: Sample mini-batch {(si, ai, ri, s ′ i) | i = 1, . . . , N } from D. 9: Lϕ = 1 N ∑ i ℓϕ(si, ai, ri, s ′ i, ν, µ) 10: Update ϕ (critic parameters) with gradient of Lϕ. 11: if t mod κ = 0 then 12: Lθ = 1 N ∑ i ℓθ(si, µ) 13: Update θ (actor parameters) with gradient of Lθ. 14: end if 15: end for In image-based RL, the underlying model is actually a partially observable MDP (POMDP): the agent observes images instead of states. However, following common practice, we approximate this POMDP with an MDP by assuming that a state is a stack of several consecutive images. With data augmentation, the same transformation is applied on all the stacked images of that state. Invariant Transformations Data augmentation in image-based control tasks assumes that a set FT = {fτ : Rh×w → Rh×w | τ ∈ T } of parameterized (h × w-sized) image transformations fτ that leave optimal policies invariant is given for some parameter set T . Exploiting this property in online RL is difficult, since an optimal policy is unknown. However, it suggests to directly focus learning on policies that are invariant with respect to this set FT . Recall: Definition 1 (π-invariance). A policy π is invariant with respect to an image transformation fτ if: π(a | s) = π(a | fτ (s)) for all s ∈ S, a ∈ A. (1) Interestingly, the Q-functions of those policies satisfy the following invariance property: Definition 2 (Q-invariance). A Q-function is invariant with respect to image transformation fτ if: Q(s, a) = Q(fτ (s), a) for all s ∈ S, a ∈ A. (2) The definitions imply that FT can be assumed to be closed under composition without loss of gener- ality. We assume that set T contains τ0 such that fτ0 is the identity function, which would allow the possibility of not performing any image transformation. Examples of image transformation are for instance: (small) random shift, which pads and then randomly crops the images; random overlay, which combines original images with extra images. Empirically, random shift has been shown to be one of the best transformations to accelerate DRL training across a diversity of RL tasks, while random overlay is helpful for generalization. We formulate a simple actor-critic scheme (Algorithm 1), which boils down to standard off-policy actor-critic if the original losses are applied. Existing data-augmentation-based DRL methods fit this scheme by enforcing Q-invariance and/or π-invariance via explicit or implicit regularization, as explained next. We discuss them next and explain how they fit Algorithm 1. Due to the page limit, we recall all the related DRL algorithms in Appendix A and consider SAC (Haarnoja et al., 2018) as the base algorithm in the main text and discuss the variant with DDPG (Lillicrap et al., 2015) in the corresponding appendices. 3.1 EXPLICIT REGULARIZATION The invariant transformations FT can be directly used to promote the invariance of the learned Q-function and learn more invariant policies with respect to them. Formally, this can be achieved 3 Published as a conference paper at ICLR 2024 by formulating the following (empirical) critic loss and actor loss with two explicit regularization terms: for a transition (s, a, r, s ′) and random variables ν, µ over T , ℓ E ϕ (s, a, r, s ′, ν) = (Qϕ(s, a) − y(s ′, a ′) )2 + αQ ˆEν[(Qϕ(fν(s), a) − Qϕ,sg(s, a))2] and (3) ℓ E θ (s, µ) = α log πθ(ˆa | s) − Qϕ(s, ˆa) + απ ˆEµ[DKL(πθ,sg(· | s) ∥ πθ(· | fµ(s)))], (4) where ϕ, θ are the parameter of the critic Qϕ and actor πθ, target y(s ′, a ′) is defined as r + γQ ¯ϕ(s ′, a ′) − α log πθ(a ′|s ′) with a′ ∼ πθ(· | s ′) and target network parameter ¯ϕ, action ˆa is sampled from πθ(· | s), coefficients α, αQ, and απ respectively correspond to the entropy term and the regularization terms to promote invariance in the critic and actor, and subscript sg represents ”stop gradient” for the corresponding term. In practice, Equations 3 and 4 could be used in an actor-critic algorithm: for instance, they could replace the losses in lines 9 and 12 in Algorithm 1. Note that DrAC (Raileanu et al., 2021) uses them by setting the distribution of ν and µ to be uniform (although DrAC is based on the PPO algorithm). 3.2 IMPLICIT REGULARIZATION Another commonly used data augmentation method in DRL is directly applying the image trans- formation on states during training. Formally, for a transition (s, a, r, s ′) and random variables ν, µ over T , the (empirical) critic and actor losses with implicit regularization are respectively: ℓ I ϕ(s, a, r, s ′, ν, µ) = ˆEν[( Qϕ(fν(s), a) − ˆEµ[y(fµ(s ′), a ′)] )2] and (5) ℓI θ(s, µ) = ˆEµ[ α log πθ(ˆa | fµ(s)) − Qϕ(fµ(s), ˆa)] , (6) with y(fµ(s ′), a ′) = r+γQ ¯ϕ(fµ(s ′), a ′)−α log πθ(a ′|s ′), a ′ ∼ πθ(· | fµ(s ′)), and ˆa ∼ πθ(· | fµ(s)). The empirical means can be calculated with different numbers of samples: M and K samples for ˆEν and ˆEµ in the critic loss, and J samples for ˆEµ in the actor loss. Note that with K > 1, the corresponding empirical expectation corresponds to the average target used in DrQ. If ν and µ have a uniform distribution, Equations 5 and 6 are the losses used in DrQ (Kostrikov et al., 2020) with J = 1 and RAD (Laskin et al., 2020) with M = 1, K = 1, and J = 1. Moreover, different image transformations for ν and µ can be used. In SVEA (Hansen et al., 2021), complex image transformations are only applied on states s and included in random variable ν in the critic loss. 4 THEORETICAL DISCUSSION In this section, we theoretically compare explicit regularization with implicit regularization. 4.1 CRITIC LOSS Before comparing the critic losses in these two regularizations, we first compare an alternative ex- plicit regularization one may think of to enforce Q-invariance, which uses: ℓ E ϕ (s, a, r, s ′, ν) = (Qϕ(s, a) − y(s ′, a ′) )2 + αQ ˆEν[(Qϕ(fν(s), a) − y(s ′, a ′))2] . (7) The only difference compared with Equation 3 is to replace the target in the regularization term with y(s ′, a ′). Intuitively, using y(s ′, a ′) is a better target because it not only promotes the invariance in the critic, but also serves as a target to improve the critic. In Appendix C.1, we show that using y(s ′, a ′) leads to a smaller bias which may be preferred in explicit regularization. Now the critic losses in explicit regularization and implicit regularization can be connected by setting the distributions of the image transformation parameters, as shown in the following simple lemma: Lemma 1. (C.2)2 There exist distributions for ˆν and ˆµ such that we have for any sample (s, a, r, s′): (αQ + 1)ℓ I ϕ(s, a, r, s′, ˆν, ˆµ) = ℓ E ϕ (s, a, r, s ′, ν) Note that the extra factor αQ + 1 can be controlled by adjusting the learning rate of gradient descent. Therefore, explicit regularization amounts to assigning a higher probability to τ0. 2All detailed derivations/proofs are in the appendix. The appendix number is provided for ease of reference. 4 Published as a conference paper at ICLR 2024 4.2 ACTOR LOSS Obviously, using the actor loss in explicit regularization (Equation 4) promotes the invariance in the policy. More interestingly, the actor loss in implicit regularization (Equation 6) also implicitly enforces policy invariance, but under some conditions, as we show next. First, recall that like in SAC, the target policy used in the actor loss for state s can be defined as g(a | s) = exp( 1 α Qϕ(s, a) − log Z(s)), in which Z is the partition function for normalizing the distribution. We can prove that the actor loss in implicit regularization can be rewritten with a KL regularization term: Proposition 4.1. (B.1) Assume that the critic is invariant with respect to transformations in FT , i.e., Qϕ(fτ (s), a) = Q(s, a) for all τ ∈ T , a ∈ A. For any random variable µ, the actor loss in implicit regularization (Equation 6) can be rewritten: ℓ I θ(s, µ) = ˆEµ[ ∫ a πθ(a | fµ(s)) log πθ,sg(a | s) g(a | s) ] + ˆEµ[DKL(πθ(· | fµ(s)) ∥ πθ,sg(· | s) )]. (8) Intuitively, with the actor loss in implicit regularization, the policy πθ(· | fµ(s)) for each augmented state is trained to get close to its target policy g(· | fµ(s)), which is completely defined by the critic. When the invariance of the critic has been learned, the πθ(· | fµ(s))’s for different augmented states are actually trained to get close to the same target policy, which then induces policy invariance. Interestingly, the direction of the KL divergences in Equation 4 and Equation 8 is reversed with respect to the stop gradient. The KL divergence in Equation 4 should be preferred because the other one also implicitly maximizes the policy entropy (see Appendix D), which leads to less stable training, as we have observed empirically. Since policy invariance is only really enforced when the critic is already invariant and that the implicitly enforced KL divergence has opposite direction, it seems preferable to directly enforce policy invariance like in explicit regularization (Equation 4). Following the idea of using averaged target in the critic loss, another interesting question about applying a KL regularization is whether to use a policy πθ,sg(· | fη(s)) of a transformed state as the target: ℓ E θ (s, µ) = α log πθ(ˆa | s) − Qϕ(s, ˆa)+απ ˆEµ[ˆEη[ DKL(πθ,sg(· | fη(s)) ∥ πθ(· | fµ(s))]], (9) where η and µ follow the same distribution. Intuitively, the regularization term in this equation enforces the invariance across policies of different transformed states and somehow is equivalent to using an averaged policy as the target. As shown in Appendix C.3, the loss in Equation 9 is an upper bound of the loss with an averaged policy πavg(· | s) = ˆEη[πθ,sg(· | fη(s))] as the target: ℓ E θ (s, µ) = α log πθ(ˆa | s) − Qϕ(s, ˆa)+απ ˆEµ[ DKL(πavg(· | s) ∥ πθ(· | fµ(s))) ]. (10) So using Equation 9 is somehow equivalent to using an averaged policy as the target in the KL regularization term and thus has an effect of smoothing, which may be preferred. 5 PRINCIPLED DATA-AUGMENTED OFF-POLICY ACTOR-CRITIC ALGORITHM Following these analyses, we finally propose our generic algorithm and provide a justification for it. 5.1 GENERIC ALGORITHM Formally, our generic algorithm follows the structure in Algorithm 1 but with the following (empir- ical) critic and actor losses, ℓϕ(s, a, r, s′, ν, µ) and ℓθ(s, µ), for a single transition (s, a, r, s ′) 3: ℓϕ = nT∑ i=1 αi ˆEνi [(Qϕ(fνi(s), a)−ˆEµ[y(fµ(s ′), a ′)] )2] + αtp ˆEµ[∇µQϕ(fµ(s), a)] and (11) ℓθ = ˆEµ[α log πθ(ˆa | fµ(s))−Qϕ(fµ(s), ˆa)+απ ˆEη[ DKL(πθ,sg(· | fη(s)) ∥ πθ(· | fµ(s))]], (12) 3To simplify notations, we drop the parameters of the losses when there is no risk of confusion. 5 Published as a conference paper at ICLR 2024 where nT is the number of types of image transformations used in the training, fνi (resp. fµ, fη) corresponds to the transformation parameterized by random variable νi (resp. µ, η) and αi (resp. αtp, απ) is the coefficient of the mean-squared errors for different image transformations (resp. tangent prop term, KL regularization term). Note that η and µ follow the same distribution. 5.2 JUSTIFICATION FOR THE GENERIC ALGORITHM In this section, we justify further the formulation of our generic algorithm by analyzing first the effects of applying different image transformations in calculating the target Q-values and then the estimation of the empirical actor/critic losses under data augmentation. While the variances of these estimations are unavoidably increased when random image transformations are applied, they can be controlled by several techniques, as we discuss below. Recall that a lower variance of an empirical loss implies a lower variance of its corresponding stochastic gradient, which leads to less noisy updates and may therefore stabilize training. Moreover, a lower variance of target Q-values entails a lower variance of the empirical critic loss, which then similarly leads to more stable training. Applying image transformations in calculating the target A simple technique to reduce the variance of an estimator is to use more samples. Recall that in contrast to RAD, DrQ leverages more augmented samples and introduces the average target, which can be interpreted as using more augmented samples for estimating the target Q-values. In Appendix E.1, we show that DrQ enjoys a smaller variance of the empirical critic loss and the target Q-values than RAD, which may explain the empirical better performance of DrQ (see e.g., (Kostrikov et al., 2020)). However, when facing complex image transformations, Hansen et al. (2021) propose to avoid using them in calculating the target Q-values to reduce the variance of the target and thus stabilize the training. Interestingly, we observe experimentally that even using complex image transformation such as random conv in the target does not induce a large variance in the target while a much larger bias is observed for the trained critic, as shown in Appendix F. Compared to using random shift which is a finite set of image transformations, using random conv actually applies an infinite set of transformations. To maintain the invariance among augmented states, more updates are required to train the agent sufficiently. To validate the idea that the invariance with respect to complex image transformations is harder to learn and more updates are required when using them in calculating the target, we evaluate increasing the number of updates in each iteration when using different image transformations in calculating the target, as shown in Appendix F. From the experimental results, we observe that the cosine similarity between the augmented features at early stage (e.g., 100k training steps) could be used as a criterion for judging if an image transformation is complex or not. Moreover, for those complex image transformations, updating more is helpful when we apply them in calculating the target Q-values. Adding KL regularization The analysis in Section 4.2 suggests that explicitly adding a KL regu- larization term, Dη,µ = DKL(πθ(· | fη(s)) ∥ πθ(· | fµ(s)) with η and µ following the same distribu- tion over FT , in the actor loss can help better learn the invariance of the actor. Thus, we define the actor loss in the generic algorithm as the sum of the actor loss in implicit regularization (Equation 6) and this KL regularization term. Below, we provide two additional justifications for this choice. Firstly, we show in the following proposition that the variance of the actor loss in implicit regu- larization can be controlled by a KL divergence if the invariance is already learned for critic Qϕ. Hence, adding a KL regularization term in the actor loss may both enforce invariance and reduce the variance of the implicit actor loss. Proposition 5.1. (E.2) Assuming that critic Qϕ is invariant with respect to transformations in FT , we have: Vµ[ℓ I θ(s, µ)] ≤ 1 n Eµ[( Eη[Dη,µ + c(fν(s)) √2Dη,µ] )2] , (13) where c(fν(s)) > 0 and n is the number of samples to estimate the empirical mean ℓ I θ(s, µ). In addition, under invariance of the target critic Q ¯ϕ, we can prove that the variance of target Q-values can be controlled by a KL divergence. Therefore, training with a KL regularization term may further reduce the variance of the target, which would also consequently reduce the variance of the critic loss. 6 Published as a conference paper at ICLR 2024 Proposition 5.2. (E.3) Assuming that target critic Q ¯ϕ is invariant with respect to transformations in FT , we have Vµ[ ˆY (s ′, µ)] ≤ 1 n Eµ[( Eη[ max a′ (y(fµ(s ′), a ′) − r) √2Dη,µ + α · Dη,µ])2] (14) where target ˆY (s ′, µ) = ˆEµ[Ea′∼πθ(·|fµ(s′))[y(fµ(s ′), a ′)]], y(fµ(s ′), a ′) = r + γQ ¯ϕ(fµ(s ′), a ′) − α log π(a ′|fµ(s ′)) and n is the number of samples to estimate the empirical mean ˆY (s ′, µ). Although those results are obtained under invariance of the (current or target) critics, we may expect an approximate reduction of the variances by adding a KL regularization term, when the critics are approximately invariant. Experimentally, we can observe that the variance of target and critic losses can be reduced thanks to this KL regularization term. Tangent Prop We now introduce an additional regularization to promote the invariance of the critic. This addition is motivated by our previous analysis, which indicates that the critic invari- ance is an important factor for stabilizing training. This additional regularization term is based on tangent prop (Simard et al., 1991), which was proposed for computer vision tasks, to promote in- variance by enforcing that the gradient of the trained model with respect to the magnitude of image transformation be zero. In DRL, when applied to the critic, it can be formulated as follows. If the invariance of Q-function over the whole set of the image transformation parameter τ is required and the Q-function is differentiable with respect to τ , tangent prop regularization is actually adding the constraints on the derivative of the Q-function with respect to τ : ∂Q(fτ (s), a) ∂τ = ∂Q(fτ (s), a) ∂fτ (s) ∂fτ (s) ∂τ ≈ ∂Q(fτ (s), a) ∂fτ (s) fτ +δτ (s) − fτ (s) δτ = 0. (15) Although the computational cost is increased due to the differentiation of Q, this regularization is still beneficial since sample efficiency is often one of main concerns when applying DRL. Compared to the original tangent prop, we also extend it to a broader application by applying it not only on the original state but also on the transformed states. Specifically, instead of only calculating the derivative around τ0, the derivative is estimated at any τ ∈ T . A theoretical justification for how tangent prop regularization is related to the training of the critic can be found in Appendix B.2. 6 EXPERIMENTAL RESULTS In order to validate our theoretical analysis and show the effectiveness of our proposed algorithm, we perform a series of experiments to (1) experimentally validate our propositions, (2) conduct a case study explicitly showing the statistics we analyzed, (3) compare our final proposed algorithm with state-of-the-art baselines (RAD, DrAC, DrQ, DrQv2, SVEA) to verify its sample efficiency, and evaluate its generalization ability against SVEA, which was specifically-designed for this purpose. Experimental Set Up We evaluate different methods on environments from DeepMind Control Suite (Tassa et al., 2018) with normal background for evaluating sample efficiency and distracted background for evaluating generalization ability. DeepMind Control Suite is a commonly used benchmark for evaluating methods of applying data augmentation in DRL. Across different envi- ronments, all hyperparameters are listed in Appendix G such as learning rates and batch size for the actor and critic. All experiments are performed with 5 different random seeds and the agent is evaluated every 10k environment steps, whose performance is measured by cumulative rewards averaged over 10 evaluation episodes. Ablation study To validate our propositions, we first compare RAD [M=1,K=1], RAD+ [M=2,K=1] and DrQ [M=2,K=2] using the official implementation from DrQ 4, where [M, K] are respectively the numbers of samples used for estimating the empirical mean over random variables [ν, µ] in the critic loss. We then show the effectiveness of KL regularization and tangent prop reg- ularization by adding them one by one based on DrQ. All these methods are trained with normal 4https://github.com/denisyarats/drq 7 Published as a conference paper at ICLR 2024 Figure 1: Aggregated results of validating our propositions. Figure 2: Performance of different methods in walker run environment. Methods Std of critic loss Std of target Q Std of actor loss KL RAD 0.772 ±0.359 0.321 ±0.084 4.971 ±1.044 0.266 ±0.015 RAD+ (a) 0.271 ±0.043 0.234 ±0.031 5.011 ±0.463 0.248 ±0.021 DrQ 0.358 ±0.123 (a) 0.225 ±0.055 5.077 ±0.964 0.293 ±0.055 DrQ+KL(fix target) 0.384 ±0.042 0.221 ±0.007 5.708 ±1.259 0.135 ±0.009 DrQ+KL 0.319 ±0.053 (b) 0.183 ±0.018 (b) 4.469 ±0.745 (b) 0.099 ±0.010 ours (c) 0.280 ±0.004 (c) 0.182 ±0.001 4.815 ±0.412 0.101 ±0.007 Table 1: Important statistics of different methods in walker run environment. (a)(b)(c) are discussed in the main text. background using random shift as the image transformation and also evaluated with normal back- ground. When applicable, we adopt hyperparameters from (Kostrikov et al., 2020) except that we reduce the batch size from 512 to 256 (to make the algorithms easier to run on our computing device, equipped with one NVIDIA RTX 3060 GPU and Intel i7-10700 CPU). Considering that only one image transformation (random shift) is applied, the weight αi for it is set as 1 in Equation 11. We tune the weights αKL and αtp based on a quick grid search in {0.1, 0.5, 1.0} and finally choose 0.1 for both αKL and αtp. The results of validating our proposition are shown in Figure 1. We follow the recommendations from Agarwal et al. (2021) to plot the aggregated performance over totally 9 environments. Detailed training curves in each environments are included in Appendix H.1. Case study We conduct a comprehensive evaluation in walker-run using the same setting as above to answer the following questions: 1) Does our proposition help reduce the variance we are con- cerned about? 2) Does our proposition help learn the invariance in the feature space? Moreover, we run the experiment of using a fixed target in the KL regularization to validate our analysis in Section 4.2. Note that the set of parameters T is finite when using random shift as image transformation. So, augmented Q-values Qϕ(fτ (s), a) and augmented target Q-values Q ¯ϕ(fτ ′(s ′), a ′) for all τ, τ ′ ∈ T are recorded within a mini-batch for every 10k environment steps. These are used to calculate the standard deviation of the empirical critic loss and the target Q-values. The KL divergence between policies for two augmented samples is also recorded with the same frequency. The results for this case study are shown in Figure 2 and Table 1. From the table, we can see (a) the variance of the critic loss and the variance of the target Q decrease thanks to more augmented samples, (b) KL regularization helps enforce the invariance in the actor and reduce the variance of the target and the variance of the actor loss, (c) tangent prop further improves learning the invariance in the critic. Meanwhile, the feature vectors of augmented samples within a mini-batch after the encoder from the actor and critic are recorded with the same frequency. To estimate the invariance in the feature space, we first calculate the cosine similarities between the augmented features. We also apply t- SNE (van der Maaten & Hinton, 2008) to project the latent features into 2D space and calculate the L2-distance between the projected points. With these measures, the learned invariance of the actor and critic features along the training are shown in Appendix H.2. Following our proposition, the agent can learn well the invariance in the feature spaces of both the actor and critic. From another view, this achieves the same goal as using the self-supervised learning losses (Srinivas et al., 2020; Grill et al., 2020) to explicitly enforce the invariance in the latent space. 8 Published as a conference paper at ICLR 2024 Figure 3: Partial results of evaluating sample efficiency for our methods. Environments SVEA ours walker walk 449.52±60.33 508.28±79.82 walker stand 788.80±89.31 843.27±51.33 cartpole swingup 351.96±62.36 366.77±58.08 ball in cup catch 445.04±145.42 569.50±142.99 finger spin 338.35±77.65 447.99±73.52 Table 2: Comparison of SVEA and ours in DMControl with video-hard backgrounds for training 500k env steps. Methods Std of critic loss Std of target Q KL SVEA 5.780 ±1.029 0.936 ±0.097 0.404 ±0.019 ours 3.197 ±0.760 0.530 ±0.038 0.205 ±0.034 Table 3: Statistics for the trained model of SVEA and our method using random overlay in walker walk en- vironment. Main result We evaluate our proposed methods in sample efficiency and generalization ability. For sample efficiency, we evaluate our method (ours) against state-of-the-art baselines: RAD, DrQ, DrAC (with SAC as the base algorithm), DrQv2 (with DDPG as the base algorithm) and SVEA (with random overlay). The experimental results shown in Figure 3 confirm that our method outperforms previous ones. Due to the page limit, the additional evaluations are included in Appendix H.3. For generalization, we compare SVEA with our method based on the official implementation of SVEA 5. Considering that random overlay shows the best generalization ability among the image transformations listed in SVEA, our comparison also includes random overlay. When applicable, we adopt the same hyperparameter values as in SVEA. Thus, we use the same αi = 0.5 for random shift and random overlay, reuse αKL = 0.1 from previous experiments and tune αtp based on a quick grid search in {0.1, 0.5} and finally choose 0.5 for αtp. Both methods (SVEA and ours) are trained in normal background for 500k environment steps, and evaluated in video-hard backgrounds. The final results comparing SVEA and our method are shown in Table 2. We can see the improvement in generalization especially in environments such as ball in cup catch, finger spin, and walker walk. Meanwhile, similar statistics we discussed before are recorded, as shown in Table 3. The complete curves for evaluations and the recorded statistics are listed in Appendices H.4 and H.5. 7 CONCLUSION We revisit state-of-the-art data augmentation methods in DRL. Our theoretical analysis helps under- stand and compare different existing methods. In addition, this analysis provides recommendations on how to exploit data augmentation in a more theoretically-motivated way. We included tangent prop, a regularization term to promote invariance, which is novel in DRL. We validated our proposi- tions and evaluated our method in DeepMind control tasks with normal background for comparing sample efficiency and distracted background for comparing generalization ability with the state-of- the-art methods. Limitations are discussed in Appendix I due to the page limit. 5https://github.com/nicklashansen/dmcontrol-generalization-benchmark 9 Published as a conference paper at ICLR 2024 Reproducibility For the experimental results, the code with comments on how to reproduce the results is released, the experimental settings are described in the main paper (Section 6) and the hyperparameters required for reproducing the results are recorded in Appendix G. For the theoretical results, all the detailed proofs can be found in the appendix. Acknowledgments This work has been supported in part by the program of National Natural Science Foundation of China (No. 62176154). REFERENCES Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C. Courville, and Marc G. Belle- mare. Deep reinforcement learning at the edge of the statistical precipice. CoRR, abs/2108.13264, 2021. URL https://arxiv.org/abs/2108.13264. Randall Balestriero, Ishan Misra, and Yann LeCun. A data-augmentation is worth a thou- sand samples: Analytical moments and sampling-free training. In S. Koyejo, S. Mo- hamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural In- formation Processing Systems, volume 35, pp. 19631–19644. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/ file/7c080cab957edab671ac49ae11e51337-Paper-Conference.pdf. Karl Cobbe, Oleg Klimov, Christopher Hesse, Taehoon Kim, and John Schulman. Quantifying generalization in reinforcement learning. CoRR, abs/1812.02341, 2018. URL http://arxiv. org/abs/1812.02341. Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo ´Avila Pires, Zhaohan Daniel Guo, Mohammad Ghesh- laghi Azar, Bilal Piot, Koray Kavukcuoglu, R´emi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-supervised learning. CoRR, abs/2006.07733, 2020. URL https://arxiv.org/abs/2006.07733. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. CoRR, abs/1801.01290, 2018. URL http://arxiv.org/abs/1801.01290. Nicklas Hansen, Hao Su, and Xiaolong Wang. Stabilizing deep q-learning with convnets and vision transformers under data augmentation. CoRR, abs/2107.00644, 2021. URL https://arxiv. org/abs/2107.00644. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9726–9735, 2020. doi: 10.1109/CVPR42600.2020.00975. Ilya Kostrikov, Denis Yarats, and Rob Fergus. Image augmentation is all you need: Regulariz- ing deep reinforcement learning from pixels. CoRR, abs/2004.13649, 2020. URL https: //arxiv.org/abs/2004.13649. Teerath Kumar, Alessandra Mileo, Rob Brennan, and Malika Bendechache. Image data augmenta- tion approaches: A comprehensive survey and future directions, 2023. Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforcement learning with augmented data. CoRR, abs/2004.14990, 2020. URL https: //arxiv.org/abs/2004.14990. Kimin Lee, Kibok Lee, Jinwoo Shin, and Honglak Lee. A simple randomization technique for generalization in deep reinforcement learning. CoRR, abs/1910.05396, 2019. URL http:// arxiv.org/abs/1910.05396. Tingguang Li, Weitao Xi, Meng Fang, Jia Xu, and Max Q.-H. Meng. Learning to solve a rubik’s cube with a dexterous hand. In 2019 IEEE International Conference on Robotics and Biomimetics (ROBIO), pp. 1387–1393, 2019. doi: 10.1109/ROBIO49542.2019.8961560. 10 Published as a conference paper at ICLR 2024 Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Manfred Otto Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. CoRR, abs/1509.02971, 2015. URL https://api.semanticscholar.org/ CorpusID:16326763. Yijiong Lin, Jiancong Huang, Matthieu Zimmer, Yisheng Guan, Juan Rojas, and Paul Weng. In- variant transform experience replay: Data augmentation for deep reinforcement learning. IEEE Robotics and Automation Letters and IROS, 2020. Sicong Liu, Xi Sheryl Zhang, Yushuo Li, Yifan Zhang, and Jian Cheng. On the data-efficiency with contrastive image transformation in reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=-nm-rHXi5ga. Guozheng Ma, Zhen Wang, Zhecheng Yuan, Xueqian Wang, Bo Yuan, and Dacheng Tao. A com- prehensive survey of data augmentation in visual reinforcement learning, 2022. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR, abs/1312.5602, 2013. URL http://arxiv.org/abs/1312.5602. Roberta Raileanu, Maxwell Goldstein, Denis Yarats, Ilya Kostrikov, and Rob Fergus. Automatic data augmentation for generalization in reinforcement learning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=FChSjfcJZVW. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/ 1707.06347. Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bach- man. Data-efficient reinforcement learning with self-predictive representations. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=uCQfPZwRaUu. Patrice Simard, Bernard Victorri, Yann Le Cun, and John Denker. Tangent prop: A formalism for specifying selected invariances in an adaptive network. In Proceedings of the 4th International Conference on Neural Information Processing Systems, NIPS’91, pp. 895–903, San Francisco, CA, USA, 1991. Morgan Kaufmann Publishers Inc. ISBN 1558602224. Aravind Srinivas, Michael Laskin, and Pieter Abbeel. CURL: contrastive unsupervised representa- tions for reinforcement learning. CoRR, abs/2004.04136, 2020. URL https://arxiv.org/ abs/2004.04136. Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Mar- tin A. Riedmiller. Deepmind control suite. CoRR, abs/1801.00690, 2018. URL http: //arxiv.org/abs/1801.00690. Manan Tomar, Utkarsh Aashu Mishra, Amy Zhang, and Matthew E. Taylor. Learning representa- tions for pixel-based control: What matters and why?, 2022. URL https://openreview. net/forum?id=Ti2i204vZON. Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Ma- chine Learning Research, 9(86):2579–2605, 2008. URL http://jmlr.org/papers/v9/ vandermaaten08a.html. Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous con- trol: Improved data-augmented reinforcement learning. CoRR, abs/2107.09645, 2021. URL https://arxiv.org/abs/2107.09645. Zhecheng Yuan, Guozheng Ma, Yao Mu, Bo Xia, Bo Yuan, Xueqian Wang, Ping Luo, and Huazhe Xu. Don’t touch what matters: Task-aware lipschitz data augmentation for visual reinforcement learning, 2022. 11 Published as a conference paper at ICLR 2024 A BACKGROUNDS Deep Deterministic Policy Gradient Incorporating a parameterized actor function µθ(s), Deep Deterministic Policy Gradient uses the following actor and critic and loss to train the agent: Jπ(θ) = ˆEst∼D[−Qϕ(st, at)|at=µθ(st)], JQ(ϕ) = ˆEst∼D[(Qϕ(st, at) − ˆQ(st, at))2|at=µθ(st)], (16) where ˆQ(st, at) = rt + γQ ¯ϕ(st+1, µ¯θ(st+1)), which is the target Q-value defined from a target network, θ and ϕ represents the parameters of the actor and the critic respectively, ¯θ and ¯ϕ represents the parameters of the target actor and the target critic respectively, and D represents the replay buffer. The weights of a target network are the exponentially moving average of the online network’s weights. Soft Actor-Critic Maximum entropy RL tackles an RL problem with an alternative objective func- tion, which favors more random policies: J = ˆEπ[∑∞ t=0 γtrt + αH(π(· | st))], where γ is the dis- count factor, α is a trainable coefficient of the entropy term and H(π(· | st)) is the entropy of action distribution π(· | st). The Soft Actor-Critic (SAC) algorithm (Haarnoja et al., 2018) optimizes it by training the actor πθ and critic Qϕ with the following respective losses: Jπ(θ) = ˆEst∼D,a∼π[α log πθ(a | st) − Qϕ(st, a)], JQ(ϕ) = ˆEst,at∼D[(Qϕ(st, at) − ˆQ(st, at)) 2], (17) where ˆQ(st, at) = rt + γQ ¯ϕ(st+1, at+1) − α log πθ(at+1|st+1), which is the target Q-value defined from a target network and at+1 ∼ πθ(· | st+1), θ ,ϕ and ¯ϕ represents the parameters of the actor, the critic and the target critic respectively, and D represents the replay buffer. The weights of a target network are the exponentially moving average of the online network’s weights. Reinforcement Learning with Augmented Data Reinforcement Learning with Augmented Data (RAD) (Laskin et al., 2020) applies data augmentation in SAC by replacing the original observation with augmented observations in the training of the actor and critic. Given image transformation fν, the actor and critic losses are Jπ(θ) = ˆEst∼D,a∼π[α log πθ(a | fν(st)) − Qϕ(fν(st), a)], JQ(ϕ) = ˆEst,at∼D[(Qϕ(fν(st), at) − ˆQ(fν(st), at))2], (18) Data-Regularized Q Data-regularized Q (DrQ) (Kostrikov et al., 2020) extends RAD by using data augmentation in the training of the critic in two new ways. Given a type of image transformation f parameterized by ν, data augmentation is first applied in the calculation of the target Q-value for every transition (s, a, r, s′): y = r + γ 1 K K∑ k=1 Q ¯ϕ(fτk (s ′), a ′), where a ′ ∼ π(· | fτk (s ′)). (19) Q ¯ϕ is the slowly updated target network. Then the critic is updated with different augmented s and this averaged target: ℓQ(ϕ) = 1 N M N∑ i=1 M∑ m=1 (Qϕ(fτm (s), a) − y)2. (20) Note that DrQ recovers RAD when M = 1 and K = 1. SVEA In order to avoid non-deterministic Q-target and over-regularization, Hansen et al. (2021) propose using state without complex augmentation for calculating the target. Let T1 and T2 be a set of random shift and a set of random shift plus one of the data augmentation mentioned in the paper such as random convolution (Lee et al., 2019). The critic loss used for training is LQ(ϕ) = 1 N N∑ i=1 αsvea(Qϕ(fτ1,i(s), a) − yi)2 + βsvea(Qϕ(fτ2,i(s), a) − yi) 2, (21) 12 Published as a conference paper at ICLR 2024 where αsvea and βsvea are constant coefficients for naively and complexly augmented data respec- tively, τ1,i ∈ T1 and τ2,i ∈ T2, yi = r + γQ ¯ϕ(τ1,i(s ′), a ′) + α log π(a ′ | fτ1,i(s ′)), where a ′ ∼ π(· | τ1,i(s ′)). DrAC Instead of directly replacing the original samples with augmented samples in the training, Raileanu et al. (2021) use two regularization terms in the training of the actor and critic to explicitly enforce the invariance. When applying it in the PPO algorithm (Schulman et al., 2017) to learn a state-value estimator Vϕ(s) and a policy πθ(s), the regularization terms are GV = ( ˆV (s) − Vϕ(fν(s))) 2, Gπ = DKL[πθ(a | s) | πθ(a | fν(s)]. (22) where ˆV (s) is the sum of rewards collected by the agent after state s and ν is the random variable for parameterizing the image transformation. Tangent Prop Regularization Tangent prop (Simard et al., 1991) is a regularization term used for learning invariance for a function G(s) with respect to a small image transformation parameterized by α on s: ∑ ∥ ∥ ∥ ∥ ∂G(s, α) ∂α ∥ ∥ ∥ ∥ 2 = 0 (23) B EXPECTED LOSS UNDER DATA AUGMENTATION B.1 ACTOR LOSS SAC as base algorithm For image-based control tasks, a data augmentation f parameterized by µ over T is applied on the observations. The actor loss with implicit regularization for the state s in a transition is ℓ I θ(s, µ) = ˆEµ[α log πθ(ˆa | fµ(s)) − Qϕ(fµ(s), ˆa) |ˆa∼πθ(·|fµ(s)) ] = ˆEµ[DKL(πθ(·|fµ(s))|| exp( 1 α Qϕ(fµ(s), ·) − log Z(fµ(s))) )] (24) Let g(fµ(s), ·) = exp( 1 α Qϕ(fµ(s), ·) − log Z(fµ(s))). ℓ I θ(s, µ) = ˆEµ[ DKL(πθ(·|fµ(s))||g(fµ(s), ·) )] = ˆEµ[DKL(πθ(·|fµ(s))||g(fµ(s), ·)) ) − DKL( πθ(·|fµ(s))||g(s, ·) ) + DKL(πθ(·|fµ(s))||g(s, ·))] = ˆEµ[ ∫ a πθ(a|fµ(s)) log πθ(a|fµ(s)) g(a|fµ(s)) − ∫ a πθ(a|fµ(s)) log πθ(a|fµ(s)) g(a|s) ] + ˆEµ[ DKL(πθ(·|fµ(s))||g(s, ·))] = ˆEµ[ ∫ a πθ(a|fµ(s)) log g(a|s) g(a|fµ(s)) ] + ˆEµ[DKL( πθ(·|fµ(s))||g(s, ·) )] = ˆEµ[ ∫ a πθ(a|fµ(s)) log g(a|s) g(a|fµ(s)) ] + ˆEµ[DKL( πθ(·|fµ(s))||g(s, ·) ) − DKL(πθ(·|fµ(s))||πθ,sg(·|s)) + DKL(πθ(·|fµ(s))||πθ,sg(·|s) )] = ˆEµ[ ∫ a πθ(a|fµ(s)) log g(a|s) g(a|fµ(s)) ] + ˆEµ[ ∫ a πθ(a|fµ(s)) log πθ,sg(a|s) g(a|s) ] + ˆEµ[ DKL(πθ(·|fµ(s))||πθ,sg(·|s) )]. (25) 13 Published as a conference paper at ICLR 2024 If the invariance in the critic has been learned: Q(fµ(s), a) = Q(s, a) for all a ∈ A, (26) the actor loss with implicit regularization becomes ℓ I θ(s, µ) = ˆEµ[ ∫ a πθ(a|fµ(s)) log πθ,sg(a|s) g(a|s) ] + ˆEµ[DKL(πθ(·|fµ(s))||πθ,sg(·|s))] , (27) because g(a|fµ(s)) = g(a|s) for all a ∈ A. (28) If the actor is well learned for state s, this actor loss become ℓ I θ(s, µ) = ˆEµ[ DKL(πθ(·|fµ(s))||πθ,sg(·|s))] , (29) because g(a|s) = πθ,sg(a|s) for all a ∈ A. (30) DDPG as base algorithm For image-based control tasks, a data augmentation f parameterized by µ over T is applied on the observations. Considering that the Q-invariant transformation is also π∗-invariant, training all policies of the transformed states to get close to the same optimal policy is equivalent to training the policy of original state and enforce the invariance in the policy. Considering actor loss with implicit regularization, we can apply a Taylor expansion with respect to the optimal action π∗(fµ(s)) = π∗(s) = arg maxa Qϕ(s, a): ℓ I θ(s, µ) = ˆEµ[ − Qϕ(fµ(s), ˆa) |ˆa=πθ(fµ(s)) ] = −ˆEµ[Qϕ(fµ(s), π∗(fµ(s)) + J(ˆa − π∗(fµ(s))) + 1 2 (ˆa − π∗(fµ(s))) T H(ˆa − π∗(fµ(s))) + o(∥ˆa − π∗(fµ(s))∥2)|ˆa=πθ(fµ(s))] ≈ − 1 2 ˆEµ[(ˆa − π∗(fµ(s))) T H(ˆa − π∗(fµ(s)))|ˆa=πθ(fµ(s))] = − 1 2 ˆEµ[(ˆa − πθ,sg(s) + πθ,sg(s) − π∗(fµ(s))) T H (ˆa − πθ,sg(s) + πθ,sg(s) − π∗(fµ(s)))|ˆa=πθ(fµ(s))] = − 1 2 ˆEµ[(ˆa − πθ,sg(s)) T H(ˆa − πθ,sg(s)) + (πθ,sg(s) − π∗(fµ(s))) T H(πθ,sg(s) − π∗(fµ(s))) + 2(ˆa − πθ,sg(s))T H(πθ,sg(s) − π∗(fµ(s)))|ˆa=πθ(fµ(s))] . (31) The first term above is enforcing the invariance of the actor with respect to the transformation. B.2 CRITIC LOSS B.2.1 LINEAR MODEL According to the analysis by Balestriero et al. (2022), the expected Mean Squared Error (MSE) under data augmentation for a linear regression model can be expressed by the expectation and variance of the transformed images. Now we want to derive a similar regularization term from the critic loss. If we use linear model for the critic and actor: Q(s, a) = Ws ∗ s + Wa ∗ a + b0 (32) ¯Q(s, a) = ¯Ws ∗ s + ¯Wa ∗ a + ¯b0 (33) 14 Published as a conference paper at ICLR 2024 π(s) = Wτ ∗ s + ϵWσ ∗ s + b1, ϵ ∼ N (0, 1) (34) in which Ws ∈ R1∗|S|, Wa ∈ R1∗|A|, Wτ ∈ R1∗|S|, Wσ ∈ R1∗|S| and b0, b1 are parameters for the model. ¯Q is the exponential moving average of Q. The critic loss for a transition (s, a, r, s ′) under data augmentation ν ∼ P and µ ∼ P ′ for state s and next state s ′ is ℓϕ = Eν[( Q(fν(s), a) − ˆEµ[y])2] = Eν[ Q(fν(s), a) 2] − 2Eν[ Q(fν(s), a) ]ˆEµ[y] + ˆEµ[y]2) (35) in which y = r + γ ¯Q(fµ(s ′), a ′) − α log π(a ′|fµ(s ′))|a′∼π(·|fµ(s′)). (36) Considering the last term is not used to update Q, we only need to focus on the first two terms. Expectation Eν[ Q(fν(s), a) ] = Eν[Wsfν(s) + Waa + b0] = WsEν[fν(s)] + Waa + b0 = Q(Eν[fν(s)], a) (37) Variance Eν[Q(fν(s), a)2] = Eν[(Wsfν(s) + Waa + b0)2] = Eν[f T (s, ν)W T s Wsfν(s) + (Waa + b0) 2 + 2(Waa + b0) (Wsfν(s) )] = Eν[T r(W T s Wsfν(s)f T (s, ν))] + (Waa + b0)2 + 2(Waa + b0)(WsEν[fν(s)] ) = T r(W T s WsEν[fν(s)f T (s, ν) ]) + (Waa + b0)2 + 2(Waa + b0)(WsEν[fν(s)] ) = T r(W T s Ws(Eν[ fν(s)f T (s, ν) ] − Eν[ fν(s) ]Eν[f T (s, ν)])) + T r(W T s WsEν[fν(s) ] Eν[ f T (s, ν) ]) + (Waa + b0)2 + 2(Waa + b0)(WsEν[fν(s)] ) = T r(W T s WsVν[fν(s)] ) + Q(Eν[fν(s)], a) 2 (38) Whole loss ℓϕ = N∑ i=1 Eν[ Q(fν(s), a) 2] − 2Eν[ Q(fν(s), a) ]ˆEµ[y] + ˆEµ[y]2 = N∑ i=1 T r(W T s WsVν[fν(s)] ) + Q(Eν[fν(s)], a) 2 − 2ˆEµ[y]Q(Eν[Tν(s)], a) + ˆEµ[y]2 = N∑ i=1 (Q (Eν[fν(s)], a ) − ˆEµ[y])2 + T r(W T a WaVν[fν(s)] ) (39) B.2.2 NON-LINEAR MODEL According to the analysis by Balestriero et al. (2022), the expected loss of transformed state has an upper bound related to the variance of the transformed state: E[(ℓ ◦ Q)(f (x))] ≤ (ℓ ◦ Q)(E[f (x)]) + κ(x)∥J Q(E[f (x)])H(x)Λ(x) 1 2 ∥2 F , (40) in which variance of the transformed image can be decomposed into V[f (x)] = H(x)Λ(x)H(x) T . (41) The second term in the RHS of Equation 40 recovers tangent prop regularization. 15 Published as a conference paper at ICLR 2024 C EXPLICIT VS IMPLICIT REGULARIZATION C.1 CRITIC LOSS IN EXPLICIT REGULARIZATION For τ = (s, a, s′, r) sampled from the replay buffer D, given current estimation Qϕ and true estima- tion Q ∗ without error, the bias of the target Ea′∼π(s′)y(s ′, a ′) is smaller than the target Qϕ,sg(s, a): Eτ ∼D[(Ea′∼π(·|s′)[y(s ′, a ′)] − Q ∗(s, a))2] =Eτ ∼D[(Ea′∼π(·|s′)[r + γQ ¯ϕ(s ′, a ′) − α log π(a ′|s ′)] − (r + γEa′∼π(·|s′)[Q ∗(s ′, a ′) − α log π(a ′|s ′)] )2] =Eτ ∼D[(γEa′∼π(·|s′)[Q ¯ϕ(s ′, a ′) − Q ∗(s ′, a ′)] )2] ≈γ2Eτ ∼D[( Ea′∼π(·|s′)[Qϕ,sg(s ′, a ′) − Q ∗(s ′, a ′)] )] <Eτ ∼D[(Ea′∼π(·|s′)[Qϕ,sg(s ′, a ′) − Q ∗(s ′, a ′)] )2] <Eτ ∼D,a′∼π(·|s′)[(Qϕ,sg(s ′, a ′) − Q ∗(s ′, a ′))2] =Eτ ∼D[(Qϕ,sg(s, a) − Q ∗(s, a) )2] (42) The bias of using a target ¯y in the explicit regularization is ϵ(¯y) =Eτ [((Qϕ(fν(s), a) − ¯y)2 − (Qϕ(fν(s), a) − Q ∗(s, a) )2)2] =Eτ [(2Qϕ(fν(s), a)(Q ∗(s, a) − ¯y) + ¯y2 − Q ∗(s, a)2)2]. (43) We only need to consider the first term 2Qϕ(fν(s), a)(Q ∗ − ¯y), considering that other terms is constant with respect to ϕ. So the bias of using different targets in the regularization term is decided by the bias of the target compared to the true estimation. The bias of using Ea′∼π(s′)[y(s ′, a ′)] in the explicit regularization term is smaller than using Qϕ,sg(s, a) according to the equations above: ϵ(¯y = Ea′∼π(·|s′)[y(s ′, a ′)]) < ϵ(¯y = Qϕ,sg(s, a)), (44) In practice, we use the sampled value y(s ′, a ′) as the target, which leads to a smaller bias and relatively larger variance. C.2 CRITIC LOSS CONNECTION Assume given ℓ E ϕ (s, a, r, s′, ν), by appropriately setting the random variables in Equations 5, it recovers the critic loss in explicit regularization (Equation 7), as shown below. If the distributions of ˆν and ˆµ are defined as follows: P(ˆν = τ ) = { P(ν=τ )αQ+1 αQ+1 , if τ = τ0 P(ν=τ )αQ αQ+1 , if τ ̸= τ0 P(ˆµ) = { 1, if ˆµ = τ0 0, if ˆµ ̸= τ0 , (45) then we have for any sample (s, a, r, s ′): (αQ + 1)ℓ I ϕ(s, a, r, s′, ˆν, ˆµ) = ℓ E ϕ (s, a, r, s ′, ν) C.3 ACTOR LOSS Considering that the policy is parameterized as normal distribution in SAC, we first define: πθ,sg(· | fη(s)) = N (λη, σ2 η), πθ(· | fµ(s)) = N (λµ, σ2 µ) (46) 16 Published as a conference paper at ICLR 2024 For simplicity, we consider µ and η are defined over discrete set T with probability P (µ = τi) = P (η = τi) = Pi, τi ∈ T . The derivation can be easily extended to using a continuous set. πavg(· | s) = ˆEη[πθ,sg(· | fη(s))] = N (λavg, σ2 avg) = N ( ∑ i Piλτi, ∑ i P 2 i σ2 τi) (47) ˆEη[ DKL(πθ,sg(· | fη(s)) ∥ πθ(· | fµ(s))] =ˆEη[ log σµ ση + σ2 η 2σ2 µ + (λη − λµ) 2 2σ2 µ − 1 2 ] = log σµ − ∑ i Pi log στi + ∑ i Piσ2 τi 2σ2 µ + ∑ i Pi(λτi − λµ) 2 2σ2 µ − 1 2 (48) DKL(πavg(· | s) ∥ πθ(· | fµ(s)) = log σµ σavg + σ2 avg 2σ2 µ + (λavg − λµ) 2 2σ2 µ − 1 2 = log σµ − 1 2 log ∑ i P 2 i σ2 τi + ∑ i P 2 i σ2 τi 2σ2 µ + (∑ i Piλτi − λµ) 2 2σ2 µ − 1 2 (49) Comparing the two equations above, the first term and the last term are the same, and the second term is a constant with respect to the parameter θ of the actor. For the third term, it is obvious that ∑ i Piσ2 τi 2σ2 µ ≥ ∑ i P 2 i σ2 τi 2σ2 µ because Pi ≥ P 2 i , for any i. For the forth term, we have: ∑ i Pi(λτi − λµ) 2 2σ2 µ − ( ∑ i Piλτi − λµ) 2 2σ2 µ = ∑ i Pi(λ 2 τi + λ2 µ − 2λτiλµ) 2σ2 µ − λ2 µ + (∑ i Piλτi)2 − 2λµ ∑ i Piλτi 2σ2 µ = λ2 µ + ∑ i Piλ 2 τi − 2λµ ∑ i Piλτi 2σ2 µ − λ2 µ + (∑ i Piλτi)2 − 2λµ ∑ i Piλτi 2σ2 µ = ∑ i Piλ 2 τi − ( ∑ i Piλτi)2 2σ2 µ = V[λη] 2σ2 µ ≥ 0 (50) So the loss of using the policy of a transformed state as the target is an upper bound of using the average policy as the target: ˆEη[ DKL(πθ,sg(· | fη(s)) ∥ πθ(· | fµ(s))] ≥ DKL(πavg(· | s) ∥ πθ(· | fµ(s)) (51) D KL DIVERGENCE Given a transformation fν(s) on state s, considering that the KL divergence is not symmetric, we discuss the differences between two kinds of KL regularization here: DKL(πθ,sg(s))||πθ(fν(s))) and DKL(πθ(fν(s))||πθ,sg(s) ), (52) Detach First DKL(πθ,sg(s))||πθ(fν(s)) ) = ∫ a πθ,sg(a|s) log πθ,sg(a|s) πθ(a|fν(s)) = ∫ a (πθ,sg(a|s) log πθ,sg(a|s) − πθ,sg(a|s) log πθ(a|fν(s)) ) = −H(πθ,sg(s)) − ∫ a πθ,sg(a|s) log πθ(a|fν(s)) (53) 17 Published as a conference paper at ICLR 2024 Detach Second DKL(πθ(fν(s))||πθ,sg(s)) = ∫ a πθ(a|fν(s)) log πθ(a|fν(s)) πθ,sg(a|s) , = ∫ a (πθ(a|fν(s)) log πθ(a|fν(s)) − πθ(a|fν(s)) log πθ,sg(a|s) ) = −H(πθ(fν(s))) − ∫ a πθ(a|fν(s)) log πθ,sg(a|s), (54) in which H represent the entropy for a distribution. ”Detach second” introduces an entropy term for the policy of the transformed state. This regulariza- tion not only makes the policy of the augmented state and the original state close, but also maximizes the entropy of the policy of the transformed state. However, in ”detach first”, the entropy term with the sign sg is not used to update the policy. E VARIANCE UNDER DATA AUGMENTATION E.1 MORE AUGMENTED SAMPLES REDUCE THE VARIANCE OF THE CRITIC LOSS Considering one transition (s, a, r, s′) and M transformations {fτm | m = 1, ..., M }, K transfor- mation {(fτ ′ k | k = 1, ..., K} respectively on s and s ′, the Q-values and target Q-values for the transformed samples are Qm = Q(fτm(s), a), yk = r + γQ ¯ϕ(fτ ′ k (s ′), a ′) − α log π(a ′|fτ ′ k (s ′))|a′∼π(·|fτ ′ k (s′)), (55) where m ∈ {1, ..., M }, k ∈ {1, ..., K}. RAD+ loss becomes ℓRAD+ = 1 M · M∑ m=1 (Qm − yk) 2 (56) DrQ loss becomes ℓDrQ = 1 M · M∑ m=1(Qm − 1 K K∑ k=1 yk)2 = 1 M · M∑ m=1 (Q 2 m + ( 1 K K∑ k=1 yk)2 − 2Qm · 1 K K∑ k=1yk) = 1 M · M∑ m=1Q 2 m + 1 M · M∑ m=1 ( 1 K K∑ k=1 yk) 2 − 1 M · M∑ m=1 2Qm · 1 K K∑ k=1 yk = 1 M · M∑ m=1Q 2 m + 1 K 2 ( K∑ k=1 yj) 2 − 2 M · K · M∑ m=1 K∑ k=1 Qm · yk (57) 18 Published as a conference paper at ICLR 2024 If all the combinations of above Q and y values are used for estimation, the loss becomes: ℓall = 1 M · K · M∑ m=1 K∑ k=1 (Qm − yk) 2 = 1 M · K · ( M∑ m=1 K · Q 2 m + K∑ k=1 M · y2 k − 2 M∑ m=1 K∑ k=1 Qm · yk) = 1 M · M∑ m=1 Q 2 m + 1 K ( K∑ k=1 yk) 2 − 2 M · K · M∑ m=1 K∑ k=1Qm · yk (58) The second terms in both Equation 57 and 58 can be ignored because the gradients of target values yj with respect to critic parameters are stopped. Obviously, ℓall and ℓDrQ have same gradients with respect to trainable parameters of the critic. The comparison between ℓDrQ and ℓRAD+ is exactly the comparison between ℓall and ℓRAD+. For one transition in one gradient step, M · K pairs of Qm and yk are used to formulate ℓall while only M pairs of Qm and yk are used to formulate ℓRAD+. Therefore, we can find out that DrQ outperforms RAD by leveraging more augmented samples and the averaged target. These operations indeed reduce the variance of the estimated critic loss. E.2 KL REDUCES THE VARIANCE OF ACTOR LOSS SAC actor loss Given data augmentation fν|ν ∼ P on state s, if Q(fν(s), a) is invariant with respect to ν for all a ∈ A, the variance of the actor loss Vν[ℓ I θ(s, ν)] is bounded by a term that depends on the KL divergence Dη,ν = DKL(π(· | fη(s))∥π(· | fν(s))) for ν, η ∼ P: Vν[ℓ I θ(s, ν)] ≤ 1 n Eν[(Eη[Dη,ν + c(fν(s)) √2Dη,ν] )2] , (59) where c(fν(s)) > 0, n is the number of samples to estimate the empirical mean ℓ I θ(s, ν). Proof. For image-based control tasks, a data augmentation f parameterized by ν ∼ P is applied on the observations. The actor loss of SAC becomes ℓ I θ(s, ν) = ˆEν[DKL(πθ(·|fν(s))|| exp( 1 α Qϕ(fν(s), ·) − log Z(fν(s))) )] (60) Let g(fν(s), ·) = exp( 1 α Qϕ(fν(s), ·) − log Z(fν(s))). The variance of empirical mean can be derived as the true variance divided by the number of samples n. V[ˆE[x]] = E[(ˆE(x) − E[x]) 2] = E[( 1 n (x1 − E[x] + x2 − E[x] + ... + xn − E[x])) 2] = 1 n2 n · V[x] = 1 n V[x] (61) The variance of ℓ I θ(s, ν) with respect to ν for a given number of samples n is Vν[ℓ I θ(s, ν)] = 1 n Vν[DKL(πθ(·|fν(s))||g(fν(s), ·))] = 1 n Eν[(DKL(πθ(·|fν(s))||g(fν(s), ·)) − Eη[DKL(πθ(·|fη(s))||g(fη(s), ·))])2] = 1 n Eν[(DKL(πθ(·|fν(s))||g(fν(s), ·)) − ∑ η P(η)DKL(πθ(·|fη(s))||g(fη(s), ·)))2] = 1 n Eν[( ∑ η P(η)(DKL(πθ(·|fη(s))||g(fη(s), ·)) − DKL(πθ(·|fν(s))||g(fν(s), ·))) )2] (62) 19 Published as a conference paper at ICLR 2024 For the term inside the above equation, we can further derive: DKL(πθ(·|fη(s))||g(fη(s), ·)) − DKL(πθ(·|fν(s))||g(fν(s), ·)) = ∫ a πθ(·|fη(s)) log πθ(·|fη(s)) g(fη(s), ·) − πθ(·|fν(s)) log πθ(·|fν(s)) g(fν(s), ·) = ∫ a πθ(·|fη(s)) log πθ(·|fη(s)) − πθ(·|fη(s)) log g(fη(s), ·) − πθ(·|fν(s)) log πθ(·|fν(s)) + πθ(·|fν(s)) log g(fν(s), ·) = ∫ a πθ(·|fη(s)) log πθ(·|fη(s)) πθ(·|fν(s)) − πθ(·|fη(s)) log g(fη(s), ·) − (πθ(·|fν(s)) − πθ(·|fη(s))) log πθ(·|fν(s)) + πθ(·|fν(s)) log g(fν(s), ·) = ∫ a πθ(·|fη(s)) log πθ(·|fη(s)) πθ(·|fν(s)) − πθ(·|fη(s)) log g(fν(s), ·) + πθ(·|fη(s)) log g(fν(s), ·) g(fη(s), ·) − (πθ(·|fν(s)) − πθ(·|fη(s))) log πθ(·|fν(s)) + πθ(·|fν(s)) log g(fν(s), ·) = DKL(πθ(·|fη(s))||πθ(·|fν(s))) + ∫ a (πθ(·|fη(s)) − πθ(·|fν(s)) ) · ( log πθ(·|fν(s)) − log g(fν(s), ·) ) + ∫ a πθ(·|fη(s)) log g(fν(s), ·) g(fη(s), ·) (63) Then plug the above results into the equation of Vν[ℓ I θ(s, ν)]. Vν[ℓ I θ(s, ν)] = 1 n Eν[( ∑ η P(η)(DKL(πθ(·|fη(s))||g(fη(s), ·)) − DKL(πθ(·|fν(s))||g(fν(s), ·))) )2] = 1 n Eν[( ∑ η P(η)DKL(πθ(·|fη(s))||πθ(·|fν(s))) + ∑ η P (η) ∫ a(πθ(·|fη(s)) − πθ(·|fν(s)))(log πθ(·|fν(s)) − log g(fν(s), ·)) + ∑ η P (η) ∫ a πθ(·|fη(s)) log g(fν(s), ·) g(fη(s), ·) )2] = 1 n Eν[( ∑ η P(η)DKL(πθ(·|fη(s))||πθ(·|fν(s))) + ∑ η P (η) ∫ a(πθ(·|fη(s)) − πθ(·|fν(s))) · (log πθ(·|fν(s)) − log g(fν(s), ·)) + 1 α ∑ η P (η) ∫ a πθ(·|fη(s))(Qϕ(fν(s), a) − Qϕ(fη(s), a)) + ∑ η P (η) ∫ a πθ(·|fη(s)) log Z(fν(s)) Z(fη(s)) )2] (64) 20 Published as a conference paper at ICLR 2024 For the second term on the right hand side of Equation 64, by applying Pinsker’s inequality, we get ∑ η P (η) ∫ a(πθ(·|fη(s)) − πθ(·|fν(s))) · (log πθ(·|fν(s)) − log g(fν(s), ·)) ≤ ∑ η P(η) ∫ a |πθ(a|fη(s)) − πθ(a|fν(s))| · max a | log πθ(a|fν(s)) − log g(fν(s), a)| ≤ ∑ η P(η) √ 2DKL(πθ(·|fη(s))||πθ(·|fν(s)) ) · max a | log πθ(a|fν(s)) − log g(fν(s), a)| (65) For the third and fourth terms of Equation 64, given data augmentation fν|ν ∼ P on state s, if Q(fν(s), a) is invariant with respect to ν for all a ∈ A, both the third and the fourth terms of ˆVν[ℓ I θ(s, ν)] are zero. Therefore, if Q(fν(s), a) is invariant with respect to ν for all a ∈ A, the variance of the augmented actor loss Vν[ℓ I θ(s, ν)] is bounded by the KL divergence Dη,ν = DKL(π(· | fη(s)) | π(· | fν(s))) for ν, η ∼ P: Vν[ℓ I θ(s, ν)] ≤ 1 n Eν[( Eη[Dη,ν + c(fν(s))√2Dη,ν] )2] (66) where c(fν(s)) = maxa | log πθ(a|fν(s)) − log g(fν(s), a)| > 0, n is the number of samples to estimate the empirical mean. DDPG actor loss Based on Equation 31, the DDPG actor loss ℓ I θ(s, µ) becomes, ℓ I θ(s, µ) ≈ − 1 2 ˆEµ[(πθ(fµ(s)) − π∗(fµ(s))) T Hµ(πθ(fµ(s)) − π∗(fµ(s))) ]. (67) The variance of the actor loss is reduced if we minimize the mean squared error between two deter- ministic actions ||πθ(fη(s ′)) − πθ(fν(s ′))|| 2, where η, ν ∼ P. Proof. Let Mµ = πθ(fµ(s)) − π∗(fµ(s)). Assuming that the Hessian matrix Hµ have a a lower bound and upper bound: lµI ⪯ Hµ ⪯ LµI, (68) we have lµ∥Mµ∥ 2 ≤ ℓ I θ(s, µ) ≤ Lµ∥Mµ∥ 2. (69) 21 Published as a conference paper at ICLR 2024 Vν[ℓ I θ(s, ν)] = 1 n Eν[(ℓI θ(s, ν) − Eη[ℓ I θ(s, η)]) 2] = 1 n Eν[( ∑ η P(η)ℓ I θ(s, ν) − ∑ η P(η)ℓ I θ(s, η)) 2] = 1 n Eν[( ∑ η P(η)(ℓI θ(s, ν) − ℓ I θ(s, η))) 2] ≤ 1 n Eν[( ∑ η P(η)(ℓ I θ(s, ν) − ℓI θ(s, η))2)] = 1 n Eν,η[(ℓ I θ(s, ν) − ℓI θ(s, η))2] ≤ 1 n · (max ν ℓI θ(s, ν) − min η ℓ I θ(s, η)) 2 ≤ 1 n · (max ν Lν∥Mν∥2 − min η lη∥Mη∥ 2)2 = 1 n · (Lνmax ∥Mνmax ∥ 2 − lηmin ∥Mηmin∥2) 2 Let νmax = arg max ν ℓ I θ(s, ν), ηmin = arg min η ℓ I θ(s, η). Vν[ℓ I θ(s, ν)] ≤ 1 n · ((Lνmax − lηmin )∥Mνmax∥ 2 + lηmin(∥Mνmax∥2 − ∥Mηmin∥2))2 = 1 n · ((Lνmax − lηmin )∥Mνmax∥ 2 + lηmin (∥πθ(fνmax(s)) − π∗(s)∥ 2 − ∥πθ(fηmin(s)) − π∗(s)∥ 2)2 = 1 n · ((Lνmax − lηmin )∥Mνmax∥ 2 + lηmin ( ∑ i (πθ(fνmax(s))i − π∗(s)i)2 − ∑ i (πθ(fηmin (s))i − π∗(s)i)2) 2 = 1 n · ((Lνmax − lηmin )∥Mνmax∥ 2 + lηmin ( ∑ i ((πθ(fνmax(s))i − π∗(s)i) 2 − (πθ(fηmin (s))i − π∗(s)i) 2)) 2 ≤ 1 n · ((Lνmax − lηmin )∥Mνmax∥ 2 + lηmin ∑ i ((πθ(fνmax(s))i − π∗(s)i) 2 − (πθ(fηmin (s))i − π∗(s)i) 2)2 = 1 n · ((Lνmax − lηmin )∥Mνmax∥ 2 + lηmin ∑ i (πθ(fνmax(s))i + πθ(fηmin (s))i − 2π∗(s)i)2(πθ(fνmax(s))i − πθ(fηmin(s))i)2 = 1 n · ((Lνmax − lηmin )∥Mνmax∥ 2 + lηmin ∑ i (πθ(fνmax(s))i + πθ(fηmin (s))i − 2π∗(s)i)2(πθ(fνmax(s))i − πθ(fηmin(s))i)2 = 1 n · ((Lνmax − lηmin )∥Mνmax∥ 2 + lηmin ∑ i (πθ(fνmax(s))i + πθ(fηmin (s))i − 2π∗(s)i)2(πθ(fνmax(s))i − πθ(fηmin(s))i)2 (70) 22 Published as a conference paper at ICLR 2024 Since (a − b)2(a + b − 2c) 2 ≤ (a − b)4 + (a + b − 2c)4 2 = (a − b)4 + ((a − c + b − c)2) 2 2 ≤ (a − b)4 + (2(a − c) 2 + 2(b − c)2) 2 2 = (a − b)4 + 4((a − c) 2 + (b − c) 2) 2 2 ≤ (a − b)4 + 8(a − c)4 + 8(b − c) 4 2 (71) we have Vν[ℓ I θ(s, ν)] ≤ 1 n · (Lνmax − lηmin)∥Mνmax ∥ 2 + lηmin ∑ i (πθ(fνmax(s))i + πθ(fηmin (s))i − 2π∗(s)i)2(πθ(fνmax(s))i − πθ(fηmin(s))i)2 ≤ 1 n · (Lνmax − lηmin)∥Mνmax ∥ 2 + 1 2 lηmin∥πθ(fνmax(s)) − πθ(fηmin(s))∥4 + 4lηmin∥πθ(fνmax(s)) − π∗(s))∥4 + 4lηmin∥πθ(fηmin (s)) − π∗(s))∥ 4 (72) In Equation 72, the third and fourth terms are minimized by the actor loss. If we minimize the second term of Equation 72 by minimizing the mean squared error between two deterministic actions ||πθ(fη(s ′)) − πθ(fν(s ′))|| 2 in the case of DDPG, the variance of the actor loss is reduced. E.3 KL REDUCES THE VARIANCE OF THE TARGET Q-VALUE DDPG target values For DDPG, when we compute target values, we add Ornstein-Uhlenbeck noise to deterministic actions for exploration. Then the policy can be regarded as a probability distribution π. For image-based control tasks, a data augmentation f parameterized by µ ∼ P is applied on the observations. Then the target value y for a given transition (s, a, r, s ′) is y(fµ(s ′), a ′) = r + γQ ¯ϕ(fµ(s ′), a ′), where a ′ ∼ π(·|fµ(s ′)). (73) The expectation of y(fµ(s ′), a ′) with respect to a′ ∼ π(·|fµ(s ′)) is Ea′[y(fµ(s ′), a ′)] = r + γEa′[Q ¯ϕ(fµ(s ′), a ′)] = r + γ ∑ a′ π(a′|fµ(s ′))Q ¯ϕ(fµ(s ′), a ′) (74) The expectation of y(fµ(s ′), a ′) with respect to µ ∼ P and a ′ ∼ π(·|fµ(s ′)) is Eµ,a′[y(fµ(s ′), a ′)] = r + γEµ,a′[Q ¯ϕ(fµ(s ′), a ′)] = r + γ ∑ µ P(µ) ∑ a′ π(a ′|fµ(s ′))Q ¯ϕ(fµ(s ′), a ′) (75) 23 Published as a conference paper at ICLR 2024 We create two tables to better illustrate the meanings of Ea′[y(fµ(s ′), a ′)] and Eµ,a′[y(fµ(s ′), a ′)]. a ′ µ ... fτm (s ′) with P(µ = τm) ... a ′ 1 ... y(fτm (s ′), a′ 1) with P(µ = τm) · πθ(a ′ 1|fτm (s ′)) ... a ′ 2 ... y(fτm (s ′), a′ 2) with P(µ = τm) · πθ(a ′ 2|fτm (s ′)) ... ... ... ... ... a ′ n ... y(fτm (s ′), a′ n) with P(µ = τm) · πθ(a ′ n|fτm (s ′)) ... ... ... ... ... E[y] wrt. a ′ ... Ea′[y(fτm (s ′), a ′)] ... fτ1 (s ′) with P(µ = τ1) ... E[y] wrt. a and µ Ea′[y(fτ1(s ′), a ′)] ... Eµ,a′[y(fµ(s ′), a ′)] The variance of y(fµ(s ′), a ′) with respect to µ and a ′ is Vµ,a′[y(fµ(s ′), a ′)] = ∑ µ P(µ) ∑ a′ π(a ′|fµ(s ′))[ (y(fµ(s ′), a ′) − Eµ,a′[y(fµ(s ′), a ′)]) 2] = ∑ µ P(µ) ∑ a′ π(a ′|fµ(s ′)) [(y(fµ(s ′), a ′) − Ea′[y(fµ(s ′), a ′)] + Ea′[y(fµ(s ′), a ′)] − Eµ,a′[y(fµ(s ′), a ′)]) 2] = ∑ µ P(µ) ∑ a′ π(a ′|fµ(s ′))[ (y(fµ(s ′), a ′) − Ea′[y(fµ(s ′), a ′)]) 2 + 2(y(fµ(s ′), a ′) − Ea′[y(fµ(s ′), a ′)]) · (ˆEa′[y(fµ(s ′), a ′)] − Eµ,a′[y(fµ(s ′), a ′)]) + (Ea′[y(fµ(s ′), a ′)] − Eµ,a′[y(fµ(s ′), a ′)]) 2] (76) The first term of Equation 76 is the expectation of squared advantage. The second term of Equation 76 is 0 because ∑ µ P(µ) ∑ a′ π(a ′|fµ(s ′)) [2(y(fµ(s ′), a ′) − Ea′[y(fµ(s ′), a ′)]) · (Ea′[y(fµ(s ′), a ′)] − Eµ,a′[y(fµ(s ′), a ′)]) ] = 2 ∑ µ [P(µ) · (Ea′[y(fµ(s ′), a ′)] − Eµ,a′[y(fµ(s ′), a ′)]) · ( ∑ a′ π(a ′|fµ(s ′))(y(fµ(s ′), a ′) − Ea′[y(fµ(s ′), a ′)]))] = 2 ∑ µ [ P(µ) · (Ea′[y(fµ(s ′), a ′)] − Eµ,a′[y(fµ(s ′), a ′)]) · (Ea′[y(fµ(s ′), a ′)] − Ea′[y(fµ(s ′), a ′)]) )] = 0 (77) The third term of Equation 76 is the variance of Ea′∼π(·|fµ(s′))[y(fµ(s ′), a ′)] with respect to µ. Both the variance Vµ[Ea′∼π(·|fµ(s′))[y(fµ(s ′), a ′)]] and the variance of the empirical mean 24 Published as a conference paper at ICLR 2024 Vµ[ˆEµ[Ea′∼πθ(·|fµ(s′))[y(fµ(s ′), a ′)]]] are bounded by the KL divergence Dη,µ = DKL(π(· | fη(s ′)) | π(· | fµ(s ′))) for µ, η ∼ P if Q ¯ϕ(fµ(s ′), a ′) is invariant with respect to µ for all a ′ ∈ A. Proof. Vµ[Ea′∼π(·|fµ(s′))[y(fµ(s ′), a ′)]] = Eµ[(Ea′[y(fµ(s ′), a ′)] − Eη,a′[y(fη(s ′), a ′)]) 2] (78) Ea′[y(fµ(s ′), a ′)] − Eη,a′[y(fη(s ′), a ′)] = γ(( ∑ a′ π(a ′|fµ(s ′))Q ¯ϕ(fµ(s ′), a ′)) − ( ∑ η P(η) ∫ a′ π(a ′|fη(s ′))Q ¯ϕ(fη(s ′), a ′))) = γ(((∑ η P(η) ∑ a′ π(a ′|fµ(s ′))Q ¯ϕ(fµ(s ′), a ′)) − ( ∑ η P(η) ∑ a′ π(a ′|fη(s ′))Q ¯ϕ(fη(s ′), a ′))) = γ( ∑ η P(η) ∑ a′ π(a ′|fµ(s ′))Q ¯ϕ(fµ(s ′), a ′) − π(a ′|fη(s ′))Q ¯ϕ(fη(s ′), a ′) ) = γ( ∑ η P(η) ∑ a′ π(a ′|fµ(s ′))Q ¯ϕ(fµ(s ′), a ′) − π(a ′|fη(s ′))Q ¯ϕ(fµ(s ′), a ′) + π(a ′|fη(s ′))Q ¯ϕ(fµ(s ′), a ′) − π(a′|fη(s ′))Q ¯ϕ(fη(s ′), a ′)) = γ( ∑ η P(η) ∑ a′ (π(a ′|fµ(s ′)) − π(a ′|fη(s ′)))Q ¯ϕ(fµ(s ′), a ′) + π(a ′|fη(s ′))(Q ¯ϕ(fµ(s ′), a ′) − Q ¯ϕ(fη(s ′), a ′))) (79) The second term of Equation 79 γ ∑ η P(η) ∑ a′ π(a ′|fη(s ′))(Q ¯ϕ(fµ(s ′), a ′) − Q ¯ϕ(fη(s ′), a ′)) is related to the difference of Q ¯ϕ(fη(s ′), a ′) and Q ¯ϕ(fµ(s ′), a ′), which is governed by the critic loss. When Q ¯ϕ(fµ(s ′), a ′) is invariant with respect to µ for all a ′ ∈ A, this term is zero. For the first term of Equation 79, ∑ η P(η) ∑ a′ (π(a ′|fµ(s ′)) − π(a ′|fη(s ′)))Q ¯ϕ(fµ(s ′), a ′) ≤ ∑ η P(η) ∑ a′ |π(a ′|fµ(s ′)) − π(a ′|fη(s ′))|Q ¯ϕ(fµ(s ′), a ′) ≤ max a′ Q ¯ϕ(fµ(s ′), a ′) · ∑ η P(η) ∑ a′ |π(a ′|fµ(s ′)) − π(a ′|fη(s ′))| ≤ max a′ Q ¯ϕ(fµ(s ′), a ′) · ∑ η P(η) √2DKL(π(·|fη(s′))||π(·|fµ(s′))) (80) where in the first inequality absolute values |π(a′|fµ(s ′)) − π(a ′|fη(s ′))| are applied, in the second inequality Q ¯ϕ(fµ(s ′), a ′) is replaced with maxa′ Q ¯ϕ(fµ(s ′), a ′) and Pinsker‘s inequality is applied in the third inequality. Similarly, a lower bound can be derived. ∑ η P(η) ∑ a′ (π(a ′|fµ(s ′)) − π(a ′|fη(s ′)))Q ¯ϕ(fµ(s ′), a ′) ≥ − max a′ Q ¯ϕ(fµ(s ′), a ′) · ∑ η P(η)√2DKL(π(·|fη(s′))||π(·|fµ(s′))) (81) Therfore, Vµ[Ea′∼π(·|fµ(s′))[y(fµ(s ′), a ′)]] ≤ Eµ[ γ2( max a′ Q ¯ϕ(fµ(s ′), a ′)Eη[√2Dη,µ])2] (82) 25 Published as a conference paper at ICLR 2024 Let ˆY (s ′, µ) = ˆEµ[Ea′∼πθ(·|fµ(s′))[y(fµ(s ′), a ′)]]. From Equation 61, Vµ[ ˆY (s ′, µ)] = 1 n Vµ[Ea′∼πθ(·|fµ(s′))[y(fµ(s ′), a ′)]], where n is the number of samples to estimate the empirical mean ˆY (s ′, µ). (83) Therefore, if Q ¯ϕ(fµ(s), a ′) is invariant with respect to µ for all a ′ ∈ A, the variance of ˆY (s ′, µ) with respect to µ is bounded by the KL divergence Dη,µ = DKL(π(· | fη(s ′)) | π(· | fµ(s ′))) for µ, η ∼ P. Vµ[ ˆY (s ′, µ)] ≤ 1 n Eµ[ γ2( max a′ Q ¯ϕ(fµ(s ′), a ′)Eη[√2Dη,µ])2] (84) For DDPG, minimizing the KL divergence between policy distributions of two augmented states DKL(π(· | fη(s ′)) | π(· | fµ(s ′))) is equivalent to minimizing the mean squared error between two deterministic actions ||¯π(fη(s ′)) − ¯π(fµ(s ′))|| 2. SAC target value with the entropy term If the entropy term is added to the target value, the variance of the empirical mean Vµ[ˆEµ[Ea′∼πθ(·|fµ(s′))[y(fµ(s ′), a ′)]]] is still bounded by the KL divergence Dη,µ = DKL(π(· | fη(s ′)) | π(· | fµ(s ′))) for µ, η ∼ P if Q ¯ϕ(fµ(s ′), a ′) is invariant with respect to µ for all a′ ∈ A. Vµ[ˆEµ[Ea′∼πθ(·|fµ(s′))[y(fµ(s ′), a ′)]]] ≤ 1 n Eµ[( Eη[ max a′ (y(fµ(s ′), a ′) − r) √2Dη,µ + α · Dη,µ])2] (85) where n is the number of samples to estimate the empirical mean, r is the reward of this transition and α is the entropy coefficient. Proof. After we add the entropy term, the target value becomes y(fµ(s ′), a ′) = r + γQ ¯ϕ(fµ(s ′), a ′) − α log π(a ′|fµ(s ′)), (86) where a ′ ∼ π(·|fµ(s ′)) and α is the entropy coefficient. Let y1(fµ(s ′), a ′) = y(fµ(s ′), a ′) − r = γQ ¯ϕ(fµ(s ′), a ′) − α log π(a′|fµ(s ′)) (87) Since r is a constant value, we can drop r when calculating the variance. Vµ[Ea′∼π(·|fµ(s′))[y(fµ(s ′), a ′)]] = Vµ[Ea′∼π(·|fµ(s′))[y1(fµ(s ′), a ′)]] = Eµ[(Ea′[γQ ¯ϕ(fµ(s ′), a ′) − α log π(a ′|fµ(s ′))] − Eη,a′[γQ ¯ϕ(fη(s ′), a ′) − α log π(a ′|fη(s ′))]) 2] (88) 26 Published as a conference paper at ICLR 2024 Ea′[y1(fµ(s ′), a ′)] − Eη,a′[y1(fη(s ′), a ′)] = ∑ a′ π(a ′|fµ(s ′))y1(fµ(s ′), a ′) − ∑ η P(η) ∫ a′ π(a ′|fη(s ′))y1(fη(s ′), a ′) = ∑ η P(η) ∑ a′ π(a ′|fµ(s ′))y1(fµ(s ′), a ′) − ∑ η P(η) ∑ a′ π(a ′|fη(s ′))y1(fη(s ′), a ′) = ∑ η P(η) ∑ a′ π(a ′|fµ(s ′))y1(fµ(s ′), a ′) − π(a ′|fη(s ′))y1(fη(s ′), a ′) = ∑ η P(η) ∑ a′ π(a ′|fµ(s ′))y1(fµ(s ′), a ′) − π(a ′|fη(s ′))y1(fµ(s ′), a ′) + π(a ′|fη(s ′))y1(fµ(s ′), a ′) − π(a ′|fη(s ′))y1(fη(s ′), a ′) = ∑ η P(η) ∑ a′ y1(fµ(s ′), a ′)(π(a′|fµ(s ′)) − π(a ′|fη(s ′))) + π(a ′|fη(s ′))(y1(fµ(s ′), a ′) − y1(fη(s ′), a ′)) = ∑ η P(η) ∑ a′ y1(fµ(s ′), a ′)(π(a′|fµ(s ′)) − π(a ′|fη(s ′))) + π(a ′|fη(s ′))(γQ ¯ϕ(fµ(s ′), a ′) − γQ ¯ϕ(fη(s ′), a ′)) + π(a ′|fη(s ′))(α log π(a ′|fη(s ′)) − α log π(a ′|fµ(s ′))) = ∑ η P(η) ∑ a′ y1(fµ(s ′), a ′)(π(a′|fµ(s ′)) − π(a ′|fη(s ′))) + π(a ′|fη(s ′))(γQ ¯ϕ(fµ(s ′), a ′) − γQ ¯ϕ(fη(s ′), a ′)) + ∑ η P(η) ∑ a′ π(a ′|fη(s ′))(α log π(a ′|fη(s ′)) − α log π(a ′|fµ(s ′))) = ∑ η P(η) ∑ a′ y1(fµ(s ′), a ′)(π(a′|fµ(s ′)) − π(a ′|fη(s ′))) + γ ∑ η P(η) ∑ a′ π(a ′|fη(s ′))(Q ¯ϕ(fµ(s ′), a ′) − Q ¯ϕ(fη(s ′), a ′)) + ∑ η P(η)α · DKL(π(a ′|fη(s ′))|π(a ′|fµ(s ′))) (89) Similar to Equation 80 and Equation 81, we apply Pinsker’s inequality and obtain the lower and the upper bounds for the first term of Equation 89. − max a′ y1(fµ(s ′), a ′) · ∑ η P(η) √2DKL(π(·|fη(s′))||π(·|fµ(s′))) ≤ ∑ η P(η) ∑ a′ (π(a ′|fµ(s ′)) − π(a′|fη(s ′)))y(fµ(s ′), a ′) ≤ max a′ y1(fµ(s ′), a ′) · ∑ η P(η) √2DKL(π(·|fη(s′))||π(·|fµ(s′))) (90) The second term of Equation 89 γ ∑ η P(η) ∑ a′ π(a ′|fη(s ′))(Q ¯ϕ(fµ(s ′), a ′) − Q ¯ϕ(fη(s ′), a ′)) is related to the difference of Q ¯ϕ(fη(s ′), a ′) and Q ¯ϕ(fµ(s ′), a ′), which is governed by the critic loss. When Q ¯ϕ(fµ(s ′), a ′) is invariant with respect to µ for all a ′ ∈ A, this term is zero. 27 Published as a conference paper at ICLR 2024 Therefore, ∑ η P(η) · ( − max a′ y1(fµ(s ′), a ′) · √2DKL(π(·|fη(s′))||π(·|fµ(s′))) + α · DKL(π(a ′|fη(s ′))|π(a ′|fµ(s ′))) ) ≤ Ea′[y(fµ(s ′), a ′)] − Eη,a′[y(fη(s ′), a ′)] ≤ ∑ η P(η) · ( max a′ y1(fµ(s ′), a ′) · √2DKL(π(·|fη(s′))||π(·|fµ(s′))) + α · DKL(π(a ′|fη(s ′))|π(a ′|fµ(s ′))) ) (91) Plug the above inequalities into Equation 88, we obtain Vµ[Ea′∼π(·|fµ(s′))[y(fµ(s ′), a ′)]] ≤ Eµ[( Eη[ max a′ y1(fµ(s ′), a ′) √2Dη,µ + α · Dη,µ])2] (92) If Q ¯ϕ(fµ(s), a ′) is invariant with respect to µ for all a ′ ∈ A, the variance of ˆY (s ′, µ) with respect to µ is bounded by the KL divergence Dη,µ = DKL(π(· | fη(s ′)) | π(· | fµ(s ′))) for µ, η ∼ P. Vµ[ ˆY (s ′, µ)] ≤ 1 n Eµ[( Eη[ max a′ (y(fµ(s ′), a ′) − r) √2Dη,µ + α · Dη,µ])2] (93) F CALCULATING TARGET WITH COMPLEX DATA AUGMENTATION In this section, we experimentally analyze using complex image transformations in calculating the target and show that cosine similarity of the augmented features at the early training stage can be used as a criteria for judging if an image transformation is complex or not. Sufficient updates is the key condition for good performance when using complex image transformations in calculating the target. In contrast to the analysis in SVEA (Hansen et al., 2021), we observe that even using complex image transformation such as random conv in the target does not induce a large variance in the target. Instead, a much larger bias is observed for the trained agent, as shown in the Table 5. This can be solved by increasing the number of updates, as shown in Figure 4. Furthermore, we test with other image transformations which are regarded as complex image trans- formations in SVEA (Hansen et al., 2021). In order to show whether it’s easy to enforce the in- variance of a image transformation, we record the cosine similarities of encoder outputs for two augmented images transformed by this image transformation, as shown in Table 4. For image trans- formations such as random overlay or gaussian blur, the invariance is easy to enforce and the cosine similarities are large. When using this kind of image transformation in calculating the target values, it won’t hurt the performance. Otherwise, for image transformations such as random convolution or random rotation, the invariance is relatively harder to enforce during training and the cosine similar- ities are small. Then directly applying this kind of image transformations in calculating the target values will decrease the learning efficiency. To resolve this issue, we need more updates for each training step. The evaluation results for SVEA with random overlay, random convolution, random rotation and gaussian blur are shown in Figure 5. G HYPERPARAMETERS Hyperparameters used in experiments on DMControl (drq), DMControl (drqv2) and DMGB can be found in Table 6, Table 7 and 8. For experiments in DMControl(drqv2) and DMGB, when applicable, we adopt hyperparameters from the official implementation of drqv2 by Yarats et al. (2021) and SVEA by Hansen et al. (2021) respectively. H ADDITIONAL RESULTS H.1 ABLATION STUDY The performance profile is shown in Figure 6 and the training curves in different environments are shown in Figure 7. 28 Published as a conference paper at ICLR 2024 Statistics svea(DA=blur) svea(DA=overlay) svea(DA=conv) svea(DA=rotation) actor sim (shift) 0.919±0.007 0.911±0.005 0.910±0.011 0.936±0.006 actor sim (DA) 0.998±0.001 0.906±0.006 0.854±0.019 0.536±0.069 critic sim (shift) 0.938±0.010 0.942±0.003 0.922±0.010 0.962±0.005 critic sim (DA) 0.998±0.001 0.939±0.003 0.883±0.014 0.660±0.057 Table 4: Recorded cosine similarity for latent features at 100k steps in walker walk environment for SVEA trained with different complex image transformations. Here, each column corresponds to SVEA with different image transformations and each row corresponds to a cosine similarity recorded at 100k steps. For example, the first number is calculated by the cosine similarity between the latent features Eπ(fshif t(s)), in which Eπ is the encoder of the actor in SVEA trained with gaussian blur and fshif t is the random shift. Considering that random shift is always applied in SVEA, the cosine similarity with respect to it is recorded as the baseline for comparisons. For gaussian blur and random overlay (second and third column), the cosine similarities of latent features are higher or similar to the cosine similarities between the latent features from two randomly shifted images which means they are not complex image transformations. In contrast, random conv and random rotation (last two columns) leads to smaller cosine similarities of latent features which indicates that they are relatively complex image transformations in this environment. Statistics Step 100k Step 200k Step 300k Step 400k Step 500k Mean (w/ conv) 64.05 112.44 140.78 166.42 185.92 Mean (w/o conv) 84.96 148.46 197.10 221.67 239.35 Variance (w/ conv) 1.228 1.109 1.148 1.323 1.306 Variance (w/o conv) 0.882 0.812 0.885 0.757 0.795 Bias (w/ conv) 52.88 67.44 54.20 63.89 55.36 Bias (w/o conv) 77.40 60.48 50.40 43.22 28.75 Table 5: Mean, variance and bias of the target Q-values for the agent trained with/without using random conv in calculating the target. Here, the mean and variance are calculated by the mean and variance of a set of sampled target Q values and the bias is calculated by the mean-squared error between the targets used in the training and the true targets estimated by the sum of discounted rewards from sampled trajectories. At the end of the training, the increase in the variance when using random conv is not significant compared to the mean of the target. However, the bias is much larger at the end. 29 Published as a conference paper at ICLR 2024 Figure 4: Performance of increasing the number of updates with/without using random conv in calculating the targets. Figure 5: Performance of increasing the number of updates in walker walk environment when using complex image transformation in calculating the targets. For random convolution and random rota- tion, ”update more” stands for doing 4 updates for each training step. Figure 6: Performance profile of different methods. 30 Published as a conference paper at ICLR 2024 Table 6: Hyperparameters used in experiments on DMControl (drq) Hyperparameter Value on DMControl frame rendering 84 × 84 × 3 stacked frames 3 action repeat 2 replay buffer capacity 100,000 seed steps 1000 environment steps 250,000 in reacher easy 250,000 in finger spin 250,000 in ball 500,000 in others batch size N 256 discount γ 0.99 optimizer (ϕ, θ) Adam (β1 = 0.9, β2 = 0.999) optimizer (α of SAC) Adam (β1 = 0.9, β2 = 0.999) learning rate (ϕ,θ) 1e-3 learning rate (α of SAC) 1e-3 target network update frequency 2 target network soft-update rate 0.01 actor update frequency κ 2 actor log stddev bounds [-10,2] init temperature α 0.1 tangent prop weight αtp 0.1 actor KL weight αKL 0.1 Table 7: Hyperparameters used in experiments on DMControl (drqv2) Hyperparameter Value on DMC frame rendering 84 × 84 × 3 stacked frames 3 action repeat 2 replay buffer capacity 10 6 seed frames 4000 exploration steps 2000 n-step returns 3 batch size N 256 discount γ 0.99 optimizer (ϕ, θ) Adam learning rate (ϕ,θ) 1e-4 agent update frequency 2 target network soft-update rate 0.01 exploration stddev clip 0.3 exploration stddev schedule linear(1.0, 0.1, 500000) tangent prop weight αtp 0.1 actor KL weight αKL 0.1 H.2 CASE STUDY The measures for invariance in the latent space are shown in Figure 8. 31 Published as a conference paper at ICLR 2024 Figure 7: Full results of validating our propositions. Figure 8: The figure shows the learned invariance in the feature space of the actor and critic. Two measures of the invariance are provided in this figure: the distances between projected points of the augmented features by t-SNE and the cosine similarities between augmented features. 32 Published as a conference paper at ICLR 2024 Table 8: Hyperparameters used in experiments on DMControl Generalization Benchmark (DMGB) Hyperparameter Value on DMGB frame rendering 84 × 84 × 3 stacked frames 3 action repeat 2(finger) 8(cartpole) 4(otherwise) replay buffer capacity 500,000 / action repeat seed steps 1000 environment steps 500,000 batch size N 128 discount γ 0.99 optimizer (ϕ, θ) Adam (β1 = 0.9, β2 = 0.999) optimizer (α of SAC) Adam (β1 = 0.5, β2 = 0.999) learning rate (ϕ,θ) 1e-3 learning rate (α of SAC) 1e-4 target network update frequency 2 target network soft-update rate 0.01(critic) 0.05(encoder) actor update frequency κ 2 actor log stddev bounds [-10,2] init temperature α 0.1 tangent prop weight αtp 0.5 actor KL weight αKL 0.1 H.3 MORE EVALUATIONS Here, we include more evaluations of our proposition. The results of comparing our proposition with DrQ are shown in Figure 9. The results of comparing our proposition with DrQv2 are shown in Figure 10. H.4 RESULTS OF GENERALIZATION ABILITY IN DMCONTROL GENERALIZATION BENCHMARK (DMGB) The comparison of generalization performance in DMGB between SVEA and our method using random overlay as data augmentation is shown in Figure 11. H.5 RESULTS OF RECORDED STATISTICS The curves for the recorded statistics, including standard deviation of the empirical critic loss, stan- dard deviation of the target Q-values, and empirical mean of KL divergence between policies for two augmented samples along the training are shown in Figure 12 and Figure 13. I LIMITATIONS We try to provide some recommendations on how to apply theoretically-sound data augmentation method in DRL. However, the analysis can still be further refined to be more comprehensive such as including the theoretical analysis of using different distributions for the image transformation and providing a thorough analysis on tangent prop regularization. Moreover, our method naturally requires the knowledge of some effective image transformations for a given task. Without such knowledge, the invariant transformations for a problem would need to be learned, which is currently an active research direction. Finally, image transformation may rely on some implicit assumptions, 33 Published as a conference paper at ICLR 2024 Figure 9: Comparison between different methods in DMControl with normal background. Figure 10: Results of running experiments with DDPG as base algorithm. 34 Published as a conference paper at ICLR 2024 Figure 11: Comparison between SVEA and our method in DMControl with normal and video- hard backgrounds. Both methods use random overlay as image transformation. We can see the improvement in generalization ability especially in environments such as ball in cup catch, finger spin and walker walk. Since the evaluation curves are not stable even at the end of training, the recorded score in Table 2 is the average over the last 15 evaluation scores. 35 Published as a conference paper at ICLR 2024 Figure 12: Some important statistics recorded along the training. The variance of critic loss and target values decreased after using more augmented samples in the training of the critic. Adding the KL divergence term to the loss can quickly enforce the invariance of the actor even at the beginning of the training. which may lead to lower/bad performance if they are not satisfied in the real application domain. For instance, random shift/crop, which has been shown to be very effective in DMControl tasks, may yield worse performance if the agent is not well-centered in the image, according to the empirical results from Tomar et al. (2022). A better understanding of why a data augmentation transformation works in DRL is needed. 36 Published as a conference paper at ICLR 2024 Figure 13: Some important statistics recorded along the training of SVEA and our method. With the help of KL loss and tangent prop loss, the variance of critic loss and target values are lower. Applying KL loss can quickly enforce the invariance of the actor. 37","libVersion":"0.3.2","langs":""}