{"path":"deep-reinforcement-learning/slides/2 Supervised Learning of Behaviors.pdf","text":"Supervised Learning of Behaviors CS 285 Instructor: Sergey Levine UC Berkeley 1. run away 2. ignore 3. pet Terminology & notation 1. run away 2. ignore 3. pet Terminology & notationAside: notation Richard Bellman Lev Pontryagin управление Images: Bojarski et al. ‘16, NVIDIA training data supervised learning Imitation Learning behavioral cloning The original deep imitation learning system ALVINN: Autonomous Land Vehicle In a Neural Network 1989 Does it work? No!Does it work? Yes! Video: Bojarski et al. ‘16, NVIDIA Why did that work? Bojarski et al. ‘16, NVIDIA The moral of the story, and a list of ideas • Imitation learning via behavioral cloning is not guaranteed to work • This is different from supervised learning • The reason: i.i.d. assumption does not hold! • We can formalize why this is and do a bit of theory • We can address the problem in a few ways: • Be smart about how we collect (and augment) our data • Use very powerful models that make very few mistakes • Use multi-task learning • Change the algorithm (DAgger) Why does behavioral cloning fail? A bit of theory The distributional shift problemLet’s define more precisely what we want training data supervised learning “Minimize the number of mistakes the policy makes when we run it” Some analysis More general analysis For more analysis, see Ross et al. “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning” More general analysis For more analysis, see Ross et al. “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning” Why is this rather pessimistic? In reality, we can often recover from mistakes But that doesn’t mean that imitation learning will allow us to learn how to do that! Why does this work?cost A paradox: imitation learning can work better if the data has more mistakes (and recoveries)! Addressing the problem in practice Where are we… • Imitation learning via behavioral cloning is not guaranteed to work • This is different from supervised learning • The reason: i.i.d. assumption does not hold! • We can formalize why this is and do a bit of theory • We can address the problem in a few ways: • Be smart about how we collect (and augment) our data • Use very powerful models that make very few mistakes • Use multi-task learning • Change the algorithm (DAgger) Where are we… • Imitation learning via behavioral cloning is not guaranteed to work • This is different from supervised learning • The reason: i.i.d. assumption does not hold! • We can formalize why this is and do a bit of theory • We can address the problem in a few ways: • Be smart about how we collect (and augment) our data • Use very powerful models that make very few mistakes • Use multi-task learning • Change the algorithm (DAgger) What makes behavioral cloning easy and what makes it hard?cost • Intentionally add mistakes and corrections • The mistakes hurt, but the corrections help, often more than the mistakes hurt • Use data augmentation • Add some “fake” data that illustrates corrections (e.g., side- facing cameras) Case study 1: trail following as classificationCase study 2: imitation with a cheap robot Rouhollah Rahmatizadeh et al., Vision-Based Multi-Task Manipulation for Inexpensive Robots Using End-To-End Learning from Demonstration. 2017. Where are we… • Imitation learning via behavioral cloning is not guaranteed to work • This is different from supervised learning • The reason: i.i.d. assumption does not hold! • We can formalize why this is and do a bit of theory • We can address the problem in a few ways: • Be smart about how we collect (and augment) our data • Use very powerful models that make very few mistakes • Use multi-task learning • Change the algorithm (DAgger) Why might we fail to fit the expert? 1. Non-Markovian behavior 2. Multimodal behavior behavior depends only on current observation If we see the same thing twice, we do the same thing twice, regardless of what happened before Often very unnatural for human demonstrators behavior depends on all past observations How can we use the whole history? variable number of frames, too many weights How can we use the whole history? sequence model shared weights Can be done with Transformers, LSTM cells, etc. Aside: why might this work poorly? “causal confusion” see: de Haan et al., “Causal Confusion in Imitation Learning” Question 1: Does including history mitigate causal confusion? Question 2: Can DAgger mitigate causal confusion? Why might we fail to fit the expert? 1. Non-Markovian behavior 2. Multimodal behavior 1. More expressive continuous distributions 2. Discretization with high- dimensional action spaces Expressive continuous distributions Quite a few options, many ways to make things work: 1. mixture of Gaussians 2. latent variable models 3. diffusion models 1. mixture of Gaussians 2. latent variable models 3. diffusion models Expressive continuous distributions The most widely used type of model of this sort is the (conditional) variational autoencoder 1. mixture of Gaussians 2. latent variable models 3. diffusion models We’ll learn about such models later in the course Expressive continuous distributions 1. mixture of Gaussians 2. latent variable models 3. diffusion models Expressive continuous distributions 1. mixture of Gaussians 2. latent variable models 3. diffusion models Expressive continuous distributionsWhat about discretization? Problem: this is great for 1D actions, but in higher dimensions, discretizing the full space is impractical Solution: discretize one dimension at a time Autoregressive discretization conv net encoder sequence model block use LSTM or Transformer sequence model block sequence model block Why does this work? Case study 3: imitation with diffusion models Chi et al. Diffusion Policy: Visuomotor Policy Learning via Action Diffusion. 2023 Case study 4: imitation with latent variables Zhao et al. Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware. 2023 Case study 5: imitation with Transformers Brohan et al. RT-1: Robotics Transformer. 2023. Where are we… • Imitation learning via behavioral cloning is not guaranteed to work • This is different from supervised learning • The reason: i.i.d. assumption does not hold! • We can formalize why this is and do a bit of theory • We can address the problem in a few ways: • Be smart about how we collect (and augment) our data • Use very powerful models that make very few mistakes • Use multi-task learning • Change the algorithm (DAgger) Does learning many tasks become easier?Goal-conditioned behavioral cloning We see distributional shift in two places here! Can you figure out what the second place is? 1. Collect data 2. Train goal conditioned policy3. Reach goals Going beyond just imitation? ➢ Start with a random policy ➢ Collect data with random goals ➢ Treat this data as “demonstrations” for the goals that were reached ➢ Use this to improve the policy ➢ Repeat Goal-conditioned BC at a huge scale Shah*, Sridhar*, Bhorkar, Hirose, Levine. GNM: A General Navigation Model to Drive Any Robot. 2022. Also related (for later…) ➢ Similar principle but with reinforcement learning ➢ This will make more sense later once we cover off-policy value-based RL algorithms ➢ Worth mentioning because this idea has been used widely outside of imitation (and was arguably first proposed there) Where are we… • Imitation learning via behavioral cloning is not guaranteed to work • This is different from supervised learning • The reason: i.i.d. assumption does not hold! • We can formalize why this is and do a bit of theory • We can address the problem in a few ways: • Be smart about how we collect (and augment) our data • Use very powerful models that make very few mistakes • Use multi-task learning • Change the algorithm (DAgger) Can we make it work more often?Can we make it work more often? DAgger: Dataset Aggregation Ross et al. ‘11 DAgger Example Ross et al. ‘11 What’s the problem? Ross et al. ‘11 Recap • Imitation learning via behavioral cloning is not guaranteed to work • This is different from supervised learning • The reason: i.i.d. assumption does not hold! • We can formalize why this is and do a bit of theory • We can address the problem in a few ways: • Be smart about how we collect (and augment) our data • Use very powerful models that make very few mistakes • Use multi-task learning • Change the algorithm (DAgger) Cost functions and reward functions, a preview of what comes next Imitation learning: what’s the problem? • Humans need to provide data, which is typically finite • Deep learning works best when data is plentiful • Humans are not good at providing some kinds of actions • Humans can learn autonomously; can our machines do the same? • Unlimited data from own experience • Continuous self-improvement 1. run away 2. ignore 3. pet Terminology & notationAside: notation Richard Bellman Lev Pontryagin A cost function for imitation? training data supervised learning Imitation learning algorithms do maximize reward when they work well! For a very particular choice of reward","libVersion":"0.3.2","langs":""}