{"path":"papers/appendix/figures/dino.png","text":"loss: o) piosn () sg Figure 2: Self-distillation with no labels. We illustrate DINO in the case of one single pair of views (z1, z2) for simplicity. The model passes two different random transformations of an input image to the student and teacher networks. Both networks have the same architecture but different parameters. The output of the teacher network is centered with a mean computed over the batch. Each networks outputs a K dimensional feature that is normalized with a temperature softmax over the feature dimension. Their similarity is then measured with a cross-entropy loss. We apply a stop-gradient (sg) operator on the teacher to propagate gradients only through the student. The teacher parameters are updated with an exponential moving average (ema) of the student parameters.","libVersion":"0.3.2","langs":"eng"}