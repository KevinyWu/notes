{"path":"empowerment/papers/2005 Empowerment.pdf","text":"128 Empowerment: A Universal Agent-Centric Measure of Control Alexander S. Klyubin*, Daniel Polani*t, and Chrystopher L. Nehaniv*t Adaptive Systems* and Algorithmst Research Groups School ofComputer Science, University ofHertfordshire College Lane, Hatfield, Herts, ALlO 9AB, UK {A. Kijubin, D. Polani, C . L .Nehaniv}@herts . ac .uk Abstract- The classical approach to using utility func- tions suffers from the drawback ofhaving to design and tweak the functions on a case by case basis. Inspired by examples from the animal kingdom, social sciences and games we propose empowerment, a rather universal function, defined as the information-theoretic capacity of an agent's actuation channel. The concept applies to any sensorimotor apparatus. Empowerment as a mea- sure reflects the properties of the apparatus as long as they are observable due to the coupling of sensors and actuators via the environment. Using two simple ex- periments we also demonstrate how empowerment in- fluences sensor-actuator evolution. 1 Introduction A common approach to designing adaptive systems is to use utility functions which tell the system which situations to prefer and how to behave in general. Fitness functions used inevolutionary algorithms are similarin spirit. They specify directly or indirectly which genotypes are better. Most utilityfunctions and fitness functions are quite spe- cific and a priori. They are designed for the particular sys- tem and task at hand and are thus not easily applicable in other situations. The task and the properties of the system have to be translated into the \"language\" ofthe utility or fit- ness function. How does Nature address this problem? Is there a more general principle? One common solution found in living organisms is homeostasis [1]. Organisms may be seen to maintain \"es- sential variables\", likebody temperature, sugar levels, toxin levels. Homeostasis provides organisms with a local gra- dient telling which actions to make or which states to seek. The mechanism itselfisuniversal and quite simple, however the choice of variables and the methods ofregulation is not. They are evolved and are specific to different phyla. 2 Empowerment The world is a hostile place. One single wrong action can have disastrous consequences. It can even lead to death. Animals and humans not only survive but also adapt under these circumstances. They all face the classical dilemma of exploration versus exploitation. How do they manage it when errors may be fatal? A standard argument is that evo- lution biases species towards trying or avoiding certain be- haviors in certain situations. However, the population avail- able to evolution is finite and often remarkably small. Evo- lutionary feedback via death seems to be hardly sufficient. Our central hypothesis is that there exist a local and uni- versal utility function which may help individuals survive and hence speed up evolution by making the fitness land- scape smoother. The function is local in the sense that it doesn't rely on infinitely long history of past experience, does not require global knowledge about the world, and that it provides localized feedback to the individual. The util- ity function is applicable to all species, hence, it should be universal though should adapt to morphology and ecolog- ical niche. The utility function should be related to other biologically relevant quantities. Here are some examples of candidate functions, which, however, are not universal because they are quite specific: sugar concentration around a bacterium, social status of a chimpanzee in a group, money in a bank account of a per- son. Can these functions be unified? In his work on ecological approach to visual percep- tion [5] Gibson proposed that animals and humans do not normally view the world in terms of a geometrical space, independent arrow of time, and Newtonian mechanics. In- stead, he argued, the natural description is in terms of what one can perceive and do. Thus, different places in the world are characterized by what they afford one to perceive and do. In some board games utility may be approximated by what one can afford to do. For example, a breakthrough in strategy occurred in Othello/Reversi when the concept of \"mobility\" had been discovered. Mobility is the number of moves a player can make in a given situation. The more moves the player can make the better offon the average he is. Everything else being equal players should seek higher mobility. Ifwe look from this perspective on the previously given examples of specific utility functions, they seem to be more related than they appeared at first. To a sugar-feeding bacterium, high sugar concentration means longer survival time and hence more possibilities of moving to different places for reproduction, to a chimpanzee higher social sta- tus means more mating choice and interaction, to a person more money means more opportunities and more options. The common feature of the above examples is the striv- 0-7803-9363-5/05/$20.00 Â©2005 IEEE. 129 ing for situations with more options, with more potential for control or influence. To capture this notion quantita- tively, as a proper utility function, we need to quantify how much control or influence an animal or human (an agent from now on) has. A fairly straightforward way is to esti- mate how many actions an agentcan perform. However, not all ofthe actions lead to different results, not to mention the cases when the same action can lead to different results. Information Theory, originally developed for transmis- sion ofinformation over communication channels by Shan- non [11], can be applied to theproblem ofcontrol and yields quantitative statements [1, 13]. Control can be success- fully treated in terms of Shannon information. Recently we demonstrated [6] that information flow optimization can structure systems and, despite the universality of Informa- tion Theory, can capture properties of an agent's morphol- ogy and the environment. However, even with Information Theory there is the problem of defining what to look at to determine whether results of actions are different or not. Is turning a page of a book using the left hand different from turning the very same page using the right hand? If one looks only at what page of the book is visible in the end, then the two actions have the same result. If, however, one also takes into the account what one perceives while turning the page, then the results of the two actions are different: when one turns the page using the lefthand, one feels and sees thatthe lefthand is moving, similarly, the right hand is perceived to move when turning the page using the right hand. What is the natural way to look at what actions do? You can move your hand left or right, up or down. Con- sequently, you may assume you have control over your hand. However, from someone else's perspective you may have other control that you do not suspect. For example, a neurosurgeon may notice that you can make a particular neuron in your brain spike. However, ifyou never directly or indirectly perceive the result, your will never know that you have control over the neuron. To make things simpler, similar to Gibson's ecological approach to visual perception [5], we define what an agent does solely in terms of what it perceives. This way it does not matter what other agents perceive about these actions; it does not matter how much control or influence the agent appears to have from a \"god's eye view\" of the world. The concept of \"the environment\" becomes a by-product of the interplay between the agent's sensors and actuators. This enables us to base our utility function solely on the sensors and actuators, without the need to refer to the \"outside\" of the agent. We build on Gibson's view ofperception-action as being central to agents and go even further by: (1) making actu- ators as important as sensors, and (2) providing an agent- centric quantification ofthe amount ofcontrol or influence the agent has andperceives. In Sec. 2.2 we will provide the quantification using In- formation Theory. The key conceptemployed is the channel capacity. We will now briefly provide the necessary back- ground from the Information Theory. 2.1 The Communication Problem Here we provide a briefoverview ofthe classical communi- cationproblem from Information Theory and define channel capacity for a discrete memoryless channel. For an in depth treatment we refer the reader to [11, 3]. There is a sender and a receiver. The sender transmits a signal, denoted by a random variable X, to the receiver, who receives a potentially different signal, denoted by a ran- dom variable Y. The communication channel between the sender and the receiver defines how transmitted signals cor- respond to received signals. In the case of discrete signals the channel can be described by a conditional probability distribution p(ylx). Given a probability distribution over the transmitted sig- nal, mutual information is defined as the amount of infor- mation, measured in bits, the received signal on the average contains about the transmitted signal. Mutual information can be expressed as a function oftheprobability distribution over the transmitted signal p(x) and the distribution charac- terizing the channel p(ylx): I(X;Y) = EP(YIX)P(X)log2 E P(ylx)p(x) (1) Channel capacity is defined as the maximum mutual in- formation for the channel over all possible distributions of the transmitted signal: C(p(ylx)) = maxI(X;Y). (2) Channel capacity is the maximum amount of informa- tion the received signal can contain about the transmitted signal. Thus, mutual information is a function ofp(x) and p(ylx), whereas channel capacity is a function of just the conditional probability distribution p(ylx) describing the channel. Another important difference is that mutual in- formation is symmetric in X and Y and is thus acausal, whereas channel capacity requires complete control over X and is thus asymmetric and causal (cf. [8]). There exist efficient algorithms to calculate the capacity of an arbitrary discrete channel, for example, the iterative algorithm by Blahut [2]. 2.2 Definition ofEmpowerment Consider an agent in an environment. The agent has a sen- sor and an actuator. The actions ofthe agent may reflect the sensoric input, and the sensoric input, in turn, may reflect past actions. The sensor and the actuator are coupled in a perception-action loop [9]. The agent's empowerment is aproperty ofitsperception- action loop. Thus, we first model the loop. Its constituent parts are treated as random variables. These are the sensor S, the actuator A, and R - the rest of the system including the environment. We need R to account forthe effects ofac- tuation, the agent's morphology and the environment on the sensors. Although we treat the agent as havingjustone sen- sor and actuator, S and A can also represent the combined state of some or all of its sensors and actuators. 129 130 As in [13, 6, 7] we interpret the perception-action loop in terms of a communication channel-like model. From this perspective actions may \"inject\" Shannon information into the environment and into the sensors. In order to model the temporal aspects we unroll the loop in time by introducing discrete time t. The random variables St, At, and Rt be- come the states of the sensor, the actuator, and the rest of the system respectively at different times t. To account for the complete loop we model the dynamics of arbitrary num- ber of time steps, as opposed to [13] where only one time step is modeled. We model the relations between the variables as a causal Bayesian network [8] which is a directed acyclic graph where any node, given its parents, is conditionally indepen- dentfrom any other node which is not itsparent or successor (any node directly or indirectlyreachable from the node). In our model this property results in conditional independence from the past. The pattern of relations between variables at two con- secutive time steps is shown in Fig. 1. We assume that the pattern of relations is time-invariant and thus holds for any t. Thus, the graph in Fig. 1 isjust a section of the network. The diagram can be read as follows: action At is picked given sensor state St, sensor state St is obtained from the state of the rest of the agent-environment system Rt, Rt+1 depends only on Rt and At. Rt >Rt+1 > / x/ \\ St At St+, At+ Figure 1: The perception-action loop as a Bayesian net- work. S - sensor, A - actuator, R - rest ofthe system. R is included to formally account for the effects of the actuation on the future sensoric input. R is the state of the actuation channel. So far, we have informally introduced empowerment in terms of the amount of control or influence the agent has over the world and is able to perceive. The only way the agent can impose its influence over the world is via its actu- ator by performing a sequence of actions. To illustrate the idea, imagine an agent with \"free will\", which at some point t in time can perform an arbitrary se- quence of actions of length n. How much information can the agent \"inject\" via the sequence of action into its sensor at time step t + n? The more of the information about the sequence of actions can be made to appear in the sensor, the more control or influence the agent has over its sensor. We redraw the Bayesian network to reflect the fact that the agent is \"free\" to choose any sequence of actions. The new network is shown in Fig. 2. We view the situation as the classical problem of com- munication from Information Theory [11] as described in Sec. 2.1. We need to measure the maximum amount of in- formation the agent could \"inject\" or transmit into its sensor by performing a sequence ofactions oflength n. This ispre- cisely the capacity of the channel between the sequence of At At+, At+2 ERt+1 > Rt+2 Rt+3'. St+i St+2 St+3 Figure 2: 3-step empowerment. Actions are independent of system's state (agent with \"free will\"). The communication channel goes from (At,At+i, At+2) to St+3. actions and sensoric input n time steps later. Let us denote the sequence of n actions taken starting at time t by a random variable A' = (At,At+, . .. At+n), and itsinstantiation by an. Let us denote the state ofthe sen- sor n time steps later by a random variable St+n, and its in- stantiationby st+,. We now view An as the transmitted sig- nal and St+n as the received signal. The system's dynamics induce a conditional probability distributionp(st+nIan)be- tween the sequence of actions A' and the state of sensor after n time steps St+n. This conditional distribution de- scribes the communication channel we need. We can now define empowerment as thechannel capacity of the agent's actuation channel terminating at the sensor (see Eq. 1 and Eq. 2): -l'=C (P(St+nlat) = max I(A'; St+n). (3) Empowerment is measured in bits. It is zero when the agent has no control over what it is sensing, and it is higher the more perceivable control or influence the agent has. Em- powerment can also be interpreted as the amount of infor- mation the agent could potentially \"inject\" [7] into the en- vironment via its actuator and later capture via its sensor. The definition of n-step empowerment above uses only a momentary reading ofthe sensor at time t + n to see how much of the \"injected\" information is captured. In general, the momentary reading of the sensor can be replaced by an arbitrary function of the variables downstream of At. For example, the function could be the sequence of sen- soric states St = (St+I,St+2, .. ., St+,). Ifthe agent has memory, the function could also include the agent's mem- ory downstream from At. The maximizing distributions over the sequences of ac- tions p(a') can be interpreted as the distributions of ac- tions the agent should follow inorderto injectthe maximum amount ofinformation into its sensor after n time steps. The actuation channel will in general have state due to the specifics of the environment or the agent's morphol- ogy. According to our model (see Fig. 1), the state is R, the rest of the system. For the information-theoretic prob- lem of channel with side information it is established [3] that knowing the state of the channel may increase its ca- pacity. Hence, knowing R could increase empowerment. Accordingly, it is useful to define context, a random vari- able approximating the state of the actuation channel R in a compact form (cf. Information Bottleneck [12], e- 130 131 machines [4, 10]). Knowing the context could increase the agent's empowerment. The context could be interpreted as the \"environment\" relevant to the agent. Moreover, the con- text can be deduced or constructed from the agent-centric perspective, from the data available \"on-board\", without the need to resort to global knowledge. However, we omit this more general treatment from the present discussion. 2.3 Maximizing Empowerment Consider an agent that tries to maximize its empowerment. From the perspective of Information Theory, there are two different ways of doing that: (1) strive to reach the parts of the world where empowerment is highest, that is, where p(st+nIan) is less noisy, (2) modify one's sensors and ac- tuators to improve empowerment, that is, modify the sets for which p(St+nIan) is used. In this paper we concentrate only on the latter case which is sensor-actuator adaptation or evolution. If the agent's actuator is fixed (not allowed to adapt or evolve) the sensor may be modified so that the agent \"sees\" more of the results of its own actions. In other words, the agent's sensor is adapted to betterdifferentiate between out- comes of various actions the agent can take. If the agent's sensor is fixed, the actuator may be modi- fied so that it can better influence the future sensoric states. In other words, the actuator is adapted to support actions which can be differentiated by the sensor better. Maximizing empowerment adapts the agent's sensors and actuators to each other. Moreover, it adapts them to the niche in which the agent exists in the world (cf. [5]). For example, letus assume thatthere is a cost to sensoric bandwidth. It may be due to energy constraints or due to information-processing constraints. Ifthe agent initiallyhas eyes, but lives in apartoftheworld where there isno lightat all, empowerment, used as a fitness function, will create an evolutionary gradient towards removing the eyes. If, on the other hand, an agent can produce a flash visible to its eyes, empowerment will keep the eyes and the flash-producing action, no matterwhetherhaving thecapability ofproducing a flash in the dark is \"useful\" in the common sense of the word. The sets of sensoric states and actions available to the agent can be seen as the physical level of its perception- action system. Empowerment builds a logical level on top of that. Actions can be defined through their effects on the sensors (cf. Gibson's approach [5]). For instance, actions with the same effect (as described by p(st+n a')) on the sensor may be seen as just one logical action and need not be differentiated, unless other criteria, such as costs of ac- tions, are taken into account. Similarly, sensors may also be seen in terms ofhow they react to actions. This logical level ofdescription built on top of the physical level may be dif- ferent in different places in the world. For instance, if one is in a completely dark room, one can disregard one's eyes as a sensor because they capture no effects of one's actions. We formulate the following hypothesis: evolution adapts the physical level of sensors and actuators to the logical level induced by empowerment for the niche of the species. Moreover, empowerment maximization will push evolution forward to exploit all existing and newly appearing commu- nication channels. 2.4 Summary We view the perception-action loop of an agent in terms of random variables describing the state of the agent- environment system. The main parts ofthe loop are the state ofthe sensor and the state the actuator. All the variables are linked into a causal Bayesian network. The network allows us to unroll the loop in time. The agent's actuation channel is a combined probabilis- tic description of \"embodiment\", the agent's morphology and environment, in terms of the effects of the agent's actions on its sensors. Empowerment is defined as the information-theoretic capacity of the actuation channel and is measured in bits. Empowerment is zero when the agent has no control over its sensors, and it grows when the con- trol or influence the agent has over its perceptions grows. The actuation channel in general has state due to the specifics of the environment and the agent's morphology. Knowing certain features of the channel's state may in- crease the agent's empowerment. These features could be viewed as the context or the \"environment\" relevant to the agent and can be deduced or constructedfrom thedataavail- able to the agent without the need to resort to global knowl- edge. Interestingly, these features are projections from the state of the rest of the system and thus capture information about the rest ofthe system, the \"outside\" of the agent. The conditional distribution describing the actuation channel induces a logical structure ofthe sensor and actua- tor on top of their physical structure. The logical structure is a potentially more compact description of the agent's ac- tuator in terms of its effects on the sensor, and ofthe sensor in terms of how it \"sees\" the actions. For example, actions having the same effecton the sensor may be seen asjust one logical action. We hypothesize that natural evolution makes the logical structure and the physical structure adjusted to each other. 3 Sensor and Actuator Evolution Experiments 3.1 The Testbed Consider an infinite two-dimensional square grid world. A source is located at the center of the grid. The source emits a signal, the strength P of which in any cell of the grid is P(d) = d-2, P(O) = 2, where d is the Cartesian distance from the source. An agent moves in the world occupying one cell at a time. Let us assume that the agent has a sensor which sam- ples the strength of the signal in nearby cells and picks the one with highest strength. If there are several such cells, one is picked at random with uniform probability. Let us also assume that at each time step the agent can stay put or jump into one of the nearby cells. We shall now present two scenarios: evolving a sensor for a given actuator, and evolving an actuator for a given sensor. 131 132 3.2 Sensor Evolution Let us assume that the agent's actuator allows it to either stay in the current cell or move into one ofthe four adjacent cells (von Neumann neighborhood). What is the best lay- out ofthe sensor so that the agent's n-step empowerment is maximal? Let us limit the set of sensors to those finding the cell with highest signal strength near the agent. Each sensor can be thought of as a set of sampling points relative to the agent. For example, a sensor measuring the local gra- dient using von Neumann neighborhood has four sampling points: {(O, -1), (1,0), (0, 1), (-1, 0)}. 3.2.1 Evolutionary Search To finda good enough sensorwe can search inthe setofsen- sors using an evolutionary algorithm. The algorithm treats sensor layout as an individual. The sampling points of any sensor are constrained to liewithin a fixed square with side b around the agent. The maximum number ofsampling points a sensor can have is fixed. At any pointin time, the state ofa sensor identifies the sampling point that measures the high- est signal strength. Hence, the number ofsampling points is also the number of states ofthe sensor ISI. The fitness F of a sensor is the 4-step empowerment of the agent equippedwith the sensor. The fitness alsoincludes a small penalty for the number of sampling points used IS: F =Q -4 S . (4) The penalty e = i0-' is there to prefer more economical sensors with less sampling points. The exact value of the penalty used here is arbitrary, though small. To evaluate the fitness of a sensor the agent is placed in the world at a predefined position (for instance, in the cen- ter which is at (0, 0) in Cartesian coordinates). The condi- tional probability distribution p(S4 a4) is exactly calculated from the Bayesian network [8]. The 4-step empowerment is the capacity of the channel described by the conditional probability distribution. The capacity is found with 10-4 bit precision using the iterative algorithm proposed by Blahut in [2]. We initialize the population with five randomly gener- ated sensors. In every generation, five best sensors produce five offspring each. Thus the size of the population is be- tween 5 and 30. Five best individuals from the parents and offspring are selected into the next generation. An offspring is produced from its parent by mutation. The mutation operator supports two operations: (1) addition ofa sampling point, and (2) deletion ofan existing sampling point. If the sensor has no sampling points, the mutation operatoralways adds apoint. Ifthe sensor has the maximum number of sampling points, the mutation operator always deletes a point. Otherwise, either a new point is added or an existing one is deleted with equal probability. To speed up the search and make it more efficient we have incorporated ideas from simulated annealing and tabu search: (1) the number ofmutations performed is uniformly distributed between 1 and 1 + (G mod 10), where G is the generation number; and (2) we do not add offspring which have been evaluated before or are already presentinthe pop- ulation. To sample the solution space thoroughly we run the evo- lutionary algorithm at least 10 times for 1000 generations each. The best individuals are selected across these runs. 3.2.2 Results We have evolved the sensor for different positions in the world to illustrate the ideathatempowerment makes sensors and actuators adapt to the niche in which the agent exists. We have constrained the sensor to a maximum of 20 sam- pling points which lie inside the square with side 21 around the agent. Sensors below are evolved for the corresponding places above[-Lt-+-r'l_i_4_F_l-s_L-XALg_-l_i*l H 1l F tl $t-4$<, th t tfttt t,t +1 i--e+-tt-t-p-i-t-v E tt1 t -- -t t, t jtt F-fri-t-,-^,-t, tte-f, E'-tXl-Xt'-it-t--i-4-+tHH f 0tj jo,t; t 1 t 1 1 t: nri 4 t4-T-t't-+'l++-F4-+++ 4-F+ JiL_ t0 0 4 322 \\, } (10,0) 4.322 (10,10) 3.907 (20,0) 3.248 il,t4 ---- i lo f-,- H-'--, A At! t tt,4 (20,10) 2.858 (20,20) 2.807 Figure 3: Best sensors evolved for six differentplaces in the world. Maximum number of sampling points max ISI = 20. All of the points lie inside a square with side b = 21 centered the agent. The layout is relative to the agent (cen- ter, marked with a cross). Sample points are shown in black. The textunder each layoutcontains the position in the world for which the sensor was evolved and the agent's empower- ment with the sensor measured in bits. Fig. 3 shows the best evolved sensors. Evolution has found two types of sensors. For the areas near the signal 132 133 source (Fig. 3 left and middle columns) the sensors capture more or less the absolute displacement from the source. For the areas farther away from the source (Fig. 3 right column) the sensors capture the bearing to the source. The best sensor layouts for starting positions (20, 0), (10, 10), (20, 10), and (20,20) are stable in the sense that they stay almost exactly the same ifthe evolutionary exper- iment is repeated. The other two layouts, (0,0) and (10, 0), are much more prone to change while retaining exactly the same empowerment and hence fitness. This suggests that they might belong to a plateau of the fitness landscape. Qualitatively though these layouts still comprise a cluster ofsampling points as seen in Fig. 3. The cluster is centered at the agent for the (0, 0) starting position, and on the far left for the (10, 0). 3.3 Actuator Evolution In this section we show what actuators are optimal in terms ofempowerment foragiven sensor. The experiment is simi- lar to the sensor evolution experiment described above. The same world is used. The same fitness function is used. The same evolutionary algorithm is used. However, in this ex- periment the agent has a 16-state sensor shown in Fig. 4. The sensor captures a coarse bearing to the signal source. The actuator is evolved to maximize empowerment. Figure 4: The layout of the fixed sensor used for actuator evolution. We have constrained the actuator to a maximum of 10 actions which move the agent no further than 4 cells hori- zontally and 4 cells vertically. The best actuators evolved in 10 runs of 1000 generations each are shown in Fig. 5. When we rerun the experiment evolution settles on differ- ent sets of best actuators. However, they retain identical empowerment and fitness. The variety in best evolved actu- ators is much larger compared to best evolved sensors from the previous section. Apparently, many more actuator lay- outs are optimal. There is thus less evolutionary pressure on the actuators in this experiment. Near the center of the world (Fig. 5 left and center columns) it suffices to have just four actions to completely control the sensor after four time steps. When the agent starts farther away it has to cover much longer distances in order to significantly change the bearing to the source and, hence, the sensoric reading. As the step length is limited to 4, there is not enough time to cover the necessary distance. The agent thus has less control over its sensor. For example, it cannot move to the area left from the center. The evo- lutionary algorithm guided by empowerment \"recognizes\" this situation (due to nonzero e in Eq. 4) and prunes the set of actions down to just two or three, which are enough to move in the right halfofthe world. Actuators below are evolved for the corresponding places above (0,0) 4.000 (10,0) 4.000 (20,0) 2.807 (10,10) 4.000 (20,10) 2.807 (20, 20) 2.807 Figure 5: Best actuators evolved for six different places in the world. Maximum number of action max JAI = 10. The displacement generated by any action is constrained to lie inside a square with side b = 9 centered at the agent. The layout is relative to the agent (center, marked with a cross). Places the actuator can move the agent to are shown in black. The text under each layout contains the position in the world forwhich the actuatorwas evolved and the agent's empowerment with the actuator measured in bits. We have also tried allowing the agent tojump farther in one go. In this case, empowerment for all of the tested six places in the world reaches the maximum of 4 bits, which is constrained by the capacity of the sensor. With longer jumps only four actions are sufficientto reach themaximum empowerment in each ofthe six places tried. 3.4 Summary ofthe Experiments We have presented a simple testbed to demonstrate how em- powerment could be used for sensor and actuator evolution. We have used an an evolutionary algorithm to maximize empowerment by evolving a sensor for a given actuator, and by evolving an actuator for a given sensor. We have demon- strated that: (1) using empowerment as a fitness function is possible, and (2) empowerment may allow evolution to switch from one representation of information to another one (absolute displacement vs. coarse bearing in the sensor evolution experiment). 133 134 4 Discussion & Conclusions In search for a general principle for adaptive behavior we have introduced empowerment, a natural and universal quantity based on an agent's \"embodiment\", the relation be- tween its sensors and actuators induced by the environment and the agent's morphology. Empowerment is defined for any agent, regardless of its particular sensorimotor apparatus and the environment, as the information-theoretic capacity of the actuation channel terminating at the sensors. Empowerment is zero when the agent has no control over what it is sensing, and it is higher the more perceivable controlor influence the agent has. Em- powerment can also be interpreted as the amount of infor- mation the agent could potentially \"inject\" into the envi- ronment via its actuators and later capture via its sensors. Thus, empowerment maximization can be colloquially sum- marized as \"everything else being equal, keep your options open.\" Empowerment quantifies what the agent can poten- tially do, not what it actually does. Empowerment is based on the interplay between the agent's sensors and actuators. The concept ofthe \"outside\" may arise as a result of maximizing empowerment. This agent-centric view is based on that of Gibson [5] where the agent sees the world in terms ofwhat itcan perceive and do. In this work we go further by: (1) treating actions and per- ceptions using the same formalism of Information Theory, as parts of the agent's perception-action loop, and (2) pro- viding a quantitative formalization of the degree of control or influence the agent has and is able to perceive. Gibson in [5] was against applying Information Theory to the agent's perceptions, his main argument being that the environment does not intentionally communicate any Shan- non information to observers.' However, Ashby's work [1] and also more recent work [13] show that Information The- ory can be successfully applied to perception-action with- out the need to assume any intentionality. We believe the concept of empowerment, which is in the spirit of Gibson and at the same time is quantified information-theoretically, also contributes to reconciling perception-action with Infor- mation Theory. To illustrate how empowerment influences evolution we have carried out two simple experiments. In the first exper- iment we have evolved sensors to maximize empowerment for a given actuator. The evolved sensors were adapted to the niche forwhich they were evolved. In the second experi- ment we have evolved actuators to maximize empowerment for a given sensor. Again, the actuators evolved were spe- cific and \"meaningful\" to the niche ofthe agent. Empower- ment, used as a fitness function for morphology or \"embod- \"The term information cannot have its familiar meaning ofknowledge communicated to a receiver.. The only recourse is to ask the reader to re- member that picking up information is not to be thought of as a case of communicating. The world does not speak to the observer... It [informa- tion] is simply there. The assumption that information can be transmit- ted and the assumption that it can be stored are appropriate for the theory ofcommunication, not for the theory of perception...The information for perception, unhappily, cannot be defined and measured as Claude Shan- non's information can be.\" - J. J. Gibson, The Ecological Approach to Visual Perception, [5, p. 242] iment\" evolution, can adapt the agent to its niche. However, as opposed to the majority of other fitness functions, em- powerment is agent-centric and universal. It is suitable for any sensorimotor apparatus, it is calculated from the sen- sor and actuator data available to the agent, and it does not require one to externally assign meaning to the states ofsen- sors and actuators. This, we believe, is its greatest power. Empowerment is useful for a number ofreasons. Firstly, itisdefined tobe universal, and the definition isindependent of a particular agent or its environment. Secondly, empow- erment maximization has a simple interpretation - it tells the agent to seek situations where it has perceivable con- trol or influence over the world, where it can change the world most. Thirdly, if the agent were to estimate empow- erment on-board, it would know what actions lead to what situations in the future - this knowledge could be used for standard planning or homeostasis. Last but not least, em- powerment can be calculated on-board in an agent-centric way or externally, as, for example, a fitness function in evo- lutionary search. In the latter case the agent need not know anything about empowerment - as a result of evolution the agent would just behave as though it maximizes empower- ment. Acknowledgments We would like to thank the Condor Team from the Univer- sity ofWisconsin, whose High Throughput Computing sys- tem Condor enabled us to conveniently run large numbers of simulations on ordinary workstations. We are also grateful to the anonymous reviewers of this paper for useful and constructive feedback. Bibliography [1] W. R. Ashby. An Introduction to Cybernetics. Chap- man & Hall Ltd., 1956. [2] R. Blahut. Computation of channel capacity and rate distortion functions. IEEE Transactions on Informa- tion Theory, 18(4):460-473, Jul 1972. [3] T. M. Cover and J. A. Thomas. Elements ofInforma- tion Theory. John Wiley & Sons, Inc., 1991. [4] J. P. Crutchfield and K. Young. Inferring statistical complexity. Physical Review Letters, 63(2):105-108, 1989. [5] J. J. Gibson. The Ecological Approach to Visual Per- ception. Houghton Mifflin Company, Boston, 1979. [6] A. S. Klyubin, D. Polani, and C. L. Nehaniv. Organi- zation ofthe information flow in the perception-action loop of evolved agents. In R. S. Zebulum, D. Gwalt- ney, G. Hornby, D. Keymeulen, J. Lohn, and A. Stoica, editors, Proceedings of2004 NASA/DoD Conference on Evolvable Hardware, pages 177-180. IEEE Com- puter Society, 2004. 134 135 [7] A. S. Klyubin, D. Polani, and C. L. Nehaniv. Track- ing information flow through the environment: Simple cases of stigmergy. In J. Pollack, M. Bedau, P. Hus- bands, T. Ikegami, and R. A. Watson, editors, Arti- ficial Life IX: Proceedings of the Ninth International Conference on the Simulation and Synthesis ofLiving Systems, pages 563-568. The MIT Press, 2004. [8] J. Pearl. Causality: Models, Reasoning, andInference. Cambridge University Press, 2001. [9] W. T. Powers. Behavior: The Control ofPerception. Wildwood House Ltd., 1974. [10] C. R. Shalizi and J. P. Crutchfield. Information bot- tlenecks, causal states, and statistical relavance bases: How to represent relevant information in memoryless transduction. Advances in Complex Systems, 5(l):91- 95, Mar 2002. [11] C. E. Shannon. A mathematical theory ofcommunica- tion. The Bell System Technical Journal, 27:379-423, Jul 1948. [12] N. Tishby, F. C. Pereira, and W. Bialek. The infor- mation bottleneck method. In Proceedings ofthe 37th Annual Allerton Conference on Communication, Con- trol, and Computing, pages 368-377, Sept. 1999. [13] H. Touchette and S. Lloyd. Information-theoretic ap- proach to the study of control systems. Physica A, 331(1-2):140-172, Jan. 2004. 135","libVersion":"0.3.2","langs":""}