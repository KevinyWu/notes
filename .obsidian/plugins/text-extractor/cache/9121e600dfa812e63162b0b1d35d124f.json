{"path":"video2reward/papers/2024 VPDD.pdf","text":"Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning Haoran He 1 2 Chenjia Bai 2 Ling Pan 1 Weinan Zhang 3 Bin Zhao 2 4 Xuelong Li 2 5 Abstract Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action- labeled robotic datasets. In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. In this pa- per, we introduce a novel framework that lever- ages a unified discrete diffusion to combine gen- erative pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos. We start by compressing both hu- man and robot videos into unified video tokens. In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffu- sion strategy to predict future video tokens in the latent space. In the fine-tuning stage, we harness the imagined future videos to guide low-level ac- tion learning trained on a limited set of robot data. Experiments demonstrate that our method gener- ates high-fidelity future videos for planning and enhances the fine-tuned policies compared to pre- vious state-of-the-art approaches with superior generalization ability. 1. Introduction How do we derive a general-purpose robot agent that can complete a wide variety of tasks? We believe that recent advances in vision and language give us a clue, which delves into pre-training foundation models on extremely large and diverse datasets, followed by fine-tuning on specific do- 1Hong Kong University of Science and Technology 2Shanghai Artificial Intelligence Laboratory 3Shanghai Jiao Tong University 4Northwestern Polytechnical University 5Institute of Artificial In- telligence (TeleAI), China Telecom Corp Ltd. Correspondence to: Chenjia Bai <baichenjia@pjlab.org.cn>. mains. For instance, through pre-training on internet-scale datasets (Penedo et al., 2023), large language models (Tou- vron et al., 2023a;b; OpenAI et al., 2023) and vision models (Blattmann et al., 2023; Ramesh et al., 2022; Saharia et al., 2022) showcase impressive performance on various down- stream tasks such as question answering, coding, and image generation. However, in contrast to general visual language tasks that can exploit copious amounts of data available on the Internet, embodied tasks necessitate high-quality egocen- tric data in robotics domains for precise control. Collecting such data can be expensive or time-consuming due to the reliance on robot interactions through teleoperation or kine- matic solvers (Hansen et al., 2022). While recent efforts (Bharadhwaj et al., 2023b; Walke et al., 2023a;b; Collab- oration, 2023) aim to provide a collection of robot data to cover a wide range of environments and tasks, significant gaps in embodiments and dynamics persist when applying them to specific robot tasks. In contrast to the limited avail- ability of robot data, there is a wealth of human interaction videos capturing intricate tasks and varied interactions with the physical world (Grauman et al., 2022). These videos inherently encapsulate rich semantic information regard- ing objects, environmental backgrounds, and hand-object interactions across diverse scenarios, making them poten- tially valuable for acquiring shareable knowledge relevant to embodied tasks. Motivated by this, existing methods learn image represen- tations pre-trained on human data via temporal contrastive (Nair et al., 2022; Ma et al., 2023b) or patch inpainting (Radosavovic et al., 2022; Xiao et al., 2022), while they focus on visual complexity rather than behavior learning of human activities. Other works try to understand human behaviors by learning reward functions via video-language correspondence (Chen et al., 2021a; Ma et al., 2023a) or ex- tracting hand trajectories as plans for transferring to robots (Bahl et al., 2022; Wang et al., 2023a; Shaw et al., 2022). Nevertheless, it either requires handling the domain gaps between humans and robots or necessitates precise track- ing of human hands. Recent attempts train a world model (Hafner et al., 2020; 2021) on human videos and fine-tune the model on robot data (Mendonca et al., 2023; Wu et al., 2023), while they focus on step-by-step transition modeling rather than the long-term behaviors. To effectively utilize 1arXiv:2402.14407v1 [cs.LG] 22 Feb 2024 Video Pre-Training via Discrete Diffusion Large-Scale Human Object Interaction Videos Limited Number of Robot Videos Self-Supervised Pre-training Mixed Video Data Fine-tuning Visual Control Tasks Actions for Robot Videos: {ùëé!, ùëé\", ùëé#, ‚ãØ , ùëé$} Predicted Future VideosDiscrete Diffusion videos actions Stage 1 & 2 Stage 3 {ùëé!\"#, ùëé!\"$, ‚ãØ , ùëé!\"%} Figure 1. Overall framework of our methods. The video tokens are pre-trained on a mixed dataset of human and robot data. Then the discrete diffusion model is trained on video tokens from the mixed dataset for video prediction. The action-labeled robot data is finally used to fine-tune the diffusion model for generative plan- ning, assisted by the predicted videos in the previous stage. actionless human videos for embodied agents with a limited amount of action-labeled robot data, we identify three key desiderata for effective pre-training and transfer: (i) an uni- fied visual representation for both human and robot data, reducing the domain gaps between them; (ii) video-level modeling instead of image-level representations to capture behavior patterns for planning in embodied tasks; (iii) a scalable architecture for actionless and action-labeled data, facilitating extensive domain-generic pre-training us- ing videos alone and domain-specific policy learning with videos and actions. To satisfy all three criteria, we propose a Video-based Policy learning framework via Discrete Diffusion (VPDD). VPDD bridges the visual gap between the human and robot do- mains by training on the latent space learned from a Vector- Quantized Variational Auto-Encoder (VQ-VAE) (van den Oord et al., 2017). By leveraging large-scale human-robot mixed data for pre-training, we employ video prediction to acquire the commonsense knowledge shared between hu- man and robot interactions. In contrast to existing methods that pre-train image representations, video prediction cap- tures the underlying temporal dynamics essential for long- term planning, which also enhances the policy fine-tuning with foresight from future video predictions. To tackle the challenge of modeling the noisy and multimodal distribu- tion of videos while enabling the simultaneous generation of both videos and actions, we leverage the generative capa- bility and flexible architecture offered by discrete diffusion models (Austin et al., 2023; Gu et al., 2022). Specifically, we formulate both video prediction and action learning as denoising problems within a unified discrete diffusion pro- cess (Hu et al., 2023). During the fine-tuning stage, the diffusion model leads to the capability of simultaneously generating the two-domain sequences (i.e., videos and ac- tions) for both video prediction and generative planning (Zhang et al., 2022), which forms a policy implicitly. An overview of our method is given in Figure 1. In summary, our proposed VPDD framework consists of three stages. (i) Learning compressed video tokens of both human and robot videos by employing a VQ-VAE. (ii) In the pre-training stage with actionless videos, the action tokens are masked and the diffusion model is trained to forecast video tokens in the latent space with a mask-and-replace dif- fusion strategy. The raw videos can be reconstructed using the codebook learned by VQ-VAE. (iii) In the fine-tuning stage with action-labeled robot videos, the diffusion model learns to simultaneously predict future video tokens and action tokens within the trajectory. This stage leverages ex- tensive knowledge gained through human video prediction, concentrating on training parameters specifically associated with action tokens. We conduct thorough experiments using human videos from Ego4D (Grauman et al., 2022), as well as embodied datasets from Meta-World (Yu et al., 2020) and RLBench (James et al., 2020), showcasing its ability to predict high-fidelity future videos. Furthermore, the action-prediction policy also exhibits superior performance compared to previous state-of- the-art approaches (He et al., 2023; Nair et al., 2022; Goyal et al., 2023; Shridhar et al., 2022; Chen et al., 2021b), en- compassing both seen and unseen scenes. Videos and codes are provided at https://video-diff.github.io. 2. Preliminaries 2.1. Multi-Task POMDP In this work, we consider a generalist vision-based agent that is capable of addressing multi-task predicaments, where the landscape is characterized by the inherent challenge of acquiring different skills across tasks and partial observ- ability when dealing with image inputs. Given a specific task T ‚àº p(T ), we further approach the problem as a task- specified Partially Observable Markov Decision Process (POMDP), defined as (S T , O, A, P T , R T , ¬µ T , Œ≥). Here, O is a shared observation space as we use image observa- tions for all tasks. We also assume all tasks share the same action space with the same embodiment. 2.2. Vector Quantized Model In order to unify the feature space of both human videos and robot videos, we leverage the Vector Quantized Variational Auto Encoder (VQ-VAE) (van den Oord et al., 2017) to com- press high-dimensional data points into information-rich discretized latent codes. Given a high-dimensional video segment x ‚àà RT √óH√óW √óC, the encoder E first converts it to the temporal-spatial features z = E(x) = {zm,i,l} ‚àà Rt√óh√ów√ód, where t √ó h √ó w represents the encoded se- quence length and is much smaller than T √ó H √ó W . Then we transfer the continuous features into discrete space by performing a nearest neighbors lookup in a codebook of 2 Video Pre-Training via Discrete Diffusion embeddings Z = {ej} J j=1 ‚àà RJ√ód to obtain the tokens zq = Quantize(zm,i,l) := arg minJ ‚à•zm,i,l ‚àí ej‚à•2 2, (1) where the video tokens zq ‚àà Rt√óh√ów√ód can be faithfully reconstructed via a decoder, i.e., ÀÜx = G(zq). The encoder E, decoder G, and codebook Z can be trained end-to-end via the following loss function L = ‚à•x‚àí ÀÜx‚à•1+‚à•sg[E(x)]‚àí zq‚à•2 2 + Œ≤‚à•sg[zq] ‚àí E(x)‚à•2 2, where sg denotes stop gradient. 2.3. Discrete Diffusion Model The discrete diffusion model was first proposed to deal with discrete state space with transitions converging to a binomial distribution (Sohl-Dickstein et al., 2015), and then extended to multinomial diffusion with more options for transition matrices (Hoogeboom et al., 2021; Austin et al., 2023). In this work, we utilize discrete diffusion with the absorbing state for sequence prediction of discrete tokens. Besides J tokens from a codebook, an additional [MASK] token is introduced. We denote xk as a one-hot vector identifying the token index. The forward process from xk‚àí1 to xk follows a Categorical distribution of Qkxk‚àí1, as q(xk|xk‚àí1) = Cat(xk; p = Qkxk‚àí1) = xT k Qkxk‚àí1, (2) where [Qk]m,n = q(xk = m|xk‚àí1 = n) ‚àà R(J+1)√ó(J+1) is the Markov transition matrix from k ‚àí 1 to k, which is formulated as: Qk‚àí1‚Üík = Ô£´ Ô£¨ Ô£≠ Œ±k+Œ≤k Œ≤k Œ≤k ¬∑¬∑¬∑ 0 Œ≤k Œ±k+Œ≤k Œ≤k ¬∑¬∑¬∑ 0 Œ≤k Œ≤k Œ±k+Œ≤k ¬∑¬∑¬∑ 0 ... ... ... ... ... Œ≥k Œ≥k Œ≥k ¬∑¬∑¬∑ 1 Ô£∂ Ô£∑ Ô£∏ , (3) where Œ±k ‚àà [0, 1] is the probability of retaining the to- ken, and each ordinary token has a probability of Œ≥k to be replaced by [MASK] token, leaving a chance of Œ≤k = (1 ‚àí Œ±k ‚àí Œ≥k)/J to be diffused. Importantly, due to the property of the Markov chain, we can derive the probability of xk at arbitrary timestep directly from x0 as q(xk|x0) = xT k Qkxk‚àí1, with Qk = Qk ¬∑ ¬∑ ¬∑ Q1. (4) Besides, the posterior of this diffusion process is tractable as q(xk‚àí1|xk, x0) = q(xk|xk‚àí1,x0)q(xk‚àí1|x0) q(xk|x0) = (xT k Qkxk‚àí1)(x T k‚àí1Qk‚àí1x0) xT k Qkx0 . In the reverse process, rather than explicitly predicting the posterior through a denoising neural network, the x0-parameterisation enhances stability and allows for fast inference (by skipping ‚àÜk steps per it- eration). The reverse transition with reparameterisation is formulated as pŒ∏(xk‚àí1|xk) = ‚àë Àúx0 q(xk‚àí1|xk, Àúx0)pŒ∏(Àúx0|xk), (5) where the neural network predicts the logits of the target data q(x0). 3. Methodology We commence with the pre-training of our model through future video prediction, enabling the learning of a general dynamic pattern across diverse domains. Subsequently, we fine-tune the model using a limited dataset of robot data for policy learning, leveraging foresight from predicted videos. Our framework is illustrated in Figure 2. 3.1. Data Preparing and Tokenizing Robot Data Collection. We use the rule-based script pol- icy to rollout 20 expert demonstrations for each task in Meta-World (Yu et al., 2020). We also run VPDD on 16 tasks from RLBench (James et al., 2020), a more chal- lenging benchmark involving 6-Dof manipulation that ne- cessitates multi-view images from a 3D scene to select actions. Following the multi-view manipulation frame- works (Goyal et al., 2023; Shridhar et al., 2022), we uti- lize the script motion-planner to collect 10 demonstra- tions for each task. Each demonstration from robot-data is formulated as œÑi = {v1, a1, ¬∑ ¬∑ ¬∑ , vt, at, ¬∑ ¬∑ ¬∑ , vT , aT } with vt = [ot‚àíI+1, ¬∑ ¬∑ ¬∑ , ot‚àí1, ot], where we set I = 4 through- out the paper and a denotes the actions. In the context of Meta-World, ot represents a single-view RGB image at timestep t. For RLBench, ot = {ofront t , o left t , o right t , o wrist t } comprises 4 RGB multi-view images (i.e., front, left shoul- der, right shoulder, and wrist). Consequently, in RLBench, vt is formulated as vt = {vfront t , vleft t , vright t , vwrist t }. Human Data Collection. As for human data collection, we obtain untrimmed videos from the open-sourced Ego4D dataset (Grauman et al., 2022), which contains massive- scale human-object interactions of various durations ranging from 5 seconds to 7 hours. Similar to the data preprocessing in Mu et al. (2023), we filter out videos without human- object interaction and segment each video into short clips with 8-frame intervals. Thus each video is represented as œÑi = {v1, v2, ¬∑ ¬∑ ¬∑ , vn}, where vt denotes a clip containing 4 frames. This approach yields a total of 996,177 clips of human videos, comprising approximately 4M frames. More details on data collection and processing are given in ¬ßC.1. VQ-VAE Encoding. To extract useful features from raw videos in both human and robot domains, a conventional ap- proach is to directly encode them into an embedding space using pre-trained vision models like ViT. However, these models are usually specifically trained on image dataset (Russakovsky et al., 2014), posing a significant challenge due to the domain gap with our interaction videos. Thus, we leverage VQ-VAE to compress the diverse and noisy videos into discrete latent codes, which provide a unified codebook for mixed videos and alleviate the domain gaps between human and robot videos. Formally, we adopt the VQ-VAE architecture introduced by Yan et al. (2021) for encoding videos into a discrete latent space. The codebook 3 Video Pre-Training via Discrete Diffusion Conv3D Encoder ùê∏ Quantize(‚ãÖ) Conv3D Decoder ùê∫ Codebook Large-Scale Human Videos Robot Videos ùë™ùüè ùë™ùüë ùë™ùüí ùë™ùüê ùë™ùüè ùë™ùüê ùë™ùüë ùë™ùüë ùë™ùüí ùë•ùêæ ùë•ùëò ùë•ùëò‚àí1 ùë•0 ùêå ùêå ùêå ùêå‚ãØ ‚ãØ ùêå ùêå ùë™ùüê ùêå‚ãØ ‚ãØ ùêÇùüèùêå ùë™ùüê ùë™ùüí‚ãØ ‚ãØ ùêÇùüèùë™ùüëùë™ùüê ùë™ùüí‚ãØ ‚ãØ ùëû(ùíôùëò|ùíôùëò‚àí1) ùëùùúÉ(ùíôk‚àí1|ùíôùëò, ùíö, ùíç) ‚ÄúPush and close a drawer‚Äù ‚ÄúHandle and spread cards‚Äù Instructions ùíç Historical Frames ùíö ùë° ùë•ùêæ ùë•ùëò ùë•ùëò‚àí1 ùë•0 ùëû(ùíôùëò|ùíôùëò‚àí1) ùëùùúÉ(ùíôùëò‚àí1|ùíôùëò, ùíö, ùíç) Instructions Historical Frames ùêå ùêå ùêå ùêå‚ãØ ‚ãØùêå ùêå ùêå ùêå ùë™ùüê ùêå‚ãØ ‚ãØùêå ùêÇùüï ùêÇùüè ùêå ùë™ùüê ùë™ùüí‚ãØ ‚ãØ ùêÇùüïùêÇùüï ùêÇùüèùë™ùüëùë™ùüê ùë™ùüí‚ãØ ‚ãØùêÇùüï ùêÇùüñ video tokens Stage 1: Video Tokenizing Stage 2: Pre-Training with Mixed Data Stage 3: Fine-Tuning with Robot Data CLIP ùë∑ùúΩùüè Denoise Video Tokens Perceiver IO Transformer ùë∑ùúΩùüê Denoise Action Tokens GPT2 Transformer Predicted Future Videos ùë∑ùúΩùüè Denoise Video Tokens Perceiver IO Transformer ùë∑ùúΩùüê Denoise Action Tokens GPT2 Transformerùë° CLIP execute ŒîT = 0.2, ‚àí0.1,0.15 gripper = [1] Predicted Actions ‡∑•ùíô0 v ‡∑•ùíô0 a ùíõ ‡∑•ùíô 0 v ùêå ùêå ùêå ùêå ùêå ùêå ùêå ùêå Figure 2. Overall pipeline of VPDD. A video-based VQ-VAE is leveraged to encode both human and robot videos into discrete latent codes. Subsequently, a unified discrete diffusion is firstly pre-trained on these video latent codes via a self-supervised objective, predicting future videos conditioning on language instructions and historical videos. The pre-trained video prediction model pŒ∏1 can capture temporal dynamics and task-specific representations. Lastly, we fine-tune our diffusion model on a limited number of robot data. In each diffusion step of the fine-tuning stage, we leverage pŒ∏1 to provide hidden representations z Àúxv 0 to accelerate downstream action learning with video foresight. This integration of video prediction and action learning is achieved through our unified discrete diffusion. comprises 2048 codes, each represented by 256-dimensional embeddings. The encoder architecture consists of a series of 3D convolutions that downsample by a factor of 4 over space-time (resulting in a 64√ó total reduction), followed by 6 attention residual blocks. Consequently, each video clip vt ‚àà {œÑi} is embedded into latent codes et. The architec- ture for the decoder is the reverse of the encoder, featuring attention residual blocks followed by an equivalent number of 3D transposed convolutions for upsampling over space- time. The VQ-VAE is pre-trained on large-scale videos and remains fixed in the subsequent processes, providing flexibility for various downstream utilization methods. Action Discretizing. For subsequent pre-training and fine- tuning, we process the collected continuous actions via uniform action discretization (Janner et al., 2021; Brohan et al., 2023). In the case of Meta-World, the action space is a 2-tuple consisting of the change in the 3D space of the end- effector followed by a normalized torque that the gripper fingers should apply. Here all the continuous dimensions are discretized into 48 bins uniformly. Thus, the robot action can be represented using ordinals of the discrete bins as a 4 integer number. For RLBench, an action consists of the gripper open state and 6-DoF pose including position and rotation. The position is discretized into 360 bins, and rotation is discretized into Euler angles as 1-degree bins for each of the 3 rotation axes (Shridhar et al., 2022). Gripper open state is a binary value. 3.2. Video Prediction via Unified Discrete Diffusion Extracting general patterns useful for downstream decision- making from large-scale in-the-wild human videos is chal- lenging, primarily because of the absence of labeled actions and the complexity of the underlying structure of human interactions. Different from previous ways of learning a vi- sual representation, we propose a novel objective to further unleash the representation and temporal modeling ability of diffusion models. Specifically, after obtaining discrete tokens from VQ-VAE encoding, we train a unified discrete diffusion model on the latent space via a self-supervised objective. This objective involves predicting future videos based on observed historical videos for both humans and robots, while masking action tokens. Benefiting from the proposed objective and the x0-parameterisation of discrete diffusion, the diffusion model is incentivized to capture both the high-level temporal dynamics and the low-level visual commonalities between historical and future videos 4 Video Pre-Training via Discrete Diffusion at each diffusion step. Then the acquired knowledge can be leveraged to guide action denoising at each step. Unified Transition Matrix. The presence of a transition matrix determines the nature of the discrete diffusion model (Austin et al., 2023). While the original discrete diffusion is limited to one modality, drawing inspiration from UniD3 (Hu et al., 2023), which enhances the transition matrix to encompass both images and text, we construct a unified transition matrix to capture global connections between the two modalities‚Äîvideos and actions. The matrix [Qk]m,n below illustrates the unified transition process: Qk = Ô£Æ Ô£Ø Ô£Ø Ô£Ø Ô£Ø Ô£Ø Ô£Ø Ô£Ø Ô£Ø Ô£Ø Ô£Ø Ô£Ø Ô£Ø Ô£Ø Ô£Ø Ô£Ø Ô£∞ Œ±k + Œ≤k Œ≤k ¬∑ ¬∑ ¬∑ Œ≤k 0 0 ¬∑ ¬∑ ¬∑ 0 0 Œ≤k Œ±k + Œ≤k ¬∑ ¬∑ ¬∑ Œ≤k 0 0 ¬∑ ¬∑ ¬∑ 0 0 ... ... . . . ... ... ... . . . ... ... Œ≤k Œ≤k ¬∑ ¬∑ ¬∑ Œ±k + Œ≤k 0 0 ¬∑ ¬∑ ¬∑ 0 0 0 0 ¬∑ ¬∑ ¬∑ 0 Œ±k + Œ≤‚àó k Œ≤‚àó k ¬∑ ¬∑ ¬∑ Œ≤‚àó k 0 0 0 ¬∑ ¬∑ ¬∑ 0 Œ≤‚àó k Œ±k + Œ≤‚àó k ¬∑ ¬∑ ¬∑ Œ≤‚àó k 0 ... ... . . . ... ... ... . . . ... ... 0 0 ¬∑ ¬∑ ¬∑ 0 Œ≤‚àó k Œ≤‚àó k ¬∑ ¬∑ ¬∑ Œ±k + Œ≤‚àó k 0 Œ≥k Œ≥k ¬∑ ¬∑ ¬∑ Œ≥k Œ≥k Œ≥k ¬∑ ¬∑ ¬∑ Œ≥k 1 Ô£π Ô£∫ Ô£∫ Ô£∫ Ô£∫ Ô£∫ Ô£∫ Ô£∫ Ô£∫ Ô£∫ Ô£∫ Ô£∫ Ô£∫ Ô£∫ Ô£∫ Ô£∫ Ô£ª , where Œ≤k and Œ≤‚àó k are the probabilities of a token to be re- placed by any other accessible tokens in different modalities. The dimension of Qk is (J + J ‚àó + 1) √ó (J + J ‚àó + 1), where J and J ‚àó are the number of tokens in different modalities, i.e., J is the size of codebook in VQ-VAE and J ‚àó is the number of action classes in discretization. The sum of each column in this transition matrix is one to preserve probabil- ity mass. Mathematically, we have Œ≤k = (1 ‚àí Œ±k ‚àí Œ≥k)/J and Œ≤‚àó k = (1 ‚àí Œ±k ‚àí Œ≥k)/J ‚àó. All the mass of the stationary distribution falls on the [MASK] token, which satisfies the prerequisite for a discrete diffusion model transition matrix (Austin et al., 2023). The details of the diffusion process are provided in ¬ßA.1. Unified Objective. We cast both video prediction and ac- tion learning as a conditional generative problem, and the goal is to maximize EœÑ ‚àº‚à™iœÑi[ log pŒ∏(x0(œÑ ) ‚à£ ‚à£ y(œÑ ), l] . Here x = [x v, xa], where xv = [et+h+1, ¬∑ ¬∑ ¬∑ , et+h+M ] rep- resents future video segments with M clips, and x a = [at, ¬∑ ¬∑ ¬∑ , at+H‚àí1] denotes action sequences of H steps. y = et serves as the condition containing historical video tokens. l is the language instructions describing current tasks. In practice, we train two separate denoising networks, namely pŒ∏1 (xv k‚àí1|xk, y, l) and pŒ∏2 (xa k‚àí1|xk, z Àúxv 0 , l), to learn videos and actions, respectively. Here, z Àúxv 0 represents the hidden representation of predicted future videos given by pŒ∏1 at each diffusion step, which is utilized to guide ac- tion learning. Formally, Àúxv 0 = Softmax(MLP(z Àúxv 0 )). As the actions are absent during the pre-training stage, we mask xa and freeze pŒ∏2 , as illustrated in Figure 2. The network is trained to minimize the following variational lower bound (VLB) (Sohl-Dickstein et al., 2015; Gu et al., 2022): Lvlb = L0 + ‚àëK k=2 Lk‚àí1 + LK, (6) where L0 = ‚àíEq(x1|x0) [ log pŒ∏1(x v 0|x1, y, l)+log pŒ∏2 (xa 0|x1, z Àúxv 0 , l] ] , Lk‚àí1 = Eq(xk|x0)[DKL(q(xk‚àí1|xk, x0) ‚à• ‚à• [pŒ∏1(xv k‚àí1|xk, y, l); pŒ∏2 (xa k‚àí1|xk, z Àúxv 0 , l)])], LK = Eq(x0) [ DKL (q(xK|x0) ‚à• ‚à• p(xK) )] . LK is a constant number that can be ignored in the training, as the prior distribution p(xK) is fixed: p(xK) = [ ¬ØŒ≤K, ¬ØŒ≤K, ¬∑ ¬∑ ¬∑ , ¬ØŒ≤‚àó K, ¬ØŒ≤‚àó K, ¬∑ ¬∑ ¬∑ , ¬ØŒ≥K]. (7) Note that pŒ∏1(xv k‚àí1) and pŒ∏2(x a k‚àí1) are obtained with x0- parameterization as follows, pŒ∏1(xv k‚àí1|xk, y, l) = ‚àë Àúxv 0 q(x v k‚àí1|xk, Àúx v 0)pŒ∏1( Àúx v 0|xk, y, l), (8) pŒ∏2(xa k‚àí1|xk, z Àúxv 0 , l) = ‚àë Àúxa 0 q(xa k‚àí1|xk, Àúxa 0)pŒ∏2( Àúx a 0|xk, z Àúxv 0 , l) (9) Model Architecture. As in Eq. (8), the neural network pŒ∏1 receives xk, history y, language l and the diffusion timestep k as inputs. These inputs are individually embedded into embeddings h of size d via separate MLPs f , depicted as: hl = fl(CLIP(l)), hT i = fT i(k), hxk = fxk (xk), hy = fy(y), where language instructions l is encoded with CLIP‚Äôs lan- guage encoder (Radford et al., 2021). Afterwards, the embeddings are formulated as input tokens as htokens = LN(hT i √ó [hl, hT i, hxk , hy] + Epos), where Epos is the positional embedding, and LN denotes layer normalization (Ba et al., 2016) for stabilizing training. The input sequence that represents a video can be extremely long so a standard Transformer with O(n2) complexity is hard to fit. We adopt Perceiver Transformer (Jaegle et al., 2022) to tackle this problem, as it has been widely utilized for modeling long se- quences (Shridhar et al., 2022; Goyal et al., 2023). Perceiver is a latent-space Transformer, where instead of attending to the entire input, it computes cross-attention between the input and a much smaller set of latent vectors (which are randomly initialized and trained). These latents are encoded with self-attention layers, and are cross-attended with the input to match the size for the final outputs. More details about the Perceiver Transformer are referred to ¬ßA.2. 3.3. Learning to Act via Few-Shot Fine-Tuning During the fine-tuning stage, we leverage a limited dataset of robot data, including both videos and actions, for rapid adaptation. Both x v and xa attend to training the diffu- sion model. Given that pŒ∏1 has been trained sufficiently to capture fruitful information to predict future videos from history, we freeze pŒ∏1 and solely tune parameters of pŒ∏2 to minimize Lvlb. As expressed in Eq. (9), the input of pŒ∏2 consists of xk, language l, hidden representation z Àúxv 0 , and 5 Video Pre-Training via Discrete Diffusion diffusion timestep k. In this case, we are tasked with pre- dicting a sequence of action tokens xa 0, considerably shorter than video-token sequence xv 0, so we employ GPT2 (Rad- ford et al., 2019) Transformer to process tokens embedded with MLPs. GPT2 has demonstrated an impressive ability to solve multi-task problems and model multimodal distri- butions. The model architecture of pŒ∏2 closely resembles that of MTDIFF-P (He et al., 2023). Fast Denoising Strategy. In the denoising (inference) stage, the x0-parameterisation within discrete diffusion enables an expedited inference process, as discussed in (Gu et al., 2022). Concretely, rather than sampling tokens in the chain of xK, xK‚àí1, ¬∑ ¬∑ ¬∑ , x0, we can sample tokens in the chain of xK, xK‚àí‚àÜk, ¬∑ ¬∑ ¬∑ , x0 with the reverse transition: pŒ∏1(xv k‚àí‚àÜk|xk, y, l) = ‚àë Àúxv 0 q(xv k‚àí‚àÜk|xk, Àúxv 0)pŒ∏1( Àúx v 0|xk, y, l), pŒ∏2(xa k‚àí‚àÜk|xk, z Àúxv 0 , l) = ‚àë Àúxa 0 q(xa k‚àí‚àÜk|xk, Àúx a 0)pŒ∏2 ( Àúxa 0|xk, z Àúxv 0 , l) 4. Related Work Robot Learning from Human Videos. Leveraging human videos (Goyal et al., 2017; Fang et al., 2018; Damen et al., 2021; Grauman et al., 2022) for policy learning is promis- ing to extract commonsense knowledge from human activi- ties, which can be shared to embodied scenarios that suffer from scarce robot data (Collaboration, 2023; Fang et al., 2023). Since the human data is actionless and the domain gap between humans and robots exists, a main branch of research employs human video to learn shareable visual rep- resentations (Burns et al., 2023; Jing et al., 2023) via time- contrastive (Ma et al., 2023b), video-language alignment (Nair et al., 2022; Karamcheti et al., 2023), value function (Bhateja et al., 2023), and perceptual skills (Huo et al., 2023; Liang et al., 2023). Visual affordance like human-object interaction hotspots (Liu et al., 2022; Goyal et al., 2022) and the post-grasp trajectory (Bahl et al., 2023) are also helpful for embodied agents in goal-conditioned imitation. Alter- native methods involve extracting hand trajectories (Bahl et al., 2022; Wang et al., 2023a) or keypoints (Xiong et al., 2021) to transfer plans to robots. Different from the above methods, we eliminate the domain gaps by learning video tokens and representations for video prediction, which im- plicitly captures visual features, affordances, and long-term plans. Other works attempt to infer actions from videos via inverse kinematics (Bharadhwaj et al., 2023a; Ko et al., 2024), whereas we learn action prediction through policy fine-tuning without external models. Pretraining for Generalized Policy Learning. Early works of policy adaptation emerged in meta-RL (Gupta et al., 2018; Yuan & Lu, 2022), while the pre-training and fine- tuning environments are assumed to be similar. Leveraging the transformer architecture, works perform pre-training in multi-task datasets by optimizing the multi-task policy (Lee et al., 2022; Reed et al., 2022; Taiga et al., 2023; Thomas et al., 2023) or self-supervised objectives (Sun et al., 2023; Seo et al., 2022; Mendonca et al., 2023). In tasks involv- ing visual observations, methods adopt visual tokens for transformer-based multi-task policy learning (Bousmalis et al., 2023; Brohan et al., 2023) and adaptation (Lin et al., 2023). Additionally, some studies pre-train a reward func- tion through video-language correspondence (Chen et al., 2021a; Ma et al., 2023a) or diffusion models (Escontrela et al., 2023; Huang et al., 2023) for downstream RL training. Different from the previous Transformer and continuous diffusion frameworks, our work first integrates visual tokens with discrete diffusion to predict consistent videos and ac- tions simultaneously. Concurrently, GR-1 (Wu et al., 2024) utilizes human data to pre-train a GPT-style architecture for predicting future observations. In contrast, we perform video prediction instead of step-by-step image prediction using a unified discrete diffusion architecture. Diffusion Models for RL. Diffusion models are a powerful family of generative models (Saharia et al., 2022; Ramesh et al., 2022) that can be categorized into continuous Gaus- sian diffusion models and discrete diffusion models that handle discrete visual tokens or symbols (Gu et al., 2022). Continuous diffusion models have found extensive applica- tions as multi-modal policies (Wang et al., 2023b; Pearce et al., 2023; Chi et al., 2023), environmental dynamics (Yang et al., 2024), and planners to generate action (Janner et al., 2022; Xian et al., 2023) or state sequences (Ajay et al., 2023; Chen et al., 2023), guided by desired properties. Several methods also extend the diffusion models for multi-task learning (He et al., 2023; Ni et al., 2023; Dong et al., 2024) with low-dimensional states, while we specifically address the more challenging image-based setting. UniPi (Du et al., 2023) is related to our method by performing video genera- tion via continuous diffusion while we adopt a more flexible architecture with discrete diffusion to seamlessly connect the pre-training and fine-tuning stages, without relying on a task-specific inverse-dynamic model for acting. Addi- tionally, we train the model on video tokens that maintain temporal consistency, while UniPi relies on super-resolution to improve the time consistency of generated frames. 5. Experiments 5.1. Bnechmarks and Baselines After the video pertaining in Ego4D (Grauman et al., 2022), we use the following robotic benchmark to learn to evaluate. Meta-World. The Meta-World benchmark (Yu et al., 2020) contains 50 distinct manipulation tasks that require a Sawyer robot to interact with various objects with different shapes, joints, and connectivity. The action is the 3D position move- ments of the robot‚Äôs end effector and the gripper openness. We follow recent works (Yang et al., 2020; Sun et al., 2022) 6 Video Pre-Training via Discrete Diffusion Figure 3. Single-view and multi-view images from Meta-World button-press and RLBench meat-off-grill tasks, sampled from videos predicted by pŒ∏1 . Meta-World 0 10 20 30 40 50Success Rate (%) 56.57 ¬± 0.41 52.93 ¬± 0.55 38.59 ¬± 0.74 48.61 ¬± 0.62 30.27 ¬± 0.48 45.90 ¬± 0.67 VPDD (Ours) VPDD-w/o-human MTDiff-p R3M-Diffusion Video-MTDT SODA Figure 4. Average success rate across 3 seeds on MT50- rand. Each task is evaluated for 50 episodes. to extend the original environment to a more challenging setting with random goals, and refer to it as MT50-rand. We train the policy with 20 demonstrations per task, and report the average success rates on 50 evaluation episodes per task. RLBench. RLBench (James et al., 2020) is a more chal- lenging 3D manipulation benchmark with diverse tasks con- cerning interactions with a wide range of objects. We select 16 tasks from RLBench to evaluate our method, where each task has several possible variations, such as the shapes, col- ors, sizes and positions of objects. The input observations are captured from four RGB cameras positioned at the front, left shoulder, right shoulder, and wrist. The action is an 8-dimensional vector including 3-dimensional transitions, 4-dimensional quaternion, and a binary value about gripper openness. We follow the convention by using macro steps (James & Davison, 2022), which are key turning points in the action trajectory where the gripper changes its state (open/close) or the joint velocities approach to zero. We train the policy with 10 demonstrations per task and report average success rates on 25 evaluation episodes per task. Baselines for Meta-World. We compare the proposed method VPDD with the following baselines: (i) R3M- Diffusion is a discrete diffusion model sharing identical architecture with pŒ∏2 , leveraging the R3M (Nair et al., 2022) ResNet50 encoder to encode images as input. R3M is also trained on Ego4D videos and stands as the state-of-the-art (SOTA) visual representation specifically designed for ma- nipulation tasks; (ii) MTDIFF-P (He et al., 2023) is the SOTA method for multi-task RL, which employs a continu- ous diffusion model with a Transformer architecture. Since it is designed to handle state-based input, we employ the R3M encoder to extract hidden embeddings from images, which are then fed into MTDIFF-P; (iii) Video-MTDT is an extension of Decision Transformer (MT) (Chen et al., 2021b), learning from multi-task data with video tokens as input; (iv) VPDD-w/o.-human excludes human videos during pre-training, remaining other parts unchanged. This baseline helps to ablate the effect of pre-train on large-scale human videos; (v) SODA (Hudson et al., 2023) is a recently proposed diffusion-based representation method that em- Table Texture Camera Pos 0 10 20 30 40 50Success Rate (%) VPDD (Ours) VPDD-w/o-human R3M-Diffusion Figure 5. Average success rate across 3 seeds on button-press-v2 and handle-press-v2 tasks, where table texture and camera position are shifted to evaluate the generalization of different methods. ploys an encoder to generate representations z from input images to support the denoising process. We pre-train the encoder by employing the video prediction objective on the same dataset and subsequently feed the learned z into pŒ∏2 during fine-tuning. See more details in ¬ßB. Baselines for RLBench. Learning policies for RLBench is more challenging as it requires understanding the 3D scene structure for predicting the 6D poses of end-effectors. The baselines used in Meta-World all fail in the benchmark since they are disadvantaged with single-view observations. In contrast, VPDD can predict multi-view images, implicitly recovering the 3D geometry in manipulation. Thus, we use the following SOTA imitation architectures designed for 3D manipulation: (i) RVT (Goyal et al., 2023) stands as the SOTA method, initially re-rendering visual observations into orthographic projections of cube views and subsequently predicting the next move based on these multi-view projec- tions.; (ii) PERACT (Shridhar et al., 2022) encodes RGB-D images into voxel grid patches for 3D representation and predicts the action using the Perceiver Transformer. 5.2. Experimental Results Question 1. Does our pre-trained diffusion model (i.e., pŒ∏1 ) capable of generating high-fidelity future videos? Although our primary contribution is not about video gener- ation, it remains crucial to predict consistent future videos to aid in policy fine-tuning. The predicted raw videos are visually depicted in Fig. 3, with more video samples acces- sible at https://video-diff.github.io. These 7 Video Pre-Training via Discrete Diffusion Method slide block sweep to dustpan meat off grill turn tap put in drawer close jar drag stick stack blocks PERACT (10 demos) (Shridhar et al., 2022) 32 72 68 72 16 32 36 12 RVT (10 demos) (Goyal et al., 2023) 54.67 ¬± 1.89 76.0 ¬± 3.27 69.33 ¬± 6.80 96.0 ¬± 3.27 91.33 ¬± 6.60 22.67 ¬± 4.99 97.33 ¬± 1.89 5.33 ¬± 1.89 VPDD 70.67 ¬± 1.89 40 ¬± 3.27 73.33 ¬± 4.99 88.67 ¬± 3.77 24.0 ¬± 3.72 37.33 ¬± 8.22 66.67 ¬± 1.89 56.0 ¬± 5.66 Method screw bulb put in safe place wine put in cupboard push buttons insert peg stack cups place cups PERACT (10 demos) (Shridhar et al., 2022) 28 16 20 0 56 4 0 0 RVT (10 demos) (Goyal et al., 2023) 28.0 ¬± 3.27 38.67 ¬± 4.99 45.33 ¬± 6.80 5.33 ¬± 1.89 65.33 ¬± 1.89 2.67 ¬± 1.89 2.67 ¬± 1.89 0 ¬± 0 VPDD 61.33 ¬± 6.80 70.67 ¬± 1.89 60.0 ¬± 6.53 30.67 ¬± 4.99 58.67 ¬± 1.89 73.33 ¬± 1.89 56.0 ¬± 5.66 30.67 ¬± 1.89 Table 1. Success rates (mean and std %) across 3 random seeds of various multi-task agents trained with 10 demonstrations and evaluated on 25 episodes per task. VPDD outperforms SOTA methods of RLBench, i.e., PERACT and RVT, with an average improvement of 1.24√ó. Note that VPDD only takes RGB images as input while both RVT and PERACT utilize additional depth images as inputs. 0 1 5 10 15 20 Number of Demonstrations 25 30 35 40 45 50 55Success Rate (%) Figure 6. Average success rate across 3 seeds on MT50-rand, where VPDD is trained on a different number of demonstrations. videos are reconstructed from predicted video tokens by using the decoder of VQ-VAE. After pre-training, the video- prediction model pŒ∏1 demonstrates the capability to generate dynamically consistent single-view videos for Meta-World and multi-view videos for RLBench. We attribute the capa- bility of generating high-quality videos to the well-trained video codebook and the proposed discrete diffusion model learned from large-scale human data. Question 2. How does VPDD compare to other offline baselines for vision-based multi-task decision-making? To evaluate the learned policy after fine-tuning, we take the first action generated by pŒ∏2 to interact with the environ- ment. The results on Meta-World and RLBench are referred to Fig. 4 and Table 1 respectively, yielding the following key findings: (i) VPDD outperforms other SOTA methods in success rate by a large margin. For Meta-World, VPDD performs the best across 50 tasks with random goals. For RLBench, VPDD even outperforms the SOTA imitation ar- chitectures based on voxel and multi-view representations that are carefully designed for 3D manipulation, which usu- ally require point clouds or 3D world rendering for scene understanding. Notably, VPDD achieves a remarkable suc- cess rate on put in cupboard, insert peg, stack cups and place cups, while both RVT and PERACT struggle on these challenging tasks. This verifies the efficacy of video pre- training for few-shot policy fine-tuning; (ii) According to Fig. 4, VPDD obtains a 6.9% relative improvement through pre-training on both human and robot videos compared with VPDD-w/o.-human. Furthermore, VPDD surpasses R3M with a notable 16.4% higher success rate, demonstrating the potential of our diffusion representation via video pre- diction; (iii) R3M-Diffusion outperforms MTDIFF-P which employ R3M encoder with continuous diffusion architecture by 26.0%, and Video-MTDT with Transformer architecture by 60.6%. This highlights the superior capacity of discrete diffusion models compared to other approaches. Question 3. How does VPDD generalize to unseen scenes? Following Xie et al. (2023), we alter the camera position and table texture in the visual scene of Meta-World to as- sess the generalization ability of our method. According to Fig. 5, VPDD exhibits superior generalizability, attributed to the training of the diffusion representation on large-scale diverse human videos. Regarding the shift in camera po- sition, VPDD outperforms the VPDD-w/o.-human by 63% and R3M by 252%. Detailed settings can be found in ¬ßC.3. Question 4. Can VPDD maintain satisfactory performance when provided with fewer robotic demonstrations? Leveraging the large-scale video pre-training, VPDD can learn the policy using only a small number of demonstra- tions. To validate the sample efficiency of our method, we conduct an ablation study on the number of demonstrations used in the fine-tuning stage. The results, depicted in Fig. 6, reveal that the performance of VPDD exhibits linear growth after training on 5 or more demonstrations, indicating the potential for VPDD to achieve better performance with in- creased demonstration data. Moreover, VPDD maintains a comparable success rate even when only 1 demonstration is used in the fine-tuning process. Question 5. Does VPDD outperforms other diffusion- based representation learning method? To verify the representation capability inherent in our unified discrete diffusion framework, we reproduce SODA (Hudson et al., 2023), which serves as a strong diffusion representa- tion method. As shown in Fig. 4, VPDD outperforms SODA in the context of policy fine-tuning. We hypothesize that VPDD provides coarse-to-fine representations (i.e., z Àúxv 0 ) throughout the action-denoising steps from K ‚Üí 1, exactly encapsulating useful information the denoising network fo- cuses on at each step (Ho et al., 2020). In contrast, SODA produces representation z that remains constant across all steps during action denoising. 8 Video Pre-Training via Discrete Diffusion 6. Conclusion We propose VPDD, a video-based policy learning frame- work via discrete diffusion. With a VQ-VAE tokenizer, we bridge the gap between human and robot videos by a discrete latent codebook. We leverage a unified discrete diffusion for pre-training on large-scale actionless mixture videos and subsequent fine-tuning the policy on a limited number of robot demonstrations. Experiments demonstrate that VPDD achieves superior performance on a variety of challenging manipulation tasks and showcases impressive generalization ability benefited from human video prediction. Discussions of limitations and future works are given in ¬ßD. Broader Impact This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. References Ajay, A., Du, Y., Gupta, A., Tenenbaum, J. B., Jaakkola, T. S., and Agrawal, P. Is conditional generative model- ing all you need for decision making? In International Conference on Learning Representations, 2023. Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and van den Berg, R. Structured denoising diffusion models in discrete state-spaces, 2023. Ba, L. J., Kiros, J. R., and Hinton, G. E. Layer normalization. CoRR, abs/1607.06450, 2016. URL http://arxiv. org/abs/1607.06450. Bahl, S., Gupta, A., and Pathak, D. Human-to-robot imita- tion in the wild. 2022. Bahl, S., Mendonca, R., Chen, L., Jain, U., and Pathak, D. Affordances from human videos as a versatile repre- sentation for robotics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13778‚Äì13790, 2023. Bharadhwaj, H., Gupta, A., Tulsiani, S., and Kumar, V. Zero-shot robot manipulation from passive human videos. arXiv preprint arXiv:2302.02011, 2023a. Bharadhwaj, H., Vakil, J., Sharma, M., Gupta, A., Tul- siani, S., and Kumar, V. Roboagent: Generalization and efficiency in robot manipulation via semantic aug- mentations and action chunking. In First Workshop on Out-of-Distribution Generalization in Robotics at CoRL 2023, 2023b. Bhateja, C., Guo, D., Ghosh, D., Singh, A., Tomar, M., Vuong, Q., Chebotar, Y., Levine, S., and Kumar, A. Robotic offline RL from internet videos via value- function pre-training. In NeurIPS 2023 Foundation Mod- els for Decision Making Workshop, 2023. Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Bousmalis, K., Vezzani, G., Rao, D., Devin, C., Lee, A. X., Bauza, M., Davchev, T., Zhou, Y., Gupta, A., Raju, A., et al. Robocat: A self-improving foundation agent for robotic manipulation. arXiv preprint arXiv:2306.11706, 2023. Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., and et al. Rt-1: Robotics transformer for real-world control at scale. In Robotics: Science and Systems, 2023. Burns, K., Witzel, Z., Hamid, J. I., Yu, T., Finn, C., and Hausman, K. What makes pre-trained visual representa- tions successful for robust manipulation? arXiv preprint arXiv:2312.12444, 2023. Chen, A. S., Nair, S., and Finn, C. Learning generaliz- able robotic reward functions from‚Äù in-the-wild‚Äù human videos. arXiv preprint arXiv:2103.16817, 2021a. Chen, H., Lu, C., Ying, C., Su, H., and Zhu, J. Offline reinforcement learning via high-fidelity generative behav- ior modeling. In International Conference on Learning Representations, 2023. Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. De- cision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084‚Äì15097, 2021b. Chi, C., Feng, S., Du, Y., Xu, Z., Cousineau, E., Burch- fiel, B., and Song, S. Diffusion policy: Visuomotor policy learning via action diffusion. arXiv preprint arXiv:2303.04137, 2023. Collaboration, O. X. Open x-embodiment: Robotic learning datasets and RT-X models. CoRR, abs/2310.08864, 2023. Damen, D., Doughty, H., Farinella, G. M., Fidler, S., Furnari, A., Kazakos, E., Moltisanti, D., Munro, J., Per- rett, T., Price, W., and Wray, M. The epic-kitchens dataset: Collection, challenges and baselines. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 43(11):4125‚Äì4141, 2021. doi: 10.1109/TPAMI.2020. 2991965. 9 Video Pre-Training via Discrete Diffusion Dong, Z., Yuan, Y., Hao, J., Ni, F., Mu, Y., Zheng, Y., Hu, Y., Lv, T., Fan, C., and Hu, Z. Aligndiff: Aligning diverse human preferences via behavior-customisable diffusion model. In International Conference on Learning Repre- sentations, 2024. Du, Y., Yang, S., Dai, B., Dai, H., Nachum, O., Tenenbaum, J. B., Schuurmans, D., and Abbeel, P. Learning univer- sal policies via text-guided video generation. In Neural Information Processing Systems, 2023. Escontrela, A., Adeniji, A., Yan, W., Jain, A., Peng, X. B., Goldberg, K., Lee, Y., Hafner, D., and Abbeel, P. Video prediction models as rewards for reinforcement learning. In Neural Information Processing Systems, 2023. Fang, H.-S., Fang, H., Tang, Z., Liu, J., Wang, C., Wang, J., Zhu, H., and Lu, C. Rh20t: A comprehensive robotic dataset for learning diverse skills in one-shot. Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition@ CoRL2023, 3:5, 2023. Fang, K., Wu, T.-L., Yang, D., Savarese, S., and Lim, J. J. Demo2vec: Reasoning object affordances from online videos. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 2139‚Äì2147, 2018. Goyal, A., Xu, J., Guo, Y., Blukis, V., Chao, Y.-W., and Fox, D. RVT: Robotic view transformer for 3d object manip- ulation. In 7th Annual Conference on Robot Learning, 2023. Goyal, M., Modi, S., Goyal, R., and Gupta, S. Human hands as probes for interactive object understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3293‚Äì3303, 2022. Goyal, R., Ebrahimi Kahou, S., Michalski, V., Materzynska, J., Westphal, S., Kim, H., Haenel, V., Fruend, I., Yianilos, P., Mueller-Freitag, M., et al. The‚Äù something something‚Äù video database for learning and evaluating visual com- mon sense. In Proceedings of the IEEE international conference on computer vision, pp. 5842‚Äì5850, 2017. Grauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., Liu, X., et al. Ego4d: Around the world in 3,000 hours of egocentric video. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18995‚Äì19012, 2022. Grauman, K., Westbury, A., Torresani, L., Kitani, K., Malik, J., Afouras, T., Ashutosh, K., Baiyya, V., Bansal, S., Boote, B., et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. arXiv preprint arXiv:2311.18259, 2023. Gu, S., Chen, D., Bao, J., Wen, F., Zhang, B., Chen, D., Yuan, L., and Guo, B. Vector quantized diffusion model for text-to-image synthesis. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10696‚Äì 10706, 2022. Gupta, A., Mendonca, R., Liu, Y., Abbeel, P., and Levine, S. Meta-reinforcement learning of structured exploration strategies. Advances in neural information processing systems, 31, 2018. Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2020. Hafner, D., Lillicrap, T. P., Norouzi, M., and Ba, J. Mas- tering atari with discrete world models. In International Conference on Learning Representations, 2021. Hansen, N. A., Su, H., and Wang, X. Temporal differ- ence learning for model predictive control. In Interna- tional Conference on Machine Learning, pp. 8387‚Äì8406. PMLR, 2022. He, H., Bai, C., Xu, K., Yang, Z., Zhang, W., Wang, D., Zhao, B., and Li, X. Diffusion model is an effective planner and data synthesizer for multi-task reinforce- ment learning. In Neural Information Processing Systems, 2023. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba- bilistic models. Advances in neural information process- ing systems, 33:6840‚Äì6851, 2020. Hoogeboom, E., Nielsen, D., Jaini, P., Forr¬¥e, P., and Welling, M. Argmax flows and multinomial diffusion: Learning categorical distributions. In Advances in Neural Informa- tion Processing Systems, 2021. Hu, M., Zheng, C., Yang, Z., Cham, T.-J., Zheng, H., Wang, C., Tao, D., and Suganthan, P. N. Unified discrete dif- fusion for simultaneous vision-language generation. In International Conference on Learning Representations, 2023. Huang, T., Jiang, G., Ze, Y., and Xu, H. Diffusion reward: Learning rewards via conditional video diffusion. arXiv preprint arXiv:2312.14134, 2023. Hudson, D. A., Zoran, D., Malinowski, M., Lampinen, A. K., Jaegle, A., McClelland, J. L., Matthey, L., Hill, F., and Lerchner, A. Soda: Bottleneck diffusion models for representation learning. arXiv preprint arXiv:2311.17901, 2023. Huo, M., Ding, M., Xu, C., Tian, T., Zhu, X., Mu, Y., Sun, L., Tomizuka, M., and Zhan, W. Human-oriented representation learning for robotic manipulation, 2023. 10 Video Pre-Training via Discrete Diffusion Jaegle, A., Borgeaud, S., Alayrac, J.-B., Doersch, C., Ionescu, C., Ding, D., Koppula, S., Zoran, D., Brock, A., Shelhamer, E., Henaff, O. J., Botvinick, M., Zisser- man, A., Vinyals, O., and Carreira, J. Perceiver IO: A general architecture for structured inputs & outputs. In International Conference on Learning Representations, 2022. James, S. and Davison, A. J. Q-attention: Enabling efficient learning for vision-based robotic manipulation. IEEE Robotics and Automation Letters, 7(2):1612‚Äì1619, 2022. James, S., Ma, Z., Arrojo, D. R., and Davison, A. J. Rlbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):3019‚Äì3026, 2020. James, S., Wada, K., Laidlow, T., and Davison, A. J. Coarse- to-fine q-attention: Efficient learning for visual robotic manipulation via discretisation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13739‚Äì13748, 2022. Janner, M., Li, Q., and Levine, S. Offline reinforcement learning as one big sequence modeling problem. Ad- vances in neural information processing systems, 34: 1273‚Äì1286, 2021. Janner, M., Du, Y., Tenenbaum, J. B., and Levine, S. Plan- ning with diffusion for flexible behavior synthesis. In International Conference on Machine Learning, 2022. Jing, Y., Zhu, X., Liu, X., Sima, Q., Yang, T., Feng, Y., and Kong, T. Exploring visual pre-training for robot manipulation: Datasets, models and methods. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 11390‚Äì11395. IEEE, 2023. Karamcheti, S., Nair, S., Chen, A. S., Kollar, T., Finn, C., Sadigh, D., and Liang, P. Language-driven representation learning for robotics. In Robotics: Science and Systems (RSS), 2023. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Ko, P.-C., Mao, J., Du, Y., Sun, S.-H., and Tenenbaum, J. B. Learning to act from actionless videos through dense correspondences. In International Conference on Learning Representations, 2024. Lee, K.-H., Nachum, O., Yang, M. S., Lee, L., Free- man, D., Guadarrama, S., Fischer, I., Xu, W., Jang, E., Michalewski, H., et al. Multi-game decision transformers. Advances in Neural Information Processing Systems, 35: 27921‚Äì27936, 2022. Liang, Z., Mu, Y., Ma, H., Tomizuka, M., Ding, M., and Luo, P. Skilldiffuser: Interpretable hierarchical planning via skill abstractions in diffusion-based task execution. arXiv preprint arXiv:2312.11598, 2023. Lin, X., So, J., Mahalingam, S., Liu, F., and Abbeel, P. Spawnnet: Learning generalizable visuomotor skills from pre-trained networks, 2023. Liu, S., Tripathi, S., Majumdar, S., and Wang, X. Joint hand motion and interaction hotspots prediction from egocentric videos. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pp. 3282‚Äì3292, 2022. Lu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pre- training task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural informa- tion processing systems, 32, 2019. Ma, Y. J., Kumar, V., Zhang, A., Bastani, O., and Jayaraman, D. LIV: Language-image representations and rewards for robotic control. In International Conference on Machine Learning, volume 202, pp. 23301‚Äì23320, 2023a. Ma, Y. J., Sodhani, S., Jayaraman, D., Bastani, O., Kumar, V., and Zhang, A. VIP: Towards universal visual reward and representation via value-implicit pre-training. In International Conference on Learning Representations, 2023b. Mendonca, R., Bahl, S., and Pathak, D. Structured world models from human videos. 2023. Mu, Y., Zhang, Q., Hu, M., Wang, W., Ding, M., Jin, J., Wang, B., Dai, J., Qiao, Y., and Luo, P. Embod- iedGPT: Vision-language pre-training via embodied chain of thought. In Neural Information Processing Systems, 2023. Nair, S., Rajeswaran, A., Kumar, V., Finn, C., and Gupta, A. R3m: A universal visual representation for robot ma- nipulation. In 6th Annual Conference on Robot Learning, 2022. Ni, F., Hao, J., Mu, Y., Yuan, Y., Zheng, Y., Wang, B., and Liang, Z. Metadiffuser: Diffusion model as conditional planner for offline meta-rl. 2023. OpenAI et al. Gpt-4 technical report, 2023. Pearce, T., Rashid, T., Kanervisto, A., Bignell, D., Sun, M., Georgescu, R., Macua, S. V., Tan, S. Z., Momennejad, I., Hofmann, K., and Devlin, S. Imitating human behaviour with diffusion models. In International Conference on Learning Representations, 2023. 11 Video Pre-Training via Discrete Diffusion Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cap- pelli, A., Alobeidli, H., Pannier, B., Almazrouei, E., and Launay, J. The refinedweb dataset for falcon llm: out- performing curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision, 2021. Radosavovic, I., Xiao, T., James, S., Abbeel, P., Malik, J., and Darrell, T. Real-world robot learning with masked visual pre-training. CoRL, 2022. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J. T., et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M. S., Berg, A. C., and Fei-Fei, L. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115:211 ‚Äì 252, 2014. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Den- ton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., Salimans, T., Ho, J., Fleet, D. J., and Norouzi, M. Photorealistic text-to-image diffusion models with deep language understanding, 2022. Seo, Y., Lee, K., James, S. L., and Abbeel, P. Reinforce- ment learning with action-free pre-training from videos. In International Conference on Machine Learning, pp. 19561‚Äì19579. PMLR, 2022. Shaw, K., Bahl, S., and Pathak, D. Videodex: Learning dex- terity from internet videos. In 6th Annual Conference on Robot Learning, 2022. URL https://openreview. net/forum?id=qUhkhHw8Dz. Shridhar, M., Manuelli, L., and Fox, D. Perceiver-actor: A multi-task transformer for robotic manipulation. In 6th Annual Conference on Robot Learning, 2022. Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequi- librium thermodynamics, 2015. Sun, L., Zhang, H., Xu, W., and Tomizuka, M. Paco: Parameter-compositional multi-task reinforcement learn- ing. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. Sun, Y., Ma, S., Madaan, R., Bonatti, R., Huang, F., and Kapoor, A. SMART: Self-supervised multi-task pretrain- ing with control transformers. In International Confer- ence on Learning Representations, 2023. Taiga, A. A., Agarwal, R., Farebrother, J., Courville, A., and Bellemare, M. G. Investigating multi-task pretraining and generalization in reinforcement learning. In International Conference on Learning Representations, 2023. Thomas, G., Cheng, C.-A., Loynd, R., Frujeri, F. V., Vi- neet, V., Jalobeanu, M., and Kolobov, A. Plex: Making the most of the available data for robotic manipulation pretraining. In Conference on Robot Learning, pp. 2624‚Äì 2641. PMLR, 2023. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation lan- guage models. arXiv preprint arXiv:2302.13971, 2023a. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine- tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. van den Oord, A., Vinyals, O., and kavukcuoglu, k. Neural discrete representation learning. In Advances in Neu- ral Information Processing Systems, volume 30. Curran Associates, Inc., 2017. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., and Polosukhin, I. At- tention is all you need. Advances in neural information processing systems, 30, 2017. Walke, H. R., Black, K., Zhao, T. Z., Vuong, Q., Zheng, C., Hansen-Estruch, P., He, A. W., Myers, V., Kim, M. J., Du, M., Lee, A., Fang, K., Finn, C., and Levine, S. Bridgedata v2: A dataset for robot learning at scale. In Conference on Robot Learning, pp. 1723‚Äì1736, 2023a. Walke, H. R., Black, K., Zhao, T. Z., Vuong, Q., Zheng, C., Hansen-Estruch, P., He, A. W., Myers, V., Kim, M. J., Du, M., et al. Bridgedata v2: A dataset for robot learning at scale. In Conference on Robot Learning, pp. 1723‚Äì1736. PMLR, 2023b. Wang, C., Fan, L., Sun, J., Zhang, R., Fei-Fei, L., Xu, D., Zhu, Y., and Anandkumar, A. Mimicplay: Long- horizon imitation learning by watching human play. arXiv preprint arXiv:2302.12422, 2023a. 12 Video Pre-Training via Discrete Diffusion Wang, Z., Hunt, J. J., and Zhou, M. Diffusion policies as an expressive policy class for offline reinforcement learning. In International Conference on Learning Representations, 2023b. Wu, H., Jing, Y., Cheang, C., Chen, G., Xu, J., Li, X., Liu, M., Li, H., and Kong, T. Unleashing large-scale video generative pre-training for visual robot manipulation. In International Conference on Learning Representations, 2024. Wu, J., Ma, H., Deng, C., and Long, M. Pre-training contex- tualized world models with in-the-wild videos for rein- forcement learning. In Thirty-seventh Conference on Neu- ral Information Processing Systems, 2023. URL https: //openreview.net/forum?id=8GuEVzAUQS. Xian, Z., Gkanatsios, N., Gervet, T., Ke, T.-W., and Fragki- adaki, K. Chaineddiffuser: Unifying trajectory diffusion and keypose prediction for robotic manipulation. In 7th Annual Conference on Robot Learning, 2023. Xiao, T., Radosavovic, I., Darrell, T., and Malik, J. Masked visual pre-training for motor control. arXiv:2203.06173, 2022. Xie, A., Lee, L., Xiao, T., and Finn, C. Decomposing the generalization gap in imitation learning for visual robotic manipulation. arXiv preprint arXiv:2307.03659, 2023. Xiong, H., Li, Q., Chen, Y.-C., Bharadhwaj, H., Sinha, S., and Garg, A. Learning by watching: Physical imita- tion of manipulation skills from human videos. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 7827‚Äì7834. IEEE, 2021. Yan, W., Zhang, Y., Abbeel, P., and Srinivas, A. Videogpt: Video generation using vq-vae and transformers, 2021. Yang, M., Du, Y., Ghasemipour, K., Tompson, J., Schuur- mans, D., and Abbeel, P. Learning interactive real-world simulators. In International Conference on Learning Representations, 2024. Yang, R., Xu, H., Wu, Y., and Wang, X. Multi-task rein- forcement learning with soft modularization. Advances in Neural Information Processing Systems, 33:4767‚Äì4777, 2020. Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S. Meta-world: A benchmark and evalua- tion for multi-task and meta reinforcement learning. In Conference on robot learning, pp. 1094‚Äì1100. PMLR, 2020. Yuan, H. and Lu, Z. Robust task representations for of- fline meta-reinforcement learning via contrastive learning. In International Conference on Machine Learning, pp. 25747‚Äì25759. PMLR, 2022. Zhang, H., Xu, W., and Yu, H. Generative planning for tem- porally coordinated exploration in reinforcement learning. In International Conference on Learning Representations, 2022. 13 Video Pre-Training via Discrete Diffusion A. The Details of VPDD A.1. Discrete Diffusion Discrete diffusion models with a mask-and-replace strategy generate sequences from [MASK] tokens. In this paper, they are trained by sampling a sequence x0 = [x v, xa], masking tokens according to a linear schedule in Gu et al. (2022) that corresponds to increasing the probability of being in the absorbing state linearly over time, and learning to predict the masked tokens given language l and historical videos. Specifically, for the proposed unified transition matrix Qk in Eq. (6), the computation of diffusion process q(xk|x0) in Eq. (4) can also be obtained in the following closed-form (Hu et al., 2023): Qkx0 = Œ±kx0 + [ Œ≥k ‚àí 1 (x0)Œ≤k ‚àí 1 ‚àó(x0)Œ≤‚àó k] x[M] + 1 (x0)Œ≤k + 1 ‚àó(x0)Œ≤‚àó k, where 1 (x0) = { 1 if argmax x0 ‚àà [0, J), 0 otherwise. , 1 ‚àó(x0) = {1 if argmax x0 ‚àà [J, J + J ‚àó), 0 otherwise. , x[M] = x ‚Üê argmax x = J + J ‚àóand Œ±k, Œ≤k, Œ≤‚àók, Œ≥k are the corresponding cumulative product. (10) The reverse process predicts Àúxv 0 and Àúxa 0 by training denoising network pŒ∏1( Àúx v 0|xk, y, l) and pŒ∏2 ( Àúxa 0|xk, z Àúxv 0 , l), respectively. Then the forward process is used to compute pŒ∏1(x v k‚àí1|xk, y, l) as expressed in Eq. (8) and pŒ∏2 (xa k‚àí1|xk, z Àúxv 0 , l) as expressed in Eq. (9). A.2. Perceiver Transformer We use the Perceiver Transformer (Jaegle et al., 2022) to encode extremely long input sequences, which improves the computation efficiency. We maintain a set of latent vectors zQ of dimensions R2048√ó256 which are randomly initialized. Then we compute cross attention between the input sequence and zQ. The process is illustrated in Fig. 7. zQ are processed with 6 self-attention layers, and then decoded into output with the same dimension space of input via cross attention. EncodeCross atten Q K V Process √ó LSelf atten Q K V DecodeCross atten K V Q Latents Input Output Figure 7. Illustration of Perceiver Transformer architecture. Perceiver is a latent-space transformer. Q, K, V represent queries, keys, and values, respectively. We use L = 6 self attention layers in our implementation. A.3. Implementation Details In this section, we give the pseudocodes of pre-training and fine-tuning in Alg. 1 and Alg. 2 respectively. Then we describe the details of the training process, architecture and hyperparameters: ‚Ä¢ Following Hoogeboom et al. (2021), we sample diffusion timestep k ‚àº q(k) with a importance sampling strategy, where q(k) ‚àù ‚àö E[L2 vlb]. ‚Ä¢ We set batch size as 20 for pre-training and 40 for fine-tuning. We train our model using Adam optimizer (Kingma & Ba, 2014) with 2e‚àí4 learning rate for 2e6 training steps. ‚Ä¢ For pre-training, we represent our denoising network pŒ∏1 as a Perceiver Transformer described in ¬ßA.2. MLP ft, which processes the language embeddings given by the CLIP encoder, MLP fy, which processes the historical video tokens, and 14 Video Pre-Training via Discrete Diffusion MLP fxk are 2-layered MLPs (prepended by a layer norm (Ba et al., 2016) and with Mish activation). MLP fT i, which processes diffusion timestep k, is a 2-layered MLP (prepended by a Sinusoidal embedding and with Mish activation). Finally, we use a linear layer to project the encodings given by the Perceiver Transformer into the original dimension space of the input. ‚Ä¢ For fine-tuning, we represent our denoising network pŒ∏2 as a GPT2 Transformer. Similar to the architecture of pŒ∏1, we first use MLPs with Mish activation to project different inputs into the same hidden dimension space, and then recover the encodings given by the GPT2 Transformer into the original dimension space via a linear layer. ‚Ä¢ We choose the sequence length H = 4 and M = 1 for future actions and videos prediction. ‚Ä¢ We set h = 20 which means that we predict future videos after 20 steps. ‚Ä¢ We set diffusion timesteps K = 100. B. The Details of Baselines We describe the implementation details of baselines used in our experiments as follows: ‚Ä¢ R3M-Diffusion. We borrow the pre-trained R3M encoder from https://github.com/facebookresearch/ r3m, and leverage the encoder to encode sing-view RGB images in Meta-World and multi-view RGB images in RLBench. Thus, we skip the pre-training process and directly train our discrete diffusion model on the robot data. The model architecture and hyper-parameters of R3M-Diffusion are almost the same as pŒ∏2. The encoded hidden representation encoded by R3M is denoted as zR3M , then the denoising network can be written as pŒ∏(xa k‚àí1|xk, zR3M , l). ‚Ä¢ MTDIFF-P. We borrow the official codes of MTDIFF-P from https://github.com/tinnerhrhe/MTDiff. In order to process high-dimensional images instead of low-dimensional states, we leverage the R3M encoder in R3M- Diffusion to obtain the visual representations. ‚Ä¢ VIDEO-MTDT. We use the language l to indicate tasks, which are encoded as a vector with size 2048 by the same CLIP encoder used in VPDD. We take the video tokens used in VPDD as the states. Then we follow the implementation from https://github.com/kzl/decision-transformer/ to train Video-MTDT on the limited robot data. ‚Ä¢ VPDD-w/o-human. During the pre-training stage of VPDD, we remove the human videos and only use the robot videos for training. ‚Ä¢ SODA. Following the SODA paper (Hudson et al., 2023) and open-sourced codes from https://github.com/ FutureXiang/soda, we first maintain an encoder ESODA equipped with the same Perceiver Transformer in pŒ∏1 . Then during the pre-training stage, a representation z is first encoded by ESODA conditioning on et and l, i.e., z = ESODA(et, l). Then xk‚àí1 is obtained via the following process: xk‚àí1 = Attn (xk, z) xk‚àí1 = Attn (xk‚àí1, xk‚àí1) , (11) where Attn is an attention operation (Vaswani et al., 2017; Lu et al., 2019) where the queries are formed from x, the keys and values from y. The encoder is trained end-to-end and to minimize the same loss term Lvlb. During the fine-tuning stage, z Àúxv 0 is replaced of z output by ESODA. The model architecture and other hyper-parameters during fine-tuning remain the same. 15 Video Pre-Training via Discrete Diffusion Algorithm 1 Pre-Training Stage of VPDD Initialize: given unified transition matrix {Qk}, well-trained VQVAE, training iterations N , initial network parameters Œ∏1 and Œ∏2, learning rate Œ∑. 1: for video clips vt in {œÑi} do 2: et ‚Üê VQVAE-Encoder(vt) 3: end for 4: for n = 1 to N do 5: Sample a batch B = (xv, xa, y, l) from human and robot videos, where x a ‚Üê œï 6: Sample diffusion timestep k from [1, K] with importance sampling strategy 7: xk ‚Üê q(xk|x0), where x0 = [xv, xa] ‚ñ∑ Eq. (10) and (4) 8: Lvlb = { L0 if k = 1, Lk‚àí1 otherwise. ‚ñ∑ Eq. (6) 9: Œ∏1 ‚Üê Œ∏1 ‚àí Œ∑‚àáŒ∏1L ‚ñ∑ Adam optimizer 10: end for Algorithm 2 Fine-Tuning Stage of VPDD and Evaluation # Fine-Tuning Process Initialize: given unified transition matrix {Qk}, well-trained VQVAE, training iterations N , pre-trained network parameters Œ∏1 and initial parameters Œ∏2, learning rate Œ∑. 1: for video clips vt and actions at in {œÑi} do 2: et ‚Üê VQVAE-Encoder(vt), at ‚Üê Discretize(at) 3: end for 4: for n = 1 to N do 5: Sample a batch B = (xv, xa, y, l) from videos and discretized actions dataset. 6: Sample diffusion timestep k from [1, K] with a importance sampling strategy 7: xk ‚Üê q(xk|x0), where x0 = [xv, xa] ‚ñ∑ Eq. (10) and (4) 8: Lvlb = { L0 if k = 1, Lk‚àí1 otherwise. ‚ñ∑ Eq. (6) 9: Œ∏2 ‚Üê Œ∏2 ‚àí Œ∑‚àáŒ∏2L ‚ñ∑ Adam optimizer 10: end for # Evaluation Process 1: Given a task, reset the environment 2: Obtain the initial video clips v0, language instructions l 3: for t = 0 to tmax do 4: Initialize [xv K, xa K] ‚àº p(xK) ‚ñ∑ Eq. (7) 5: Construct y ‚Üê et = VQVAE-Encoder(vt) 6: for k = K to 1 do 7: Sample xv k‚àí1 ‚àº pŒ∏1 (xv k‚àí1|xk, y, l) and obtain z Àúxv 0 from pŒ∏1 encoding ‚ñ∑ Eq. (8) 8: Sample xa k‚àí1 ‚àº pŒ∏2 (xa k‚àí1|xk, z Àúxv 0 , l) ‚ñ∑ Eq. (9) 9: xk‚àí1 = [xv k‚àí1, xa k‚àí1] 10: end for 11: Obtain predicted videos via VQVAE-Decoder(xv 0) 12: Reconstruct executable action sequence [at, ¬∑ ¬∑ ¬∑ , at+H‚àí1] from x a 0 13: Execute the first action at as the current action to interact with the environment 14: Obtain the next observed image(s), and update vt 15: end for 16 Video Pre-Training via Discrete Diffusion C. Datasets and Experiments C.1. Dataset Meta-World. We use the official codes from https://github.com/Farama-Foundation/Metaworld to collect 20 expert demonstrations for each task in Meta-World. The image size is 260 √ó 260. The dimension of action is 4, representing the 3D position movements of the end effector and the variation of gripper openness. Every demonstration collected has 150 time steps. RLBench. We use the official codes from https://github.com/peract/peract to generate a dataset for RLBench via a motion planner. Each demonstration consists of multi-view RGB images (i.e., front, left shoulder, right shoulder and wrist) and 8-dimensional actions including 3-dimensional transitions, 4-dimensional quaternion and a binary value about gripper openness. Following prior works (James & Davison, 2022; Shridhar et al., 2022), we extract macro actions (keyframe actions) from collected demonstrations and leverage networks to predict the keyframe actions instead of consistent continuous actions. Specifically, a set of keyframe actions {k1, k2, ¬∑ ¬∑ ¬∑ , km} ‚äÇ A is captured with a simple heuristic: an action is a keyframe if (1) the joint velocities are near zero and (2) the gripper open state has not changed. Each data point in the demonstration œÑ can then be cast as a ‚Äúpredict the next (best) keyframe action‚Äù task (James et al., 2022). In this way, the sequence length of actions that need to be predicted is significantly reduced from hundreds of small steps to typically less than 10 macro steps. C.2. Task Details Task Variation Type # of Variations Language Template slide block color 4 ‚Äúslide the block to target‚Äù sweep to dustpan size 2 ‚Äúsweep dirt to the dustpan‚Äù meat off grill category 2 ‚Äútake the off the grill‚Äù turn tap placement 2 ‚Äúturn tap‚Äù put in drawer placement 3 ‚Äúput the item in the drawer‚Äù close jar color 20 ‚Äúclose the jar‚Äù drag stick color 20 ‚Äúuse the stick to drag the cube onto the target‚Äù stack blocks color, count 60 ‚Äústack blocks‚Äù screw bulb color 20 ‚Äúscrew in the light bulb‚Äù put in safe placement 3 ‚Äúput the money away in the safe on the shelf‚Äù place wine placement 3 ‚Äústack the wine bottle to the of the rack‚Äù put in cupboard category 9 ‚Äúput the in the cupboard‚Äù push buttons color 50 ‚Äúpush the button, [then the button]‚Äù insert peg color 20 ‚Äúput the ring on the spoke‚Äù stack cups color 20 ‚Äústack the other cups on top of the cup‚Äù place cups count 3 ‚Äúplace cups on the cup holder‚Äù Table 2. Language-Conditioned Tasks in RLBench (James et al., 2020) with various variations. We take Meta-World as a main benchmark to evaluate our method and baselines, which consists of 50 diverse manipulation tasks. The poses and positions of goals are randomly generated during evaluation. These tasks require an agent to identify the observed sing-view RGB images and reach the goals with the correct behavior. See Fig. 8 for a sample visualization of the tasks. We select 16 tasks out of 100 tasks from RLBench (James et al., 2020) that involve at least two or more variations to evaluate the multi-task and generalization capabilities of agents. Task variations include randomly sampled colors, sizes, counts, placements, and categories of objects. The set of colors include 20 instances: colors = {red, maroon, lime, green, blue, navy, yellow, cyan, magenta, silver, gray, orange, olive, purple, teal, azure, violet, rose, black, white}. The set of sizes include 2 instances: sizes = {short, tall}. The set of counts include 3 instances: counts = {1, 2, 3}. The placements and object categories are specific to each task. For instance, put in cupboard includes 9 YCB objects. In addition to these semantic variations, objects are placed on the tabletop at random poses. The 17 Video Pre-Training via Discrete Diffusion tasks in RLBench require an agent to process multi-view RGB images properly and generalize to different variations that could be unseen in training. The details of variations for each task are referred to Table 2. (a) window-open (b) dial-turn (c) close jar (d) meat-off-grill (e) faucet-open (f) sweep-into (g) stack blocks (h) place cups Figure 8. Visualization of several tasks in Meta-World and RLBench. C.3. Experimental Setup To evaluate the generalization ability of our method, we change the camera position and table texture in the Meta-World benchmark to generate out-of-distribution observations. Following Xie et al. (2023) and using the official codes from https://github.com/RLAgent/factor-world, we set the camera at another corner position and generate unseen table texture randomly while rendering. See Fig. 9 for visualization. 18 Video Pre-Training via Discrete Diffusion (a) button-press (b) shifted camera position (c) shifted table texture (d) handle-press (e) shifted camera position (f) shifted table texture Figure 9. A visualized example of shifted camera position and table texture on button-press-v2 and handle-press-v2 tasks. Table texture is generated randomly during evaluation. D. Limitations and Future Work We present illustrative samples of robot videos generated using discrete diffusion model pŒ∏1 in Fig. 3 and Fig. 10. More samples are available at https://video-diff.github.io. It is noteworthy that VPDD is able to generate dynamic consistent future videos, incorporating information beneficial for low-level control while maintaining coherence across distinct perspectives. However, it is imperative to acknowledge that our primary contribution is not on generating high-quality videos with exceptional resolution and meticulous semantic details. Consequently, some blurriness may be observed in our predicted videos, exemplified by deviations in the gripper‚Äôs pose. See Fig. 11 for a failure example. For future work, we could consider enhancing the coherence of videos across diverse views by leveraging the recently released Ego-exo4d data (Grauman et al., 2023). This extension encompasses considerations beyond solely temporal dynamics. Moreover, for the augmentation of video quality and the optimization of decision-making performance, it is worth exploring to scale up both the training dataset and model capacity. 19 Video Pre-Training via Discrete Diffusion (a) front (b) front (c) front (d) front (e) left shoulder (f) left shoulder (g) left shoulder (h) left shoulder (i) right shoulder (j) right shoulder (k) right shoulder (l) right shoulder (m) wrist (n) wrist (o) wrist (p) wrist Figure 10. Predicted video given by pŒ∏1 for task put the crackers in the cupboard. 20 Video Pre-Training via Discrete Diffusion (a) front (b) front (c) front (d) front (e) left shoulder (f) left shoulder (g) left shoulder (h) left shoulder (i) right shoulder (j) right shoulder (k) right shoulder (l) right shoulder (m) wrist (n) wrist (o) wrist (p) wrist Figure 11. Predicted video given by pŒ∏1 for task put the mustard in the cupboard. The pose of the robotic arm in the video is somewhat blurry, with deficiencies in correspondence across different views. 21","libVersion":"0.3.2","langs":""}