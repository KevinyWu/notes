{"path":"deep-reinforcement-learning/slides/4 Introduction to RL.pdf","text":"Introduction to Reinforcement Learning CS 285 Instructor: Sergey Levine UC Berkeley Definitions 1. run away 2. ignore 3. pet Terminology & notation Images: Bojarski et al. ‘16, NVIDIA training data supervised learning Imitation LearningReward functionsDefinitions Andrey Markov Definitions Andrey MarkovRichard Bellman Definitions Richard Bellman DefinitionsThe goal of reinforcement learning we’ll come back to partially observed later The goal of reinforcement learningThe goal of reinforcement learningFinite horizon case: state-action marginal state-action marginal Infinite horizon case: stationary distribution stationary distribution stationary = the same before and after transition Infinite horizon case: stationary distribution stationary distribution stationary = the same before and after transition Expectations and stochastic systems infinite horizon case finite horizon case In RL, we almost always care about expectations +1 -1 Algorithms The anatomy of a reinforcement learning algorithm generate samples (i.e. run the policy) fit a model/ estimate the return improve the policy A simple example generate samples (i.e. run the policy) fit a model/ estimate the return improve the policy Another example: RL by backprop generate samples (i.e. run the policy) fit a model/ estimate the return improve the policy Which parts are expensive? generate samples (i.e. run the policy) fit a model/ estimate the return improve the policy real robot/car/power grid/whatever: 1x real time, until we invent time travel MuJoCo simulator: up to 10000x real time trivial, fast expensive Value Functions How do we deal with all these expectations? what if we knew this part? Definition: Q-function Definition: value function Using Q-functions and value functionsThe anatomy of a reinforcement learning algorithm generate samples (i.e. run the policy) fit a model/ estimate the return improve the policy this often uses Q- functions or value functions Types of Algorithms Types of RL algorithms • Policy gradients: directly differentiate the above objective • Value-based: estimate value function or Q-function of the optimal policy (no explicit policy) • Actor-critic: estimate value function or Q-function of the current policy, use it to improve policy • Model-based RL: estimate the transition model, and then… • Use it for planning (no explicit policy) • Use it to improve a policy • Something else Model-based RL algorithms generate samples (i.e. run the policy) fit a model/ estimate the return improve the policy Model-based RL algorithms improve the policy 1. Just use the model to plan (no policy) • Trajectory optimization/optimal control (primarily in continuous spaces) – essentially backpropagation to optimize over actions • Discrete planning in discrete action spaces – e.g., Monte Carlo tree search 2. Backpropagate gradients into the policy • Requires some tricks to make it work 3. Use the model to learn a value function • Dynamic programming • Generate simulated experience for model-free learner Value function based algorithms generate samples (i.e. run the policy) fit a model/ estimate the return improve the policy Direct policy gradients generate samples (i.e. run the policy) fit a model/ estimate the return improve the policy Actor-critic: value functions + policy gradients generate samples (i.e. run the policy) fit a model/ estimate the return improve the policy Tradeoffs Between Algorithms Why so many RL algorithms? • Different tradeoffs • Sample efficiency • Stability & ease of use • Different assumptions • Stochastic or deterministic? • Continuous or discrete? • Episodic or infinite horizon? • Different things are easy or hard in different settings • Easier to represent the policy? • Easier to represent the model? generate samples (i.e. run the policy) fit a model/ estimate return improve the policy Comparison: sample efficiency • Sample efficiency = how many samples do we need to get a good policy? • Most important question: is the algorithm off policy? • Off policy: able to improve the policy without generating new samples from that policy • On policy: each time the policy is changed, even a little bit, we need to generate new samples generate samples (i.e. run the policy) fit a model/ estimate return improve the policy just one gradient step Comparison: sample efficiency More efficient (fewer samples) Less efficient (more samples) on-policyoff-policy Why would we use a less efficient algorithm? Wall clock time is not the same as efficiency! evolutionary or gradient-free algorithms on-policy policy gradient algorithms actor-critic style methods off-policy Q-function learning model-based deep RL model-based shallow RL Comparison: stability and ease of use Why is any of this even a question??? • Does it converge? • And if it converges, to what? • And does it converge every time? • Supervised learning: almost always gradient descent • Reinforcement learning: often not gradient descent • Q-learning: fixed point iteration • Model-based RL: model is not optimized for expected reward • Policy gradient: is gradient descent, but also often the least efficient! Comparison: stability and ease of use • Value function fitting • At best, minimizes error of fit (“Bellman error”) • Not the same as expected reward • At worst, doesn’t optimize anything • Many popular deep RL value fitting algorithms are not guaranteed to converge to anything in the nonlinear case • Model-based RL • Model minimizes error of fit • This will converge • No guarantee that better model = better policy • Policy gradient • The only one that actually performs gradient descent (ascent) on the true objective Comparison: assumptions • Common assumption #1: full observability • Generally assumed by value function fitting methods • Can be mitigated by adding recurrence • Common assumption #2: episodic learning • Often assumed by pure policy gradient methods • Assumed by some model-based RL methods • Common assumption #3: continuity or smoothness • Assumed by some continuous value function learning methods • Often assumed by some model-based RL methods Examples of Algorithms Examples of specific algorithms • Value function fitting methods • Q-learning, DQN • Temporal difference learning • Fitted value iteration • Policy gradient methods • REINFORCE • Natural policy gradient • Trust region policy optimization • Actor-critic algorithms • Asynchronous advantage actor-critic (A3C) • Soft actor-critic (SAC) • Model-based RL algorithms • Dyna • Guided policy search We’ll learn about most of these in the next few weeks! Example 1: Atari games with Q-functions • Playing Atari with deep reinforcement learning, Mnih et al. ‘13 • Q-learning with convolutional neural networks Example 2: robots and model-based RL • End-to-end training of deep visuomotor policies, L.* , Finn* ’16 • Guided policy search (model-based RL) for image-based robotic manipulation Example 3: walking with policy gradients • High-dimensional continuous control with generalized advantage estimation, Schulman et al. ‘16 • Trust region policy optimization with value function approximation Example 4: robotic grasping with Q-functions • QT-Opt, Kalashnikov et al. ‘18 • Q-learning from images for real-world robotic grasping","libVersion":"0.3.2","langs":""}