{"path":"papers/video2reward/figures/rank2reward.png","text":"- ﬂ-ﬁ o Classify between expert and non-expert states Dy (s) Rank states using temporal order log(rr () (7= 9 5(S) t=0 —mM ] W W Wl T“;:;z‘.,:s:::f“\"——»fﬁﬁ e - Expert Video [ 5 e E » - E I g.‘ — * > T — ’ FREF R S E = - / Ls > = 5(5) TTTrTTrrTTTTTITTrTTITTTITTTTTTTT 0 | X ‘] g 1 2 - — - (o1, B TOVAV I = i a _~ = s =w Fig. 2: A schematic depiction of reward inference using Rank2Reward. Given video demonstrations from a human supervisor, Rank2Rewardlearns a reward function by combining two distinct elements — (1) a ranking function that temporally orders frames, providing a monotonically increasing reward signal prF(s). Secondly, (2) a classifier D between expert and on-policy data, so that expert data is weighted higher than on-policy data. When combined multiplicateively, they yield a well-shaped reward function for RL that pushes down on-policy data, and pushes up expert data.","libVersion":"0.3.2","langs":"eng"}