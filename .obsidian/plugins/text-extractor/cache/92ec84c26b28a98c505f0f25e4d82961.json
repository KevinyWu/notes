{"path":"papers/video2reward/figures/diffusion_reward.png","text":"| A Noise A : S o ~ W ~ , il Condition I I I I I ) —£4 J/ S Historical Frames z, Video Diffusion Model Denoised Samples Interaction ©0©\\ Diffusion Reward a i —H(Po (- |22)) Environment RL Agent ConditionaliEniopy Metaworld (7 tasks Adroit (3 tasks 100 80 __ 80 § 60 5 = g A / 73 — 40 g 40 / @ /. 20 / 20 4 N Z 0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Training Progress Training Progress == = Raw Sparse Reward = === RND e=== AMP <= VIPER e Diffusion Reward (ours) Figure 1. Overview. (fop) We present a reward learning frame- work in RL using video diffusion models. We perform diffusion processes conditioned on historical frames to estimate conditional entropy as rewards to encourage RL exploration of expert-like be- haviors. (bottom) The mean success rate of 10 visual robotic ma- nipulation tasks demonstrates the effectiveness of our Diffusion Reward over 5 runs. Shaded areas are standard errors.","libVersion":"0.3.2","langs":"eng"}