{"path":"appendix/figures/vit.png","text":"Vision Transformer (ViT) [ Transformer Encoder 1 Class red Bl ! ® Ball Head 1 & . 1 Transormer Eacoder ! ! ® \" 1 Patch + Positi i-] e @ @) B)6) 6)E) @) GUE) @) | | [P * Extra learnable - —— 1 [class] embedding Linear Projection of Flattened Patches 1 SEE ‘ i | T - wER- ——SNENENEEE P 1 Embedded [ Patches Figure 1: Model overview. We split an image into fixed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classification, we use the standard approach of adding an extra learnable “classification token” to the sequence. The illustration of the Transformer encoder was inspired by @017).","libVersion":"0.3.2","langs":"eng"}