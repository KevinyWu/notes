{"path":"papers/appendix/figures/mae.png","text":"[ | [ | . [ | [ | /] ] EnsEE 5 S EpEET EHEEE S FENEN > encoder > decoder u > FENRE nnnes M - HEEES EEmEEE H n NHENES input . ] target N : N [ | [ | Figure 1. Our MAE architecture. During pre-training, a large random subset of image patches (e.g., 75%) is masked out. The encoder is applied to the small subset of visible patches. Mask tokens are introduced after the encoder, and the full set of en- coded patches and mask tokens is processed by a small decoder that reconstructs the original image in pixels. After pre-training, the decoder is discarded and the encoder is applied to uncorrupted images (full sets of patches) for recognition tasks.","libVersion":"0.3.2","langs":"eng"}