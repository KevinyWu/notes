{"path":"papers/deep-reinforcement-learning/slides/3 Pytorch Tutorial.pdf","text":"PyTorch and Neural Nets CS285 Deep RL Instructor: Kyle Stachowicz [Adapted from Marwa Abdulhai’s CS285 Fa22 Slides] PyTorch Tutorial (Colab) http://bit.ly/cs285-pytorch-2023 https://colab.research.google.com/drive/12nQiv6aZHXNuCfAAuTjJenDWKQbIt2Mz Goal of this course Train an agent to perform useful tasks train the model data agent collect data Goal of this course Train an agent to perform useful tasks train the model data agent collect data train the model focus for today’s lecture! How do train a model? dataset neural network loss gradient descent PyTorch does all of these! What is PyTorch? Python library for: • Defining neural networks • Automating computing gradients • And more! (datasets, optimizers, GPUs, etc.) How does PyTorch work? [picture from Stanford’s CS231n] You define: PyTorch computes: • Fast CPU implementations • CPU-only • No autodiff • Imperative • Fast CPU implementations • Allows GPU • Supports autodiff • Imperative Other features include: • Datasets and dataloading • Common neural network operations • Built-in optimizers (Adam, SGD, …) The Basics 100x faster! Multidimensional ArraysMultidimensional Indexing 32 27 5 54 1 99 4 23 3 57 76 42 34 82 5 A A.shape == (3, 5)Axis 0 Axis 1 Multidimensional Indexing 32 27 5 54 1 99 4 23 3 57 76 42 34 82 5 A[0, 3]Axis 0 Axis 1 Multidimensional Indexing 32 27 5 54 1 99 4 23 3 57 76 42 34 82 5 A[:, 3]Axis 0 Axis 1 Multidimensional Indexing 32 27 5 54 1 99 4 23 3 57 76 42 34 82 5 A[0, :]Axis 0 Axis 1 Multidimensional Indexing 32 27 5 54 1 99 4 23 3 57 76 42 34 82 5 A[0, 2:4]Axis 0 Axis 1 32 27 5 54 1 99 4 23 3 57 76 42 34 82 5 32 27 5 54 1 99 4 23 3 57 76 42 34 82 5 32 27 5 54 1 99 4 23 3 57 76 42 34 82 5 Multidimensional Indexing 32 27 5 54 1 99 4 23 3 57 76 42 34 82 5 A A.shape == (3, 5, 4)Axis 0 Axis 1 Axis 2 32 27 5 54 1 99 4 23 3 57 76 42 34 82 5 32 27 5 54 1 99 4 23 3 57 76 42 34 82 5 32 27 5 54 1 99 4 23 3 57 76 42 34 82 5 Multidimensional Indexing 32 27 5 54 1 99 4 23 3 57 76 42 34 82 5 A A[0, ...]Axis 0 Axis 1 Axis 2 32 27 5 54 1 99 4 23 3 57 76 42 34 82 5 32 27 5 54 1 99 4 23 3 57 76 42 34 82 5 32 27 5 54 1 99 4 23 3 57 76 42 34 82 5 Multidimensional Indexing 32 27 5 54 1 99 4 23 3 57 76 42 34 82 5 A A[..., 1]Axis 0 Axis 1 Axis 2 Broadcasting https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html TL;DR: Shape (1, 3, 2) acts like (6, 5, 4, 3, 2) when added to shape (6, 5, 4, 3, 2) (Trailing dimensions will be matched, arrays will be repeated along matching dimensions) Shape OperationsDevice Management • Numpy: all arrays live on the CPU’s RAM • Torch: tensors can either live on CPU or GPU memory • Move to GPU with .to(“cuda”)/.cuda() • Move to CPU with .to(“cpu”)/.cpu() YOU CANNOT PERFORM OPERATIONS BETWEEN TENSORS ON DIFFERENT DEVICES! loss loss Computing Gradients P b x y loss Computing Gradients P b x y loss .detach() Training Loop REMEMBER THIS! Converting Numpy / PyTorch Numpy -> PyTorch: torch.from_numpy(numpy_array).float() PyTorch -> Numpy: • (If requires_grad) Get a copy without graph with .detach() • (If on GPU) Move to CPU with .to(“cpu”)/.cpu() • Convert to numpy with .numpy All together: torch_tensor.detach().cpu().numpy() Custom networks • Prefer net() over net.forward() • Everything (network and its inputs) on the same device!!! Torch Best Practices •When in doubt, assert is your friend assert x.shape == (B, N), \\ f”Expected shape ({B, N}) but got {x.shape}” •Be extra careful with .reshape/.view • If you use it, assert before and after • Only use it to collapse/expand a single dim • In Torch, prefer .flatten()/.permute()/.unflatten() •If you do some complicated operation, test it! • Compare to a pure Python implementation Torch Best Practices (continued) •Don’t mix numpy and Torch code • Understand the boundaries between the two • Make sure to cast 64-bit numpy arrays to 32 bits • torch.Tensor only in nn.Module! •Training loop will always look the same • Load batch, compute loss • .zero_grad(), .backward(), .step() PyTorch Tutorial (Colab) http://bit.ly/cs285-pytorch-2023 https://colab.research.google.com/drive/12nQiv6aZHXNuCfAAuTjJenDWKQbIt2Mz","libVersion":"0.3.2","langs":""}