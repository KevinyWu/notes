{"path":"papers/empowerment/papers/2024 Learning to Assist Humans without Inferring Rewards.pdf","text":"Learning to Assist Humans without Inferring Rewards Vivek Myers 1 Evan Ellis 1 Benjamin Eysenbach 2 Sergey Levine 1 Anca Dragan 1 Abstract Assistive agents should make humans’ lives eas- ier. Classically, such assistance is studied through the lens of inverse reinforcement learning, where an assistive agent (e.g., a chatbot, a robot) infers a human’s intention and then selects actions to help the human reach that goal. This approach requires inferring intentions, which can be difficult in high- dimensional settings. We build upon prior work that studies assistance through the lens of em- powerment: an assistive agent aims to maximize the influence of the human’s actions such that they exert a greater control over the environmen- tal outcomes and can solve tasks in fewer steps. We lift the major limitation of prior work in this area—scalability to high-dimensional settings— with contrastive successor representations. We formally prove that these representations estimate a similar notion of empowerment to that studied by prior work and provide a ready-made mecha- nism for optimizing it. Empirically, our proposed method outperforms prior methods on synthetic benchmarks, and scales to Overcooked, a cooper- ative game setting. Theoretically, our work con- nects ideas from information theory, neuroscience, and reinforcement learning, and charts a path for representations to play a critical role in solving assistive problems. 1 Introduction AI agents deployed in the real world should be helpful to humans. When we know the utility function of the humans an agent could interact with, we can directly train assistive agents through reinforcement learning with the known hu- man objective as the agent’s reward. In practice, agents rarely have direct access to a scalar reward corresponding to human preferences (if such a consistent model even exists) 1University of California, Berkeley 2Princeton University. Cor- respondence to: Vivek Myers <vmyers@berkeley.edu>. Our code is available at https://anonymous.4open. science/r/esr-7E94. Preliminary work. Under review by the ICML 2024 Workshop on Models of Human Feedback for AI Alignment. Do not distribute. (Casper et al., 2023), and must infer them from human be- havior (Hadfield-Menell et al., 2016; 2017). This inference can be challenging, as humans may act suboptimally with respect to their stated goals, not know their goals, or have changing preferences (Carroll et al., 2021). Optimizing a misspecified reward function can have poor consequences (Turner et al., 2023). An alternative paradigm for assistance is to train agents that are intrinsically motivated to assist humans, rather than di- rectly optimizing a model of their preferences. An analogy can be drawn to a parent raising a child. A good parent will empower the child to make impactful decisions and flourish, rather than proscribing an “optimal” outcome for the child. Likewise, AI agents might seek to empower the human agents they interact with, maximizing their capacity to change the environment (Du et al., 2020). In practice, con- crete notions of empowerment can be difficult to optimize as an objective, requiring extensive modeling assumptions that don’t scale well to the high-dimensional settings deep reinforcement learning agents are deployed in. What is a good intrinsic objective for assisting humans that doesn’t require these assumptions? We propose a notion of assistance based on maximizing the influence of the human’s actions on the environment. This approach only requires one structural assumption: the AI agent is interacting with an environment where there is a notion of actions taken by the human agent—a more general setting than the case where we model the human actions as the outcome of some optimization procedure, as in IRL (Russell, 1998; Arora & Doshi, 2021) or PbRL (Wirth et al., 2017). Prior work has studied many effective objectives for empow- erment. For instance, Du et al. (2020) approximates human empowerment as the variance in the final states of random rollouts. Despite excellent results in certain settings, this approach can be challenging to scale to higher dimensional settings, and does not necessarily enable human users to achieve the goals the want to achieve. By contrast, our ap- proach exclusively empowers the human with respect to the distribution of (useful) behaviors induced by their current policy, and can be implemented through a simple objec- tive derived from contrastive successor features, which can then be optimized with scalable deep reinforcement learning (Fig. 1). We provide a theoretical framework connecting our 1 Without Empowerment Empowerment Caption: We propose an algorithm training assistive agents to empower human users – the assistant should take actions that enable human users to visit a wide range of future states, and the human's actions should exert a high degree of influence over the future outcomes. Our algorithm scales to high-dimensional settings, opening the door to building assistive agents that need not directly reason about human intentions. Without Empowerment Empowerment Figure 1: We propose an algorithm training assistive agents to empower human users—the assistant should take actions that enable human users to visit a wide range of future states, and the human’s actions should exert a high degree of influence over the future outcomes. Our algorithm scales to high-dimensional settings, opening the door to building assistive agents that need not directly reason about human intentions. objective to prior work on empowerment and goal inference, and empirically show that agents trained with this objective can assist humans in the Overcooked environment (Carroll et al., 2020) as well as the obstacle gridworld assistance benchmark proposed by Du et al. (2020). Our core contribution is a novel objective for training agents that are intrinsically motivated to assist humans without requiring a model of the human’s reward function. Our ob- jective maximizes the influence of the human’s actions on the environment, and, unlike past approaches for assistance without reward inference, is based on a scalable model-free objective that can be derived from learned successor fea- tures that encode which states the human is likely to want to reach given their current action. Our objective empowers the human to reach the desired states, not all states, without as- suming a human model. We analyze this objective in terms of empowerment and goal inference, drawing novel math- ematical connections between time-series representations, decision-making, and assistance. We empirically show that agents trained with our objective can assist humans in two benchmarks proposed by past work: the Overcooked envi- ronment (Carroll et al., 2020) and an obstacle-avoidance gridworld (Du et al., 2020). 2 Related Work Our approach broadly connects ideas from contrastive con- trastive representation learning and intrinsic motivation to the problem of assisting humans. Assistive Agents. There are two lines of past work on assistive agents that are most relevant. The first line of work focuses on the setting of an assistance game (Hadfield-Menell et al., 2016), where a robot (AI) agent tries to optimize a human reward of which it is ini- tially unaware. Practically, inverse reinforcement learning (IRL) can be used in such a setting to infer the human’s reward function and assist the human in achieving their goals (Hadfield-Menell et al., 2017). The key challenge with this approach is that it requires modeling the human’s reward function. This can be difficult in practice, especially if the human’s behavior is not well-modeled by the reward architecture. Slightly mispecified reward functions can lead to catastrophic outcomes (i.e., directly harmful behavior in the assistance context) (Pan et al., 2022; Tien et al., 2023; Laidlaw et al., 2024). By contrast, our approach does not require modeling the human’s reward function. The second line of work focuses on empowerment-like ob- jectives for assistance and shared autonomy. Empowerment generally refers to a measure of an agent’s ability to influ- ence the environment (Salge et al., 2013; de Abril & Kanai, 2018). In the context of assistance, Du et al. (2020) show one such approximation of empowerment (AvE) can be ap- proximated in simple environments through random rollouts to assist humans. Meanwhile, empowerment-like objectives have been used in shared autonomy settings to assist hu- mans with teleoperation (Chen et al., 2022) and general assistive interfaces (Reddy et al., 2022). A key limitation of these approaches for general assistance is they only model empowerment over one time step. Our approach enables a more scalable notion of empowerment that can be computed over multiple time steps. Intrinsic Motivation. Intrinsic motivation broadly refers to agents that accomplish behaviors in the absence of an 2 externally-specified reward or task (Barto, 2013). Com- mon applications of intrinsic motivation in single-agent re- inforcement learning include exploration and skill discovery (Aubret et al., 2019; Eysenbach et al., 2018; Burda et al., 2018), empowerment (de Abril & Kanai, 2018; Salge et al., 2013), and surprise minimization (Friston, 2010; Berseth et al., 2021; de Abril & Kanai, 2018). When applied to set- tings with humans, these objectives may lead to antisocial behavior (Turner et al., 2023). Our approach applies intrin- sic motivation to the setting of assisting humans, where the agent’s goal is an empowerment objective—to maximize the human’s ability to change the environment. Information-theoretic Decision Making. Information- theoretic approaches have seen broad applicability across unsupervised reinforcement learning (Poole et al., 2019; de Abril & Kanai, 2018; Aubret et al., 2019). These methods have been applied to goal-reaching (Choi et al., 2021), skill discovery (Mohamed & Rezende, 2015; Jung et al., 2011; Eysenbach et al., 2018; Laskin et al., 2022; Park et al., 2021), and exploration (Burda et al., 2018; Still & Precup, 2012; Nikolov et al., 2019). In the context of assisting humans, information-theoretic methods have primarily been used to reason about the human’s goals or rewards (Biyik et al., 2021; Myers et al., 2022; Houlsby et al., 2011). Our approach is made possible by advances in contrastive representation learning for efficient estimation of the mutual information of sequence data (van den Oord et al., 2019). While these methods have been widely used for represen- tation learning (Chen et al., 2020; Wu et al., 2018) and re- inforcement learning (Laskin et al., 2020; Eysenbach et al., 2022; Dayan, 1993; Momennejad et al., 2017), to the best of our knowledge prior work has not used these contrastive techniques for learning assistive agents. 3 The Information Geometry of Empowerment We will first state a general notion of an assistive setting, then show how an empowerment objective based on learned successor representations can be used to assist humans with- out making assumptions about the human following an un- derlying reward function. In Section 5, we provide empirical evidence supporting these claims. 3.1 Preliminaries Formally, we adapt the notation of Hadfield-Menell et al. (2016), and assume a “robot” (R) and “human” (H) policy are training together in an MDP M = (S, AH, AR, R, P, γ). The observations s consistent of the joint states of the robot and the human; we do not have separate observations for the human and robot. At any state s ∈ S, the robot pol- icy selects actions distributed according to πR(aR | s) for a R ∈ AR and the human selects actions from πH (a H | s) for a H ∈ AH. The transition dynamics are defined by a dis- tribution P (s ′ | s, a H, a R) over the next state s ′ ∈ S given the current state s ∈ S and actions a H ∈ AH and a R ∈ AR, as well as an initial state distribution P (s0). For notational convenience, we will additionally define random variables st to represent the state at time t, and a R t ∼ πR(• | st) and a H t ∼ πH (• | st) to represent the human and robot actions at time t, respectively. Empowerment. Our work builds on a long line of prior methods that use information theoretic objectives for RL. Specifically, we adopt empowerment as an objective for training an assistive agent (Du et al., 2020; Salge et al., 2014; Klyubin et al., 2005). This section provides the math- ematical foundations for empowerment, as developed in prior work. Our work will build on the prior work by (1) providing an information geometric interpretation of what empowerment does (Sec. 3.3) and (2) providing a scalable algorithm for estimating and optimizing empowerment, go- ing well beyond the gridworlds studied in prior work. The idea behind empowerment is to think about the changes that an agent can effect on a world; an agent is more em- powered if it can effect a larger degree of change over future outcomes. Following prior work (Choi et al., 2021; Klyubin et al., 2005; Salge et al., 2014), we measure empowerment by looking at how much the actions taken now affect out- comes in the future. An agent with a high degree of empow- erment exerts a high degree of control of the future states by simply changing the actions taken now. Like prior work, we measure this degree of control through the mutual informa- tion I(s +; a H) between the current action a H and the future states s +. Note that these future states might occur many time steps into the future. Empowerment depends on several factors: the environment dynamics, the choice of future actions, the current state, and other agents in the environment. Different problem settings involve maximizing empowerment using these different fac- tors. In this work, we study the setting where a “human” agent and a “robot” agent collaborate in an environment; the robot will aim to maximize the empowerment of the human. This problem setting was introduced in prior work (Du et al., 2020). Compared with other mathematical frameworks for learning assistive agents (Reddy et al., 2018), framing the problem in terms of empowerment means that the assistive agent need not infer the human’s underlying intention, an inference problem that is typically challenging (Ratliff et al., 2006; Abbeel & Ng, 2004). Formally, we define the empowerment E(πH , πR) as the mutual information between the human’s actions and the future states s + while interacting with the robot: E(πH , πR) = E [ ∞∑ t=0 γtI(a H t ; s + | st)] , (1) where s + is a future state sampled K ∼ Geom(1 − γ) 3 (a) State marginal polytope (b) Mutual information (c) Maximizing empowerment Figure 2: The Information Geometry of Empowerment, illustrating the analysis in Sec. 3.3. (Left) For a given controlled Markov process, each policy induces a distribution over states. In a 3-state MDP, we can represent each policy as a vector lying on the 2-dimensional probability simplex. We refer to the set of all possible state distributions as the state marginal polytope. (Center) Mutual information corresponds to the distance between the center of the polytope and the vertices that are maximally far away. (Right) Empowerment corresponds to maximizing the size of this polytope. For example, when an assistive agent moves an obstacle out of a human user’s way, the human user can spend more time at desired state. steps into the future under the behavior policies πH , πR, and where the mutual information is defined as I(a H t ; s + | st) ≜ Est,st+k,aH t ,aR t [ log p(st+K = st+k | st = st, a H t = at) p(st+K = st+k | st = st) ]. Note that this objective resembles an RL objective: we do not just want to maximize this objective greedily at each time step, but rather want the assistive agents to take actions now that help the human agent reach states where it will have high empowerment in the future. 3.2 Assistive Agents Maximize Coverage Intuitively, the assistive agent should aim to maximize the size of this set of possible measures. We can formalize this intuition by employing a result from Eysenbach et al. (2021, Lemma 6.2), which says that a human maximizing mutual information will only select those skills z that are maximally far away from the prior. Lemma 1 (Lemma 6.2 from Eysenbach et al. (2021)). Let πH (z) be the human’s skill distribution that maximizes mu- tual information. Then we have π∗ H (z) > 0 =⇒ DKL(ρ(s | z) ∥ ∥ ρ(s)) = max z∗ DKL(ρ(s | z∗) ∥ ∥ ρ(s) ). (2) Noting that the mutual information is the expected value of this KL divergence over π∗ H (z), we have I π∗ H (s +; z) = max z∗ DKL(ρ(s | z∗)∥ ∥ρ(s) ) ≜ dmax. (3) Thus, we can think about mutual information maximization as finding the set of skills with the maximal coverage – where skills are maximally far away from their center. Now, by extension, a robot assistant that is maximizing this mutual information also aims to increase the size of this set: max πR I πR,π∗ H (s +; z) = max πR dmax. (4) In other words, an agent maximizing the empowerment of the human will aim to increase the support of goals the human can reach conditioned on their intention (Fig. 2). 3.3 The Information Geometry of Empowerment To build on this intuition, we will show that in the special case where the human is well-modeled as optimizing a re- ward function, we can relate empowerment maximization to reward maximization. Since a key advantage of empower- ment is that it does not necessarily require this assumption to be a meaningful assistance objective, we can view our objective as a generalization of the assistance problem be- yond the CIRL setting (Hadfield-Menell et al., 2016). In particular, we will show that under certain assumptions max- imizing empowerment corresponds to provably increasing their expected rewards. Lemma 2. Assume that a human has learned skills π(a | s, z) by maximizing mutual information I(s +; z) and adapts to a reward function by minimizing the regularized regret: min ρ∗∈CπR max ρ+∈C Eρ+(s)[r(s)] − Eρ∗(s)[r(s)] + DKL(ρ∗(s)∥ρ(s)). (5) We assume that the human chooses the prior ρ(s) that mini- mizes this regret for the worst-case choice of reward function (i.e., the minimax optimal prior). An assistive agent that maximizes I πR (s +; z) minimizes the worst-case (regular- ized) regret incurred by the human. 4 Letting π∗ R ∈ arg maxπR I πR (s +; z), we have π∗ R ∈ arg min πR ( min ρ(s)∈CπR max r(s) min ρ∗∈CπR max ρ+∈CπR Eρ+(s)[r(s)] − Eρ∗(s)[r(s)] + DKL(ρ∗(s)∥ρ(s))) . (6) The proof is in Appendix B. To the best of our knowl- edge, this theoretical result provides the first formal link between empowerment maximization and reward maximiza- tion. This motivates us to develop a scalable algorithm for empowerment maximization, which we introduce in the following section. 4 Estimating and Maximizing Empowerment with Contrastive Representations Directly computing equation 1 would require access to the human policy, which we don’t have. Therefore, we want a tractable estimation that still performs well in large en- vironments which are more difficult to model due to the exponentially increasing set of possible future states. To better-estimate empowerment, we learn contrastive repre- sentations that encode information about which future states are likely to be reached from the current state. These con- trastive representations learn to model mutual information between the current state, action, and future state, which we then use to compute the empowerment objective. 4.1 Estimating Empowerment To estimate this empowerment objective, we need a way of learning the probability ratio inside the expectation. Prior methods such as Du et al. (2020) and Salge et al. (2014) rollout possible future states and compute a measure of their variance as a proxy for empowerment, however this doesn’t scale when the environment becomes complex. Other meth- ods learn a dynamics model, which also doesn’t scale when dynamics become challenging to model (Jung et al., 2011). Modeling these probabilities directly is challenging in set- tings with high-dimensional states, so we opt for an indirect approach. Specifically, we will learn representations that encode two probability ratios. Then, we will be able to compute the desired probability ratio by combining these other probability ratios. Our method will learn three representations: 1. ϕ(s, a R, a H) – This representation can be understood as a sort of latent-space model, predicting the future represen- tation given the current state s and the human’s current action a H as well as the robot’s current action a R. 2. ϕ′(s, a R) – This representation can be understood as an uncontrolled model, predicting the representation of a future state without reference to the current human action a H. This representation is analogous to a value function. 3. ψ(g) – This is a representation of a future state. We will learn these three representations with two con- trastive losses, one that aligns ϕ(s, a H) ↔ ψ(g) and one that aligns ϕ′(s) ↔ ψ(g) max ϕ,ϕ′,ψ E{(si,ai,s′ i)∼p(st,aH t ,st+k)}N i=1 [ Lc({ϕ(si, ai) }, {ψ(s ′ j)} ) + Lc({ϕ′(si) }, {ψ(s ′ j)})] , (7) where the contrastive loss Lc is the symmetrized infoNCE objective (van den Oord et al., 2019): Lc({xi}, {yj}) ≜ N∑ i=1 [ log( exT i yi ∑N j=1 exT i yj ) + log( exT i yi ∑N j=1 exj T yi )] . (8) We have colored the index j for clarity. At convergence, these representations encode two probabilities ratios (Poole et al., 2019), which we will ultimately be able to use to estimate empowerment (Eq. 1): ϕ(s, a R, a H) T ψ(g) = log[ p(st+K =g|st=s,a H t =a) C1p(st+K =g) ] (9) ϕ′(s, a R) T ψ(g) = log[ p(st+K =st+k|st=st) C2p(st+K =g) ] . (10) Note that our definition of empowerment (Eq. 1) is defined in terms of similar probability ratios. The constants C1 and C2 will mean that our estimate of empowerment may be off by an additive constant, but that constant will not affect the solution to the empowerment maximization problem. 4.2 Estimating Empowerment with Learned Representations To estimate empowerment, we will look at the difference between these two inner products: ϕ(st+K, a R, a H) T ψ(g) − ϕ(st+K, a R)T ψ(g) = log p(st+K | s, a H) − log C1 − ˘˘˘˘˘˘ log p(st+K) − log p(st+K | s) + log C2 + ˘˘˘˘˘˘ log p(st+K) = log p(st+K | s, a H) p(st+K | s) + log C2 C1 . Note that the expected value of the first term is the condi- tional mutual information I(st+K; a H | s). Our empow- erment objective corresponds to averaging this mutual in- formation across all the visited states. In other words, our objective corresponds to an RL problem, where empower- ment corresponds to the expected discounted sum of these log ratios: E(πH , πR) = EπH ,πR [∑∞ t=0 γtI(aH t ; st | st)] ≈ EπH ,πR [∑∞ t=0 γt(ϕ(st, a R, a H) − ϕ(st, a R))T ψ(g) − log C2 C1 ]. 5 Algorithm 1: Empowerment via Successor Representations (ESR) Input: Human policy πH (a | s) Randomly initialize assistive agent policy πR(a | s), and representations ϕ(s, a R, a H), ψ(s, a T ), and ψ(g). Initialize replay buffer B. while not converged do Collect a trajectory of experience with human policy and assistive agent policy, store in replay buffer B. Update representations ϕ(s, a R, a H), ψ(s, a T ), and ψ(g) with the contrastive losses in Eq. (7). Update πR(a | s) with RL using reward function r(s, a R, a H) = (ϕ(s, a R, a H) − ϕ′(s, a R))T ψ(g). Return: Assistive policy πR(a | s). (a) Obstacle Gridworld (b) Cramped Room (c) Coordination Ring Figure 3: The modified environment from Du et al. (2020) scaled to N = 7 blocks (left), and the two layouts of the Overcooked environment (Carroll et al., 2020) (middle and right). The approximation above comes from function approxima- tion in learning the Bayes optimal representations. Again, note that the constants C1 and C2 do not change the opti- mization problem. Thus, to maximize empowerment we will apply RL to the assistive agent πR(a | s) using a reward function r(s, a R) = (ϕ(st, a R, a H) − ϕ(st, a R))T ψ(g). (11) 4.3 Algorithm Summary We propose an actor-critic method for learning the assis- tive agent. Our method will alternative between updating these contrastive representations and using them to estimate a reward function (Eq. 11 that is optimized via RL. We summarize the algorithm in Alg. 1. In practice, we use SAC (Haarnoja et al., 2018) as our RL algorithm. In our experiments, we will also study the setting where the human user updates their policy alongside the assistive agent. 5 Experiments We hope to answer two questions with our experiments: (1) Does our approach enable assistance in standard coopera- tion benchmarks? (2) Does our approach scale to harder benchmarks where prior methods fail? Our experiments will use two benchmarks designed by prior work to study assistance: the obstacle gridworld (Du et al., 2020) and Overcooked (Carroll et al., 2020). Our main base- line will be AvE (Du et al., 2020), a prior empowerment- based method. Our conjecture is that both methods will perform well on the lower-dimensional gridworld task, and that our method will scale more gradefully to the higher dimensional Overcooked environment. We will also com- pare against a naïve baseline where the assistive agent acts randomly. 5.1 Do contrastive successor representations effectively estimate empowerment? We test our approach in the assistance benchmark suggested in Du et al. (2020). The human (orange) is tasked with reaching a goal state (green) while avoiding the obstacles (purple). The AI assistant can move blocks one step at a time in any direction (Du et al., 2020). While the original bench- mark used N = 2 obstacles, we will additionally evaluate on harder versions of this task with N = 5, 7, 10 obstacles. We show results in Fig. 4. On the easiest task, both our method and AvE achieve similar asymptotic reward, though our method learns more slowly than AvE. However, on the tasks with moderate and high degrees of complexity, our approach (ESR) achieves significantly higher rewards than AvE, which performs worse than a random controller. These experiments support our claim that contrastive successor representations provide an effective means for estimating empowerment, and hint that ESR might be well suited for solving higher dimensional tasks. 6 50k 100k 150k 200k 250k 10 20 30Reward 2 obstacles 100k 200k 300k 400k 0 10 20 5 obstacles 200k 400k 600k 10 20 StepReward 7 obstacles 500k 1M 1.5M 5 10 Step 10 obstacles ESR (Ours) AvE Random Figure 4: We apply our method to the benchmark proposed in prior work (Du et al., 2020), visualized in Fig. 3a. The four subplots show variant tasks of increasing complexity (more blocks), (±1 SE). The prior approach (AvE (Du et al., 2020)) fails on all except the easiest task, highlighting the importance of scalability. 5.2 Does ESR scale to image-based observations? Our second set of experiments look at scaling ESR to the image-based Overcooked environment. Since contrastive learning is often applied to image domains, we conjectured that ESR would scale gradefully to this setting. We will eval- uate our approach in assisting a human policy trained with behavioral cloning taken from Laidlaw & Dragan (2022). The human prepares dishes by picking up ingredients and cooking them on a stove, while the AI assistant moves in- gredients and dishes around the kitchen. We focus on two environments within this setting: a cramped room where the human must pass ingredients and dishes through a narrow corridor, and a coordination ring where the human must pass ingredients and dishes around a ring-shaped kitchen (Figs. 3b and 3c). As belore, we compare with AvE as well as a naïve random controller. We report results in Fig. 5. On both tasks, we observe that our approach achieves higher rewards than AvE baseline, which performs no better than a random controller. Taken together with the results in the previous setting, these results highlight the scalability of ESR to higher dimensional problems. 6 Discussion One of the most important problems in AI today is equip- ping AI agents with the capacity to assist humans achieve their goals. While much of the amazing work in this area requires inferring the human’s intention, our work builds on prior work in studying how an assistive agent can empower a human user without inferring their intention. Relative to prior methods, we demonstrate how empowerment can be readily estimated using contrastive learning, paving the way for deploying these techniques on high-dimensional problems. Limitations. One of the main limitations of our approach is the assumption that the assistive agent has access to the human’s actions, which could be challenging to observe in practice. Automatically inferring the human’s actions re- mains an important problem for future work. A second limi- tation is that the method is currently an on-policy method, in the sense that the assistive agent has to learn by trial and error. Moving forward, we look forward to investigat- ing techniques from off-policy evaluation and cooperative game theory to enable faster learning of assistive agents with fewer trials. 7 60 80Reward Overcooked: Cramped Room 200k 400k 600k 800k 0 15 Step 500k 1M 1.5M 4 6 8 10 Step Overcooked: Coordination Ring ESR (Ours) AvE Random Figure 5: We test our approach in the Overcooked environment (Carroll et al., 2020). Our approach outperforms the prior method (AvE (Du et al., 2020)) and random selection without access to the human reward function (plotted ±1SE.) Safety risks. Perhaps the main risk involved with maxi- mizing empowerment is that it may be at odds with a hu- man’s agents goal, especially in contexts where the pursuit of that goal limit’s the human’s capacity to persue other goals. For example, a family choosing to have a kid has many fewer options over where they can travel for vacation, yet we do not want assistive agents to stymie families from having children. One key consideration is whom should be empowered. The present paper assumes there is a single human agent. Equiv- alently, this can be seen as maximizing the empowerment of all exogenous agents. However, it is easy to adapt the proposed method to maximize the empowerment of a single target individual. Given historical inequities in the distribu- tion of power, practitioners must take care when considering who’s empowerment to maximize. Similarly, while we fo- cused on maximizing empowerment, it is trivial to change the sign so that an “assistive” agent minimizes empower- ment. One could imagine using such a tool in policies to handicap one’s political opponents. References Abbeel, P. and Ng, A. Y. Apprenticeship learning via inverse reinforcement learning. In Twenty-First International Conference on Machine Learning - ICML ’04, pp. 1, Banff, Alberta, Canada, 2004. ACM Press. doi: 10.1145/ 1015330.1015430. Arora, S. and Doshi, P. A survey of inverse reinforcement learning: Challenges, methods and progress. Artificial Intelligence, 297:103500, August 2021. ISSN 0004-3702. doi: 10.1016/j.artint.2021.103500. Aubret, A., Matignon, L., and Hassas, S. A survey on intrinsic motivation in reinforcement learning, November 2019. Barto, A. G. Intrinsic Motivation and Reinforcement Learn- ing. In Baldassarre, G. and Mirolli, M. (eds.), Intrinsically Motivated Learning in Natural and Artificial Systems, pp. 17–47. Springer, Berlin, Heidelberg, 2013. ISBN 978- 36423-2-3-7-5-1. doi: 10.1007/978-3-642-32375-1_2. Berseth, G., Geng, D., Devin, C., Rhinehart, N., Finn, C., Jayaraman, D., and Levine, S. SMiRL: Surprise Minimiz- ing Reinforcement Learning in Unstable Environments. In International Conference on Learning Representations. arXiv, February 2021. doi: 10.48550/arXiv.1912.05510. Biyik, E., Losey, D. P., Palan, M., Landolfi, N. C., Shevchuk, G., and Sadigh, D. Learning Reward Functions from Di- verse Sources of Human Feedback: Optimally Integrating Demonstrations and Preferences, 2021. Burda, Y., Edwards, H., Storkey, A., and Klimov, O. Explo- ration by Random Network Distillation, October 2018. Carroll, M., Shah, R., Ho, M. K., Griffiths, T. L., Seshia, S. A., Abbeel, P., and Dragan, A. On the Utility of Learn- ing about Humans for Human-AI Coordination, January 2020. Carroll, M., Hadfield-Menell, D., Russell, S., and Dragan, A. Estimating and Penalizing Preference Shift in Recom- mender Systems. In Proceedings of the 15th ACM Confer- ence on Recommender Systems, RecSys ’21, pp. 661–667, New York, NY, USA, September 2021. Association for Computing Machinery. ISBN 978-145-038-4-5-8-2. doi: 10.1145/3460231.3478849. 8 Casper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer, J., Rando, J., Freedman, R., Korbak, T., Lindner, D., Freire, P., et al. Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback, Septem- ber 2023. Chen, S., Gao, J., Reddy, S., Berseth, G., Dragan, A. D., and Levine, S. ASHA: Assistive Teleoperation via Human- in-the-Loop Reinforcement Learning. In 2022 Interna- tional Conference on Robotics and Automation (ICRA), pp. 7505–7512, May 2022. doi: 10.1109/ICRA46639. 2022.9812442. Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual rep- resentations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020. Choi, J., Sharma, A., Lee, H., Levine, S., and Gu, S. S. Vari- ational empowerment as representation learning for goal- conditioned reinforcement learning. In International Con- ference on Machine Learning, pp. 1953–1963. PMLR, 2021. Dayan, P. Improving generalization for temporal difference learning: The successor representation. Neural computa- tion, 5(4):613–624, 1993. de Abril, I. M. and Kanai, R. A unified strategy for im- plementing curiosity and empowerment driven reinforce- ment learning, June 2018. Du, Y., Tiomkin, S., Kiciman, E., Polani, D., Abbeel, P., and Dragan, A. AvE: Assistance via Empowerment. In Advances in Neural Information Processing Systems, vol- ume 33, pp. 4560–4571. Curran Associates, Inc., 2020. Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S. Diver- sity is All You Need: Learning Skills without a Reward Function, October 2018. Eysenbach, B., Salakhutdinov, R., and Levine, S. The Information Geometry of Unsupervised Reinforcement Learning, October 2021. Eysenbach, B., Zhang, T., Levine, S., and Salakhutdinov, R. R. Contrastive Learning as Goal-Conditioned Rein- forcement Learning. Advances in Neural Information Processing Systems, 35:35603–35620, December 2022. doi: 10.48550/arXiv.2206.07568. Eysenbach, B., Myers, V., Salakhutdinov, R., and Levine, S. Inference via Interpolation: Contrastive Repre- sentations Provably Enable Planning and Inference. https://arxiv.org/abs/2403.04082v1, March 2024. Friston, K. The free-energy principle: A unified brain the- ory? Nature Reviews Neuroscience, 11(2):127–138, February 2010. ISSN 1471-003X, 1471-0048. doi: 10.1038/nrn2787. Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V., Zhu, H., Gupta, A., Abbeel, P., et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018. Hadfield-Menell, D., Russell, S. J., Abbeel, P., and Dragan, A. Cooperative inverse reinforcement learning. Advances in neural information processing systems, 29, 2016. Hadfield-Menell, D., Milli, S., Abbeel, P., Russell, S. J., and Dragan, A. Inverse reward design. Advances in neural information processing systems, 30, 2017. Houlsby, N., Huszár, F., Ghahramani, Z., and Lengyel, M. Bayesian active learning for classification and preference learning. arXiv preprint arXiv:1112.5745, 2011. Jung, T., Polani, D., and Stone, P. Empowerment for continu- ous agent—environment systems. Adaptive Behavior, 19 (1):16–39, February 2011. ISSN 1059-7123, 1741-2633. doi: 10.1177/1059712310392389. Kingma, D. P. and Ba, J. Adam: A Method for Stochastic Optimization, January 2017. Klyubin, A. S., Polani, D., and Nehaniv, C. L. Empower- ment: A universal agent-centric measure of control. In 2005 ieee congress on evolutionary computation, vol- ume 1, pp. 128–135. IEEE, 2005. Laidlaw, C. and Dragan, A. The Boltzmann Policy Distribu- tion: Accounting for Systematic Suboptimality in Human Models, April 2022. Laidlaw, C., Singhal, S., and Dragan, A. Preventing Reward Hacking with Occupancy Measure Regularization, March 2024. Laskin, M., Srinivas, A., and Abbeel, P. Curl: Contrastive unsupervised representations for reinforcement learning. In International conference on machine learning, pp. 5639–5650. PMLR, 2020. Laskin, M., Liu, H., Peng, X. B., Yarats, D., Rajeswaran, A., and Abbeel, P. CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery, March 2022. Mohamed, S. and Rezende, D. J. Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning. In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. Momennejad, I., Russek, E. M., Cheong, J. H., Botvinick, M. M., Daw, N. D., and Gershman, S. J. The successor representation in human reinforcement learning. Nature human behaviour, 1(9):680–692, 2017. 9 Myers, V., Biyik, E., Anari, N., and Sadigh, D. Learning multimodal rewards from rankings. In Conference on Robot Learning, pp. 342–352. PMLR, 2022. Nikolov, N., Kirschner, J., Berkenkamp, F., and Krause, A. Information-Directed Exploration for Deep Reinforce- ment Learning, March 2019. Pan, A., Bhatia, K., and Steinhardt, J. The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models, February 2022. Park, S., Choi, J., Kim, J., Lee, H., and Kim, G. Lipschitz- constrained Unsupervised Skill Discovery. In Interna- tional Conference on Learning Representations, October 2021. Poole, B., Ozair, S., Oord, A. V. D., Alemi, A., and Tucker, G. On Variational Bounds of Mutual Information. In Proceedings of the 36th International Conference on Ma- chine Learning, pp. 5171–5180. PMLR, May 2019. Ratliff, N. D., Bagnell, J. A., and Zinkevich, M. A. Maxi- mum margin planning. In Proceedings of the 23rd Inter- national Conference on Machine Learning - ICML ’06, pp. 729–736, Pittsburgh, Pennsylvania, 2006. ACM Press. ISBN 978-159-593-3-8-3-6. doi: 10.1145/1143844. 1143936. Reddy, S., Dragan, A. D., and Levine, S. Shared Autonomy via Deep Reinforcement Learning, May 2018. Reddy, S., Levine, S., and Dragan, A. First Contact: Un- supervised Human-Machine Co-Adaptation via Mutual Information Maximization. Advances in Neural Infor- mation Processing Systems, 35:31542–31556, December 2022. Russell, S. Learning agents for uncertain environments (ex- tended abstract). In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, pp. 101– 103, Madison Wisconsin USA, July 1998. ACM. ISBN 978-158-113-0-5-7-7. doi: 10.1145/279943.279964. Salge, C., Glackin, C., and Polani, D. Empowerment – an Introduction, October 2013. Salge, C., Glackin, C., and Polani, D. Empowerment–an introduction. Guided Self-Organization: Inception, pp. 67–114, 2014. Still, S. and Precup, D. An information-theoretic approach to curiosity-driven reinforcement learning. Theory in Biosciences, 131(3):139–148, September 2012. ISSN 1611-7530. doi: 10.1007/s12064-011-0142-z. Tien, J., He, J. Z.-Y., Erickson, Z., Dragan, A. D., and Brown, D. S. Causal Confusion and Reward Misiden- tification in Preference-Based Reward Learning, March 2023. Turner, A. M., Smith, L., Shah, R., Critch, A., and Tadepalli, P. Optimal Policies Tend to Seek Power. In NeurIPS 2021. arXiv, January 2023. doi: 10.48550/arXiv.1912.01683. van den Oord, A., Li, Y., and Vinyals, O. Representation Learning with Contrastive Predictive Coding, January 2019. Wang, T. and Isola, P. Understanding Contrastive Repre- sentation Learning through Alignment and Uniformity on the Hypersphere. In Proceedings of the 37th Interna- tional Conference on Machine Learning, pp. 9929–9939. PMLR, November 2020. Wirth, C., Akrour, R., Neumann, G., and Fürnkranz, J. A Survey of Preference-Based Reinforcement Learning Methods. Journal of Machine Learning Research, 2017. Wu, Z., Xiong, Y., Yu, S. X., and Lin, D. Unsupervised feature learning via non-parametric instance discrimina- tion. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3733–3742, 2018. 10 A Experimental Details We ran all our experiments on NVIDIA RTX A6000 GPUs with 48GB of memory within an internal cluster. Each evaluation seed took around 5-10 hours to complete. Our losses (Eqs. (7) and (11)) were computed and optimized in JAX with Adam (Kingma & Ba, 2017). The experimental results described in Section 5 were obtained by averaging over 5 seeds for the Overcooked coordination ring layout, 15 for the cramped room layout, and 20 for the obstacle gridworld environment. Specific hyperparameter values can be found in our code, which is available at https://anonymous. 4open.science/r/esr-7E94. B The Information Geometry of Empowerment This section considers an objective that might be slightly different: I πR (s +; z), where z is a representation of the human’s intention. In practice, this could be represented as a sequence of actions (as in the main doc above), but it also includes reactive and closed loop policies. This mutual information also depends on the human’s policy πH , but here we are interested in just the dependence on the robot. Here’s the primary question of interest: what actions/behaviors should the robot employ to maximize the mutual information between the human’s intentions and the outcomes. Note that this is a standard mutual information skill learning objective. However, whereas prior work typically optimizes this objective w.r.t. the human’s policy πH (a | s, z), here we aim to optimize this w.r.t. the robot’s policy πR(a | s). Note that the robot is not conditioned on the human’s intention z. We assume that this intention is not observed. 1 The objective can then be written as max πR I πR (s+; z). (12) One way of thinking about this optimization problem is that we are modifying the MDP itself. However, rather than (say) changing the positions of clouds or changing the framerate, we will only consider changes that can be mediated by an interactive robot agent. These include changes such as pushing an object, opening a drawer, charging or discharging another robot. B.1 Preliminaries: Relating Mutual Information to Reward Maximization The mutual information is an information theoretic quantity, defined in terms of bits and probabilities. However, what we actually care about is the ability of an assisted human to achieve high rewards. So, we need a way of relating this mutual information objective to reward maximizing. We start by recalling the result from Eysenbach et al. (2021), which provides one such relation: max πH (z) I(s+; z) = min ρ(s)∈C max r(s) min ρ∗∈C max ρ+∈C Eρ+(s)[r(s)] − Eρ∗(s)[r(s)] ︸ ︷︷ ︸ regret +DKL(ρ∗(s)∥ρ(s)). (13) Unpacking this objective. There’s a lot of math in that equation, so let’s unpack it a bit. The LHS is about learning skills for the human policy πH . We assume that all possible skills are enumerated, so the human simply has to select from this menu of skills by deciding much more of each skill πH (z) to order from this menu. Of course, this menu is exponentially long, but it is finite and well defined, and practical algorithms won’t actually attempt to enumerate this menu of skills. The optimization problem on the LHS is about selecting those skills that most readily maximize the mutual information – the skills that have a strong influence over the states visited in the future. The RHS has a whole bunch of terms. For a given reward function r(s), we care about how much reward a particular policy gets. The RHS studies this standard expected reward by using the dual of the RL problem, thinking about the states ρ(s) visited by a policy and counting up the rewards at those states. The term Eρ+(s)[r(s)] is the expected reward for a policy with occupancy measure ρ +(s). Thus, maxρ+(s) Eρ+(s)[r(s)] is the maximal reward that any policy can get on this particular reward function. We are often given a policy (or its occupancy measure ρ∗(s)) and and reward function r(s) and want to know how good that policy is for that reward function. While we could directly measure the expected reward, we usually don’t know whether 1One area for future work is to study whether actively inferring this intention improves assistance. Another area for future work is to study whether non-Markovian robot policies can perform better than their Markovian counterparts because they can accumulate information about the human’s intentions across time. 11 this is a particularly good value or not. Instead, we might measure the regret of the policy: how much lower is its expected reward, as compared to the optimal policy for that reward function: REGRET(ρ(s)∗, r(s)) = max ρ+(s) Eρ+(s)[r(s)] − Eρ∗(s)[r(s)]. (14) This is the term that appears on the RHS on Eq. 13. Now, when we have limited data, we usually want to minimize a regularized notion of regret. This is what ρ∗ is doing, using the KL divergence against a prior ρ(s) as the regularization term: min ρ∗(s) REGRET(ρ∗(s), r(s)) + DKL(ρ∗(s)∥ρ(s)). (15) This objective above can be interpreted as the difficulty of learning to maximize reward function r(s). But, in the unsupervised RL setting, which reward functions should we learn how to optimize? We could take an average case approach, but this runs into challenges because “average” depends on a choice of measure. Instead, we take a worst-case approach, selecting the reward function that is most challenging to adapt to: max r(s) min ρ∗(s) REGRET(ρ∗(s), r(s)) + DKL(ρ∗(s)∥ρ(s)). (16) Finally, when discussing adaptation, we had some prior ρ(s) to which we were referring. The overall aim is to find the prior ρ(s) that makes it easy to adapt to the most challenging reward function: min ρ(s)∈C max r(s) ADAPTATIONOBJECTIVE(ρ(s), r(s)). (17) Eq. 13 tells us that the problem of finding this optimal prior is equivalent to maximizing mutual information. B.2 Application to Empowerment We can extend this result to the assistive setting, thinking about how an assistive robot should act to make it easier for a human to maximize their worst-case rewards. From the human’s perspective, the robot is just another part of the MDP. 2 So, to apply the result from Eq. 13 to empowerment, we just need to modify the definitions to depend on the choice of πR. On the LHS, let’s use I πR (s +; z) to denote the mutual information between the human’s choice of skills πH (z) and the future states, when interacting in an environment alongside a robot πR(a | s). The RHS thinks about the state occupancy measure of the human, terms like ρ(s), ρ +(s), ρ ∗(s). An effective assistive agent will enable a human to visit a wide distribution over states, or to spend more time visiting any given state. We will use CπR to denote the feasible occupancy measures when interacting alongside an assistive agent. B.3 Assistive Agents Minimize Regret We can now state our main result, which is a direct corollary of Eq. 13 Consider the human and the robot as one monolithic agent selecting actions aH , aR ∼ πH (aH | s, z)πR(aR | s). This policy is Markovian, so we can immediately apply Eq. 13. We start with some intuition: we would like an assistive agent to help the human maximize rewards. The challenge is that the assistive agent doesn’t know what reward function the human is trying to solve, and we would like to avoid this inverse RL problem. So, we will take a worst-case approach, thinking about how the assistive agent can help the human solve the hardest task. We will measure difficulty as a combination of (1) regret versus the optimal policy, and (2) divergence from a prior over policies. Notation. Let CπR denote the set of feasible state marginal distributions with cooperating with assistive agent πR(a | s). We assume that this assistive agent does not know the human’s intention. We will measure regret against an omniscient assistive agent, which knows the human’s intent. Thus, we compare to an occupancy measure optimized within the larger set C, which includes adaptive strategies. Assume human πH (a | s) and robot πR(a | s) induce state occupancy measure ρ∗(s). We define their regret, which is measured relative to the highest reward they could achieve with any assistive agent (hence, we use ρ+ ∈ C rather than ρ+ ∈ CπR ): max ρ+∈C Eρ+(s)[r(s)] − Eρ∗(s)[r(s)]. 2The reason we wanted to assume that the robot was Markovian was so that this remains a Markov decision process. 12 We will include an additional regularization term, so the overall objective becomes max ρ+∈C Eρ+(s)[r(s)] − Eρ∗(s)[r(s)] + DKL(ρ∗(s)∥ρ(s)). Given a reward function r(s), we assume that the human adapts by minimizing this regularized regret. We assume that the assistive agent does not adapt. Thus, the human is optimizing over the smaller set CπR : min ρ∗∈CπR max ρ+∈C Eρ+(s)[r(s)] − Eρ∗(s)[r(s)] + DKL(ρ∗(s)∥ρ(s)). As before, the reward function is adversarially chosen. And, the human’s job is to find the prior ρ(s) that is minimax optimal: min ρ(s)∈CπR max r(s) min ρ∗∈CπR max ρ+∈C Eρ+(s)[r(s)] − Eρ∗(s)[r(s)] + DKL(ρ∗(s)∥ρ(s)). Lemma 3. Assume that a human has learned skills π(a | s, z) by maximizing mutual information I(s +; z) and adapts to a reward function by minimizing the regularized regret: min ρ∗∈CπR max ρ+∈C Eρ+(s)[r(s)] − Eρ∗(s)[r(s)] + DKL(ρ∗(s)∥ρ(s)). We assume that the human chooses the prior ρ(s) that minimizes this regret for the worst-case choice of reward function (i.e., the minimax optimal prior). An assistive agent that maximizes I πR (s +; z) minimizes the worst-case (regularized) regret incurred by the human. Letting π∗ R ∈ arg maxπR I πR (s +; z), we have π∗ R ∈ arg min πR ( min ρ(s)∈CπR max r(s) min ρ∗∈CπR max ρ+∈C Eρ+(s)[r(s)] − Eρ∗(s)[r(s)] + DKL(ρ∗(s)∥ρ(s) )) . (18) C Simplifying the Objective The reward function in Eq. 11 is itself a random variable because it depends on future states g. This subsection describes how this randomness can be removed. To do this, we follow prior work (Wang & Isola, 2020; Eysenbach et al., 2024) in arguing that the learned representations ψ(g) follow a Gaussian distribution: Assumption 1 (Based on Wang & Isola (2020)). The representations of future states ψ(g) learned by contrastive learning have a marginal distribution that is Gaussian: p(ψ) = ∫ p(g)δ(ψ = ψ(g)) dg d = N (0, I). (19) With this assumption, we can remove the random sampling of g from the reward function. We start by noting that the learned representations tell us the relative likelihood of seeing a future state (Eq. 10). Assumption 1 will allow us to convert these relative likelihoods into likelihoods. Ep(s+|s,aR,aH)[r(s, a R)] = Ep(s+) [ p(s +|s,a R,a H) p(s+) r(s, a R) ] = Ep(s+)[C1eϕ(s,a R,a H) T ϕ(s +)r(s, a R)] = C1Eψ∼p(ϕ(s+))[eϕ(s,a R,a H) T ψ(ϕ(s, a R, a H) − ϕ(s, a R))T ψ] = C1(ϕ(s, a R, a H) − ϕ(s, a R) )T ∫ 1 (2π)d/2 e− 1 2 ∥ψ∥2 2+ϕ(s,a R,a H) T ψψ dψ = C1(ϕ(s, a R, a H) − ϕ(s, a R) )T e 1 2 ∥ϕ(s,a R,aH)∥2 2 ∫ 1 (2π)d/2 e− 1 2 ∥ψ∥2 2+ϕ(s,a R,aH)T ψ− 1 2 ∥ϕ(s,a R,a H )∥2 2 ψ dψ = C1(ϕ(s, a R, a H) − ϕ(s, a R) )T e 1 2 ∥ϕ(s,a R,aH)∥2 2Eψ∼N (µ=ϕ(s,aR,aH),Σ=I)[ ψ] = C1e 1 2 ∥ϕ(s,a R,a H)∥2 2(ϕ(s, a R, a H) − ϕ(s, a R))T ϕ(s, a R, a H). (20) 13","libVersion":"0.3.2","langs":""}