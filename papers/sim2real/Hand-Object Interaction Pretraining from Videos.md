# Hand-Object Interaction Pretraining from Videos

**Authors**: Himanshu Gaurav Singh, Antonio Loquercio, Carmelo Sferrazza, Jane Wu, Haozhi Qi, Pieter Abbeel, Jitendra Malik

#sim2real
#learning-from-video
#reinforcement-learning
#behavioral-cloning

[[papers/sim2real/README#[2024-09] Hand-Object Interaction Pretraining from Videos|README]]

[Paper](http://arxiv.org/abs/2409.08273)
[Code](https://github.com/hgaurav2k/hop)
[Website](https://hgaurav2k.github.io/hop/)

## Abstract

> We present an approach to learn general robot manipulation priors from 3D hand-object interaction trajectories. We build a framework to use in-the-wild videos to generate sensorimotor robot trajectories. We do so by lifting both the human hand and the manipulated object in a shared 3D space and retargeting human motions to robot actions. Generative modeling on this data gives us a task-agnostic base policy. This policy captures a general yet flexible manipulation prior. We empirically demonstrate that finetuning this policy, with both reinforcement learning (RL) and behavior cloning (BC), enables sample-efficient adaptation to downstream tasks and simultaneously improves robustness and generalizability compared to prior approaches. Qualitative experiments are available at: <https://hgaurav2k.github.io/hop/>.

## Summary

- Advantages of learning from video
    - They are abundant
    - They cover a wide range of skills that we want robots to master
    - They reflect natural or socially acceptable behaviors that we want robots to emulate
- Disadvantages of learning from video
    - Hard to estimate physics
    - Embodiment gap
- Trajectories are generated by mapping 3D hand-object interactions to the robot's embodiment via a physically grounded simulator
- Extract 3D hand movements from videos
    - Map these motions to robot actions in simulator

## Background

- In contrast to previous work, **we do not assume a strict alignment of the human's intent in the video and the downstream robot tasks**
    - Basic skills for manipulation lie on a manifold well covered by unstructured human-object interactions
- **Simulator as intermediary between video and robot trajectory**
    - Can add physics lost in videos
    - Synthesize large datasets without breaking robot
    - Increase dataset diversity by randomizing simulation environment
- Previous works learning from video
    - VIP, R3M, MVP
    - These works focus on visual generalization, e.g., picking up two objects with the same shape but different colors
    - They have not yet demonstrated action generalization, where motor skills are adapted to accomplish novel objectives

## Method

- HOP pipeline ![[hop.png]]
    - Hand-object interaction videos to 3D
        - Closely follows MCC-HO, which infers hand-object geometry as point cloud
    - Mapping 3D human trajectories to robot actions
        - Non-linear optimization problem
        - **Find action by optimizing cost function at each time step**: $\min_{a[k]}\frac{1}{2}||x_h[k]-f(a[k])||^{2}+ \lambda||a[k] - \phi[k-1]||^{2}$
            - $f$ is robot's forward kinematics
            - $x_h[k]$ are 3D coordinates of a set of keypoints on human hand
        - Intuitively, first term minimizes difference between robot and human hand
            - Second term proportional to energy required to execute action, which we want to minimize
    - **In-the-wild videos do not have reliable information about objects' physical properties, e.g., mass or friction**
        - Disregard the dynamics of the manipulated object and place it on every step at the location observed in the video
    - In simulation, randomize simulated scene (obstacles, vary positions of objects) to increase data diversity
- **Resulting dataset $\mathcal{T}$ contains knowledge that could be valuable to any manipulation tasks
    - Train a transformer with $\mathcal{T}$
        - Takes proprioception and observation input from past 16 timesteps, predicts next action
        - Minimize loss $\mathcal{L}(\tau;\theta) = E_{t\sim[1\dots T]}\left [||a[t-L:t] - \pi_b(o[t-L:t])||_1\right]$
            - Choose policy to minimize difference between predicted and true action
    - Unlike other work, $\mathcal{T}$ contains neither real-world demonstrations nor complete task executions
    - Downstream finetuning with RL and BC

## Results

- Experimental setup
    - 7-DOF arm with Allegro hand
    - Simulation in IsaacGym
    - Hand datasets: DexYCB, 100 Days of Hands
    - NLOpt library for optimization
    - Finetune transformer with PPO in simulation
    - Real world finetuning on limited demonstrations
    - Real-world tasks
        - Grasp and drop (15 demonstrations)
        - Grasp and pour (15 demonstrations)
        - Grasp and lift (50 demonstrations)
            - **A single policy needs to pick four objects with different affordances**
- Comparison to visual pre-training (real-world)
    - Baselines: VIT-B (ImageNet), R3M, VIP, MVP (Ego4D), Diffusion-policy with UNet
    - Outperform baseline by 30% on hardest task: grasp and lift
        - Shows generalization of HOP
- Comparison to RL (simulation)
    - HOP improves sample-efficient RL and efficient exploration
