# Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion

**Authors**: Yongyuan Liang, Tingqiang Xu, Kaizhe Hu, Guangqi Jiang, Furong Huang, Huazhe Xu

#diffusion

[[papers/generative-models/README#[2024-11] Make-An-Agent A Generalizable Policy Network Generator with Behavior-Prompted Diffusion|README]]

[Paper](http://arxiv.org/abs/2407.10973)
[Code](https://github.com/cheryyunl/Make-An-Agent)
[Website](https://cheryyunl.github.io/make-an-agent/)

## Abstract

> Can we generate a control policy for an agent using just one demonstration of desired behaviors as a prompt, as effortlessly as creating an image from a textual description? In this paper, we present Make-An-Agent, a novel policy parameter generator that leverages the power of conditional diffusion models for behavior-to-policy generation. Guided by behavior embeddings that encode trajectory information, our policy generator synthesizes latent parameter representations, which can then be decoded into policy networks. Trained on policy network checkpoints and their corresponding trajectories, our generation model demonstrates remarkable versatility and scalability on multiple tasks and has a strong generalization ability on unseen tasks to output well-performed policies with only few-shot demonstrations as inputs. We showcase its efficacy and efficiency on various domains and tasks, including varying objectives, behaviors, and even across different robot manipulators. Beyond simulation, we directly deploy policies generated by Make-An-Agent onto real-world robots on locomotion tasks. Project page: <https://cheryyunl.github.io/make-an-agent/>

## Summary

- Traditional policy learning involves using demonstrations to make states to actions, modeling a narrow behavior distribution
- **Can we predict optimal policy network parameters using suboptimal trajectories from offline data?**
	- Learn underlying parameter distributions in parameter space
- Policy network generation as a conditional denoising diffusion process
	- Diffusion models demonstrated good performance on text (low-dim) to image (high-dim) synthesis
	- **Hypothesis: behavior (low-dim) to policy (high-dim) synthesis?**
- Leverage **low-dimensional agent behaviors** as prompts to generate optimal policies in parameter space without downstream fine-tuning
- Different tasks often share underlying skills or environment information
	- Policy generator can exploit these correlations in parameter space
- Advantages of Make-An-Agent
	- Versatility: conditioning on agent behavior embeddings
	- Generalization
	- Robustness: can synthesize good policies when fed noisy trajectories

## Background

- Parameter generation
	- Hypernetworks, hypertransformer, G.pt, p-diff
- Learning to Learn for Policy Learning
	- Meta-RL: learning a policy that can adapt to new tasks; requires rewards
	- Meta-IL: requires expert demonstrations
- Diffusion models: iteratively add noise to data, then denoising it to recover original signal
	- [Blog post](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)
	- [Diffusion without Tears](https://baincapitalventures.notion.site/Diffusion-Without-Tears-14e1469584c180deb0a9ed9aa6ff7a4c)
	- Forward process: $q(x_t|x_{t-1}) = \mathcal{N}(x_{t}|\sqrt{1-\beta_t}x_{t-1},\beta_tI)$
		- $x_t$ is randomly sampled from a diagonal Gaussian
			- Variance $\beta_t\in(0,1)$ at time $t$ and increases with time
			- Mean $\sqrt{1-\beta_t}x_{t-1}$ brings the mean closer to zero with time
		- In practice, $x_{t}=\sqrt{1-\beta_t}x_{t-1}+\epsilon$ where $\epsilon\sim\mathcal{N}(0,\beta_tI)$
		- $q(x_{1:T}|x_0)=\prod_{t=1}^{T}q(x_t|x_{t-1})$
	- Denoising process: $p_{\theta}(x_{t-1}|x_{t)}= \mathcal{N}(x_{t-1}|\mu_{\theta}(x_{t},t),\Sigma_{\theta}(x_{t},t)))$
		- Time step taken as input
		- $p(x_{o:T}|x_0)=p(x_T)\prod_{t=1}^{T}q(x_{t-1}|x_{t})$
		- Noise prediction neural network parameterized by $\theta$
		- Learn reverse transitions that maximize the likelihood of forward transitions at each times step $t$
	- Noise prediction network $\theta$ optimized using the objective
		- $\mathcal{L}_{\text{DM}}(\theta):=E_{x_{0}\sim q; \epsilon\sim \mathcal{N}(0,1),t}\left [ \| \epsilon-\epsilon_{\theta}(\sqrt{\bar{\alpha_t}}x_{0}+\sqrt{1-\bar{\alpha_t}}\epsilon,t)\|^2\right]$
			- $\epsilon\sim \mathcal{N}(0, I)$ is target Gaussian noise
			- $\bar{\alpha_t}=\prod_{s=1}^{t}1-\beta_s$
			- $\sqrt{\bar{\alpha_t}}x_{0}+\sqrt{1-\bar{\alpha_t}}\epsilon$ is the estimated distribution of $x_t$ in closed-form
- Forward process of diffusion analogous to encoder of VAE
	- Reverse process analogous to decoder

## Method

- Make-an-Agent ![[make_an_agent.png]]
- MLP with $m$ layers as common policy approximator, flattened to produce high-dimensional vector
	- $x=[x_{0},\dots,x_{m-1}]$ are the layers of the MLP
	- Encoder $\mathcal{E}$ encodes each flattened layer $x_i$ to lower-dimensional $z_i$
	- Decoder $\mathcal{D}$ can decode $z$ into $x$
	- Minimize $\mathcal{L}=\text{MSE}(x, \mathcal{D}(\mathcal{E}(x+\xi_{\mathcal{D}})+\xi_{\mathcal{E}}))$
		- $z=\mathcal{E}(x+\xi_{\mathcal{D}})$
		- $\xi_{\mathcal{D}}, \xi_{\mathcal{E}}$ are augmented noise
	- Autoencoder only needs to be trained once for each domain
- Behavior embeddings
	- Learned from training dataset
	- Goal is not to model distribution of states and actions, but provide conditional information for policy parameter generation
	- **Learn mutual information between preceding $n$ step trajectories and subsequent states with success signals**
		- $\mathbb{I}=\mathcal{I}(s_{success}; \{s_{i},a_{i}\}_{i=0}^n)$
		- Mutual information quantifies the amount of information obtained about one random variable by observing the other
			- $\mathbb{I}$ measures how much observing the past trajectory reduces uncertainty about whether the agent will succeed
	- For long trajectory $\tau$, decouple initial $n$ state-action pairs $\tau^n=(s_0,a_0,\dots,s_n,a_n)$ and $m$ states after the first success time $K$ as $\hat{\tau}=(s_K,s_{K+1},\dots_{K+m})$
		- Given batch of trajectories decoupled as $\left \{ \tau_i^n,\hat{\tau_i}\right\}_{i=1}^N$, optimize contrastive objective: $\mathcal{L}(\phi_{\theta},\psi_{\theta},W)=-\frac{1}{N}\sum\limits_{i=1}^{N}\log \frac{h_i^TWv_i}{\sum\limits_{j=1}^{N}h_i^TWv_j}$
		- **Ensures that embedding of past trajectory is similar to embedding of its own successful future state but dissimilar to other trajectories' future states, capturing important information about what leads to success**
		- $h_i=\phi_{\theta}(\tau_{i}^{n}),v_i=\psi_{\theta}(\hat{\tau_i})$ are embeddings and $W$ is a learnable metric that measures similarity between embeddings $h_i$ and $v_i$
	- Embeddings $(h_i,v_i)$ used as conditional input in our experiments
- Flexibility
	- With the consideration that in many scenarios, rewards are often sparse or non-existent, whereas success signals serve as a more direct indicator of whether a policy has achieved its objective
		- **We therefore use original trajectories that exclude reward information but include success information**
			- This could be a limitation as the training trajectories are suboptimal (from RL training), reward "ranks" trajectories
			- **Could this limit fine-grained policy improvements beyond binary success/failure?**
			- In simulation, reward could easily be incorporated into the behavior embedding
			- In real life, you can use models that estimate reward from visual perception (PROGRESSOR, VIP) to incorporate reward into the behavior embedding
	- For tasks without explicit success signals, such as locomotion, segment long trajectories into multiple shorter trajectories
		- For each segment, use the last $m$ states as $\hat{\tau}$ and the $0-n$ state-action pairs as $\tau^n$
- Conditional policy generator
	- Now we have parameter autoencoder and behavior embeddings
	- Transfer policy parameters $x$ to latent representation $z$ with autoencoder $\mathcal{E}$
	- Transfer trajectory $\tau$ as behavior embedding $\tau_{e}= \{h_i,v_i\}$
	- Train conditional diffusion generator with latent parameters $z$ conditioned on $\tau_e$
		- $\mathcal{L}_{\text{LDM}}(\theta):=E_{z.\epsilon\sim \mathcal{N}(0,1),t}\left [ \|\epsilon-\epsilon_{\theta}(z_{t}, \tau_{e}, t)\|_{2}^2\right]$
		- $z_t$ is a noisy version of the latent representation at time $t$
		- Neural backbone $\epsilon_{\theta}(z_{t}, \tau_{e}, t)$ is a **1D convolutional UNet** that predicts noise
			- Diffusion models start from pure noise and iteratively refine the image
			- This requires a network that can process multiple levels of image detail without losing fine features—which U-Net excels at
			- Contracting Path (Encoder) – Captures high-level features by progressively downsampling the input image
			- Expanding Path (Decoder) – Reconstructs the image by upsampling while incorporating high-resolution features from the encoder via skip connections
	- Conditioning on $\tau_e$ ensures that generated policies align with past behaviors, as the neural network $\epsilon_{\theta}$ learns to remove noise in a way that is consistent with $\tau_e$
		- **Policy generator will only produce policies aligned with trajectories that had high success probability**
	- Inference
		- Extract a behavior embedding $\tau_e$​ from a past trajectory (i.e. a partially completed task)
			- **How do they get the success embedding? Is this conditioned on a goal state?**
		- Sample from the diffusion model to generate a policy latent representation $z$
		- Decode $z$ into policy parameters $x$
		- Deploy the generated policy for better success
- Dataset
	- Build dataset containing 10000s of policy parameters and trajectories from deploying these policies
	- Obtained from multiple RL trainings across a range of tasks
	- Used to train autoencoder and behavior embedding models
	- Use encoded parameter representations and behavioral embeddings to train conditional diffusion model

## Results

- Simulated experiments
	- Metaworld
		- 10 seen tasks, 8 unseen tasks
		- Action space: [x,y,z,gripper]
		- State space: 39 dimensions
	- Robosuite
		- 3 tasks
		- Action space: [joint positions, gripper]
		- State space: 41 dimensions
	- Baselines
		- Multi-task BC
		- Multi-task RL, CARE
		- Meta-RL with hypernetworks
		- Meta-IL with decision transformer
	- Compared with baselines, we report both the best result among the generated policies and the average performance of the top 5 policies
		- **Is this fair? how many policies did you train with the other methods?**
	- Adaptability to environmental randomness on seen tasks
		- Despite test trajectories originating from the same environment initialization, the generated policy parameters are more diverse, thus possessing a strong ability to adapt to environmental randomness
	- Generalizability to unseen tasks
		- Figure 7: method remains resilient to noisy inputs, while the performance of the baselines is significantly impacted
			- We believe this is because our behavior embeddings only need to capture key dynamic information as conditions to generate policies, without directly learning state-action relationships from trajectories, resulting in better robustness
	- Ablations
		- Choice of behavior embeddings: utilizing $h$ and $v$ individually achieves comparable performance
		- Trajectory length: not sensitive past 40 steps
		- Policy network size: balance between policy performance and parameter reconstruction complexity
		- Number of parameters: not sensitive past 1000
		- Latent representation size: can't be too small to hinder autoencoder's decoding capacity, but can't be too large to affect performance of the generative model
- Real-world experiments
	- Quadrupedal locomotion
		- Task 1: avoid bouquet
		- Task 2: circumvent goal and ball
- Limitations
	- Excluding reward information - see "flexibility" section in methods
	- Exploring larger and more diverse policy networks
		- **These simple MLP networks work because the state space is small and tasks are simple, can this method scale?**
	- Capabilities of the parameter diffusion generator are limited by the parameter autoencoder
	- Generating other structures, further facilitating exploration in policy learning within the parameter space
