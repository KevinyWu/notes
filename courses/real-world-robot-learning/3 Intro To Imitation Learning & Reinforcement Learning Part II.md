# Intro To Imitation Learning & Reinforcement Learning Part II

#imitation-learning
#reinforcement-learning

[[courses/real-world-robot-learning/README#[3] Intro To Imitation Learning & Reinforcement Learning Part II|README]]

## Imitation Learning Through Inverse Reinforcement Learning

- BC might not generalize beyond demonstrations
- Learn explicitly the reward function that the demonstrator is trying to maximize
- Recall BC objective: $-\frac{1}{N}\sum\limits_{i=1}^{N}\left(\sum\limits_{t=1}^{T}\log \pi_{\theta}(a_{i,t}|s_{i,t})\right)$
	- Sum over optimal demonstrations
- Reward-weighted regression objective: $-\frac{1}{N}\sum\limits_{i=1}^{N}\left(\sum\limits_{t=1}^{T}\log \pi_{\theta}(a_{i,t}|s_{i,t}) \left(\sum\limits_{\tau = t,t+1,\dots}\gamma^{\tau-t}r_{i,\tau}\right) \right)$
	- Sum over non-optimal demonstrations, weighted by reward returns

## Limitations of Imitation Learning

- Data scarcity
- Demo sub-optimality
- Multimodality
- Cross-embodiment
- Teacher-student discrepancies
- Safety

## Online Reinforcement Learning

- Notation
	- "Reward": instantaneous reward
	- "Return": discounted sum of future rewards
- **Goal: given some environment, find the optimal policy $\pi^{*}(s):S\rightarrow A$**
	- "Optimal" means following $\pi^{*}$ maximizes expected return $\sum\limits_t\gamma^{t} r_{t+1}$
	- Contrast with supervised learning where we want to find a function $h(s):S\rightarrow A$ that minimizes loss $L$ over training pairs
	- Unlike supervised learning, RL can find solutions that the problem specifier did not already know
- Problems in RL
	- Exploration vs. exploitation
	- Credit assignment: which actions in a sequence were the good/bad ones

## Policy Gradients

- BC gradient: $-\frac{1}{N}\sum\limits_{i=1}^{N}\left(\sum\limits_{t=1}^{T}\nabla_{\theta}\log \pi_{\theta}(a_{i,t}|s_{i,t})\right)$
- Reward-weighted regression gradient $-\frac{1}{N}\sum\limits_{i=1}^{N}\left(\sum\limits_{t=1}^{T}\nabla_{\theta}\log \pi_{\theta}(a_{i,t}|s_{i,t}) \left(\sum\limits_{\tau = t,t+1,\dots}\gamma^{\tau-t}r_{i,\tau}\right) \right)$
- RL objective: maximize $E_{\tau_{i}\sim \pi_{\theta}(tau)}\left[\sum\limits_t\gamma^tr_{i,t}\right]$
	- Expectation over **data generated by the policy**
	- Vanilla policy gradient: $E_{\tau_{i}\sim \pi_{\theta}(tau)}\left[\sum\limits_{t=1}^{T}\nabla_{\theta}\log \pi_{\theta}(a_{i,t}|s_{i,t}) \left(\sum\limits_{\tau = t,t+1,\dots}\gamma^{\tau-t}r_{i,\tau}\right)\right]$
	- Looks similar to RWR gradient, except trajectories and actions are generated by the policy during learning
	- The reward term $\left(\sum\limits_{\tau = t,t+1,\dots}\gamma^{\tau-t}r_{i,\tau}\right)$ is often estimated by a Q-function
