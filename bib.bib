@online{amin$p^_06$VLAThat2025,
  title = {\$π\textasciicircum\{*\}\_\{0.6\}\$: A {{VLA That Learns From Experience}}},
  shorttitle = {\$π\textasciicircum\{*\}\_\{0.6\}\$},
  author = {Amin, Ali and Aniceto, Raichelle and Balakrishna, Ashwin and Black, Kevin and Conley, Ken and Connors, Grace and Darpinian, James and Dhabalia, Karan and DiCarlo, Jared and Driess, Danny and Equi, Michael and Esmail, Adnan and Fang, Yunhao and Finn, Chelsea and Glossop, Catherine and Godden, Thomas and Goryachev, Ivan and Groom, Lachy and Hancock, Hunter and Hausman, Karol and Hussein, Gashon and Ichter, Brian and Jakubczak, Szymon and Jen, Rowan and Jones, Tim and Katz, Ben and Ke, Liyiming and Kuchi, Chandra and Lamb, Marinda and LeBlanc, Devin and Levine, Sergey and Li-Bell, Adrian and Lu, Yao and Mano, Vishnu and Mothukuri, Mohith and Nair, Suraj and Pertsch, Karl and Ren, Allen Z. and Sharma, Charvi and Shi, Lucy Xiaoyang and Smith, Laura and Springenberg, Jost Tobias and Stachowicz, Kyle and Stoeckle, Will and Swerdlow, Alex and Tanner, James and Torne, Marcel and Vuong, Quan and Walling, Anna and Wang, Haohuan and Williams, Blake and Yoo, Sukwon and Yu, Lili and Zhilinsky, Ury and Zhou, Zhiyuan},
  date = {2025-11-18},
  eprint = {2511.14759},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2511.14759},
  url = {http://arxiv.org/abs/2511.14759},
  urldate = {2025-11-20},
  abstract = {We study how vision-language-action (VLA) models can improve through real-world deployments via reinforcement learning (RL). We present a general-purpose method, RL with Experience and Corrections via Advantage-conditioned Policies (RECAP), that provides for RL training of VLAs via advantage conditioning. Our method incorporates heterogeneous data into the self-improvement process, including demonstrations, data from on-policy collection, and expert teleoperated interventions provided during autonomous execution. RECAP starts by pre-training a generalist VLA with offline RL, which we call \$π\textasciicircum\{*\}\_\{0.6\}\$, that can then be specialized to attain high performance on downstream tasks through on-robot data collection. We show that the \$π\textasciicircum\{*\}\_\{0.6\}\$ model trained with the full RECAP method can fold laundry in real homes, reliably assemble boxes, and make espresso drinks using a professional espresso machine. On some of the hardest tasks, RECAP more than doubles task throughput and roughly halves the task failure rate.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/KBFMINEX/Amin et al. - 2025 - $π^ _ 0.6 $ a VLA That Learns From Experience.pdf}
}

@online{ayalewPROGRESSORPerceptuallyGuided2024,
  title = {{{PROGRESSOR}}: {{A Perceptually Guided Reward Estimator}} with {{Self-Supervised Online Refinement}}},
  shorttitle = {{{PROGRESSOR}}},
  author = {Ayalew, Tewodros and Zhang, Xiao and Wu, Kevin Yuanbo and Jiang, Tianchong and Maire, Michael and Walter, Matthew R.},
  date = {2024-11-26},
  eprint = {2411.17764},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2411.17764},
  url = {http://arxiv.org/abs/2411.17764},
  urldate = {2025-02-12},
  abstract = {We present PROGRESSOR, a novel framework that learns a task-agnostic reward function from videos, enabling policy training through goal-conditioned reinforcement learning (RL) without manual supervision. Underlying this reward is an estimate of the distribution over task progress as a function of the current, initial, and goal observations that is learned in a self-supervised fashion. Crucially, PROGRESSOR refines rewards adversarially during online RL training by pushing back predictions for out-of-distribution observations, to mitigate distribution shift inherent in non-expert observations. Utilizing this progress prediction as a dense reward together with an adversarial push-back, we show that PROGRESSOR enables robots to learn complex behaviors without any external supervision. Pretrained on large-scale egocentric human video from EPIC-KITCHENS, PROGRESSOR requires no fine-tuning on in-domain task-specific data for generalization to real-robot offline RL under noisy demonstrations, outperforming contemporary methods that provide dense visual reward for robotic learning. Our findings highlight the potential of PROGRESSOR for scalable robotic applications where direct action labels and task-specific rewards are not readily available.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/L8GHJ2AN/Ayalew et al. - 2024 - PROGRESSOR A Perceptually Guided Reward Estimator with Self-Supervised Online Refinement.pdf;/Users/kevinwu/Zotero/storage/AYN2DVJ2/2411.html}
}

@online{bajcsyLearningVisionbasedPursuitEvasion2023,
  title = {Learning {{Vision-based Pursuit-Evasion Robot Policies}}},
  author = {Bajcsy, Andrea and Loquercio, Antonio and Kumar, Ashish and Malik, Jitendra},
  date = {2023-08-30},
  eprint = {2308.16185},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.16185},
  url = {http://arxiv.org/abs/2308.16185},
  urldate = {2024-10-06},
  abstract = {Learning strategic robot behavior -- like that required in pursuit-evasion interactions -- under real-world constraints is extremely challenging. It requires exploiting the dynamics of the interaction, and planning through both physical state and latent intent uncertainty. In this paper, we transform this intractable problem into a supervised learning problem, where a fully-observable robot policy generates supervision for a partially-observable one. We find that the quality of the supervision signal for the partially-observable pursuer policy depends on two key factors: the balance of diversity and optimality of the evader's behavior and the strength of the modeling assumptions in the fully-observable policy. We deploy our policy on a physical quadruped robot with an RGB-D camera on pursuit-evasion interactions in the wild. Despite all the challenges, the sensing constraints bring about creativity: the robot is pushed to gather information when uncertain, predict intent from noisy measurements, and anticipate in order to intercept. Project webpage: https://abajcsy.github.io/vision-based-pursuit/},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/7GZK3ZCL/Bajcsy et al. - 2023 - Learning Vision-based Pursuit-Evasion Robot Policies.pdf;/Users/kevinwu/Zotero/storage/W4BANNFN/2308.html}
}

@online{belkhaleDataQualityImitation2023,
  title = {Data {{Quality}} in {{Imitation Learning}}},
  author = {Belkhale, Suneel and Cui, Yuchen and Sadigh, Dorsa},
  date = {2023-06-04},
  url = {https://arxiv.org/abs/2306.02437v1},
  urldate = {2024-09-27},
  abstract = {In supervised learning, the question of data quality and curation has been over-shadowed in recent years by increasingly more powerful and expressive models that can ingest internet-scale data. However, in offline learning for robotics, we simply lack internet scale data, and so high quality datasets are a necessity. This is especially true in imitation learning (IL), a sample efficient paradigm for robot learning using expert demonstrations. Policies learned through IL suffer from state distribution shift at test time due to compounding errors in action prediction, which leads to unseen states that the policy cannot recover from. Instead of designing new algorithms to address distribution shift, an alternative perspective is to develop new ways of assessing and curating datasets. There is growing evidence that the same IL algorithms can have substantially different performance across different datasets. This calls for a formalism for defining metrics of "data quality" that can further be leveraged for data curation. In this work, we take the first step toward formalizing data quality for imitation learning through the lens of distribution shift: a high quality dataset encourages the policy to stay in distribution at test time. We propose two fundamental properties that shape the quality of a dataset: i) action divergence: the mismatch between the expert and learned policy at certain states; and ii) transition diversity: the noise present in the system for a given state and action. We investigate the combined effect of these two key properties in imitation learning theoretically, and we empirically analyze models trained on a variety of different data sources. We show that state diversity is not always beneficial, and we demonstrate how action divergence and transition diversity interact in practice.},
  langid = {english},
  organization = {arXiv.org},
  file = {/Users/kevinwu/Zotero/storage/GCQHCPJ8/Belkhale et al. - 2023 - Data Quality in Imitation Learning.pdf}
}

@online{belkhaleHYDRAHybridRobot2023,
  title = {{{HYDRA}}: {{Hybrid Robot Actions}} for {{Imitation Learning}}},
  shorttitle = {{{HYDRA}}},
  author = {Belkhale, Suneel and Cui, Yuchen and Sadigh, Dorsa},
  date = {2023-06-29},
  url = {https://arxiv.org/abs/2306.17237v2},
  urldate = {2024-09-27},
  abstract = {Imitation Learning (IL) is a sample efficient paradigm for robot learning using expert demonstrations. However, policies learned through IL suffer from state distribution shift at test time, due to compounding errors in action prediction which lead to previously unseen states. Choosing an action representation for the policy that minimizes this distribution shift is critical in imitation learning. Prior work propose using temporal action abstractions to reduce compounding errors, but they often sacrifice policy dexterity or require domain-specific knowledge. To address these trade-offs, we introduce HYDRA, a method that leverages a hybrid action space with two levels of action abstractions: sparse high-level waypoints and dense low-level actions. HYDRA dynamically switches between action abstractions at test time to enable both coarse and fine-grained control of a robot. In addition, HYDRA employs action relabeling to increase the consistency of actions in the dataset, further reducing distribution shift. HYDRA outperforms prior imitation learning methods by 30-40\% on seven challenging simulation and real world environments, involving long-horizon tasks in the real world like making coffee and toasting bread. Videos are found on our website: https://tinyurl.com/3mc6793z},
  langid = {english},
  organization = {arXiv.org},
  file = {/Users/kevinwu/Zotero/storage/3NEWRFB8/Belkhale et al. - 2023 - HYDRA Hybrid Robot Actions for Imitation Learning.pdf}
}

@online{caronEmergingPropertiesSelfSupervised2021,
  title = {Emerging {{Properties}} in {{Self-Supervised Vision Transformers}}},
  author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  date = {2021-05-24},
  eprint = {2104.14294},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2104.14294},
  url = {http://arxiv.org/abs/2104.14294},
  urldate = {2024-10-15},
  abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/kevinwu/Zotero/storage/P5FCGE5U/Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Transformers.pdf;/Users/kevinwu/Zotero/storage/QUAHIMIC/2104.html}
}

@online{chenBigSelfSupervisedModels2020,
  title = {Big {{Self-Supervised Models}} Are {{Strong Semi-Supervised Learners}}},
  author = {Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey},
  date = {2020-10-26},
  eprint = {2006.10029},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2006.10029},
  url = {http://arxiv.org/abs/2006.10029},
  urldate = {2024-10-15},
  abstract = {One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\% ImageNet top-1 accuracy with just 1\% of the labels (\$\textbackslash le\$13 labeled images per class) using ResNet-50, a \$10\textbackslash times\$ improvement in label efficiency over the previous state-of-the-art. With 10\% of labels, ResNet-50 trained with our method achieves 77.5\% top-1 accuracy, outperforming standard supervised training with all of the labels.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/kevinwu/Zotero/storage/BNKGNNTI/Chen et al. - 2020 - Big Self-Supervised Models are Strong Semi-Supervised Learners.pdf}
}

@online{chenSARMStageAwareReward2025,
  title = {{{SARM}}: {{Stage-Aware Reward Modeling}} for {{Long Horizon Robot Manipulation}}},
  shorttitle = {{{SARM}}},
  author = {Chen, Qianzhong and Yu, Justin and Schwager, Mac and Abbeel, Pieter and Shentu, Fred and Wu, Philipp},
  date = {2025-10-02},
  eprint = {2509.25358},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2509.25358},
  url = {http://arxiv.org/abs/2509.25358},
  urldate = {2025-10-05},
  abstract = {Large-scale robot learning has recently shown promise for enabling robots to perform complex tasks by integrating perception, control, and language understanding. Yet, it struggles with long-horizon, contact-rich manipulation such as deformable object handling, where demonstration quality is inconsistent. Reward modeling offers a natural solution: by providing grounded progress signals, it transforms noisy demonstrations into stable supervision that generalizes across diverse trajectories. We introduce a stage-aware, video-based reward modeling framework that jointly predicts high-level task stages and fine-grained progress. Reward labels are automatically derived from natural language subtask annotations, ensuring consistent progress estimation across variable-length demonstrations. This design overcomes frame-index labeling, which fails in variable-duration tasks like folding a T-shirt. Our reward model demonstrates robustness to variability, generalization to out-of-distribution settings, and strong utility for policy training. Building on it, we propose Reward-Aligned Behavior Cloning (RA-BC), which filters high-quality data and reweights samples by reward. Experiments show the reward model alone outperforms baselines on validation and real robot rollouts. Integrated into RA-BC, our approach achieves 83\textbackslash\% success on folding T-shirts from the flattened state and 67\textbackslash\% from the crumpled state -- far surpassing vanilla behavior cloning, which attains only 8\textbackslash\% and 0\textbackslash\% success. Overall, our results highlight reward modeling as a key enabler for scalable, annotation-efficient, and robust imitation learning in long-horizon manipulation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/Q9RLKRCJ/Chen et al. - 2025 - SARM Stage-Aware Reward Modeling for Long Horizon Robot Manipulation.pdf;/Users/kevinwu/Zotero/storage/JT7V7G8C/2509.html}
}

@online{chenSimpleFrameworkContrastive2020,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  date = {2020-07-01},
  eprint = {2002.05709},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2002.05709},
  url = {http://arxiv.org/abs/2002.05709},
  urldate = {2024-10-15},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/kevinwu/Zotero/storage/HSEV7AJL/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Visual Representations.pdf;/Users/kevinwu/Zotero/storage/ZMA87PQU/2002.html}
}

@online{cuiDynaMoInDomainDynamics2024,
  title = {{{DynaMo}}: {{In-Domain Dynamics Pretraining}} for {{Visuo-Motor Control}}},
  shorttitle = {{{DynaMo}}},
  author = {Cui, Zichen Jeff and Pan, Hengkai and Iyer, Aadhithya and Haldar, Siddhant and Pinto, Lerrel},
  date = {2024-09-18},
  eprint = {2409.12192},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2409.12192},
  url = {http://arxiv.org/abs/2409.12192},
  urldate = {2024-10-20},
  abstract = {Imitation learning has proven to be a powerful tool for training complex visuomotor policies. However, current methods often require hundreds to thousands of expert demonstrations to handle high-dimensional visual observations. A key reason for this poor data efficiency is that visual representations are predominantly either pretrained on out-of-domain data or trained directly through a behavior cloning objective. In this work, we present DynaMo, a new in-domain, self-supervised method for learning visual representations. Given a set of expert demonstrations, we jointly learn a latent inverse dynamics model and a forward dynamics model over a sequence of image embeddings, predicting the next frame in latent space, without augmentations, contrastive sampling, or access to ground truth actions. Importantly, DynaMo does not require any out-of-domain data such as Internet datasets or cross-embodied datasets. On a suite of six simulated and real environments, we show that representations learned with DynaMo significantly improve downstream imitation learning performance over prior self-supervised learning objectives, and pretrained representations. Gains from using DynaMo hold across policy classes such as Behavior Transformer, Diffusion Policy, MLP, and nearest neighbors. Finally, we ablate over key components of DynaMo and measure its impact on downstream policy performance. Robot videos are best viewed at https://dynamo-ssl.github.io},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/ARQ2RZ67/Cui et al. - 2024 - DynaMo In-Domain Dynamics Pretraining for Visuo-Motor Control.pdf;/Users/kevinwu/Zotero/storage/CNKVFCX6/2409.html}
}

@online{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  eprint = {2010.11929},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.11929},
  url = {http://arxiv.org/abs/2010.11929},
  urldate = {2024-09-08},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/kevinwu/Zotero/storage/VGD5R35D/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf;/Users/kevinwu/Zotero/storage/3ICKKPZU/2010.html}
}

@online{douTactileAugmentedRadianceFields2024,
  title = {Tactile-{{Augmented Radiance Fields}}},
  author = {Dou, Yiming and Yang, Fengyu and Liu, Yi and Loquercio, Antonio and Owens, Andrew},
  date = {2024-05-07},
  eprint = {2405.04534},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2405.04534},
  url = {http://arxiv.org/abs/2405.04534},
  urldate = {2024-10-12},
  abstract = {We present a scene representation, which we call a tactile-augmented radiance field (TaRF), that brings vision and touch into a shared 3D space. This representation can be used to estimate the visual and tactile signals for a given 3D position within a scene. We capture a scene's TaRF from a collection of photos and sparsely sampled touch probes. Our approach makes use of two insights: (i) common vision-based touch sensors are built on ordinary cameras and thus can be registered to images using methods from multi-view geometry, and (ii) visually and structurally similar regions of a scene share the same tactile features. We use these insights to register touch signals to a captured visual scene, and to train a conditional diffusion model that, provided with an RGB-D image rendered from a neural radiance field, generates its corresponding tactile signal. To evaluate our approach, we collect a dataset of TaRFs. This dataset contains more touch samples than previous real-world datasets, and it provides spatially aligned visual signals for each captured touch signal. We demonstrate the accuracy of our cross-modal generative model and the utility of the captured visual-tactile data on several downstream tasks. Project page: https://dou-yiming.github.io/TaRF},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/kevinwu/Zotero/storage/T5H7VHTX/Dou et al. - 2024 - Tactile-Augmented Radiance Fields.pdf;/Users/kevinwu/Zotero/storage/MUAPQGJJ/2405.html}
}

@online{droletComparisonImitationLearning2024,
  title = {A {{Comparison}} of {{Imitation Learning Algorithms}} for {{Bimanual Manipulation}}},
  author = {Drolet, Michael and Stepputtis, Simon and Kailas, Siva and Jain, Ajinkya and Peters, Jan and Schaal, Stefan and Amor, Heni Ben},
  date = {2024-08-24},
  eprint = {2408.06536},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2408.06536},
  url = {http://arxiv.org/abs/2408.06536},
  urldate = {2024-09-26},
  abstract = {Amidst the wide popularity of imitation learning algorithms in robotics, their properties regarding hyperparameter sensitivity, ease of training, data efficiency, and performance have not been well-studied in high-precision industry-inspired environments. In this work, we demonstrate the limitations and benefits of prominent imitation learning approaches and analyze their capabilities regarding these properties. We evaluate each algorithm on a complex bimanual manipulation task involving an over-constrained dynamics system in a setting involving multiple contacts between the manipulated object and the environment. While we find that imitation learning is well suited to solve such complex tasks, not all algorithms are equal in terms of handling environmental and hyperparameter perturbations, training requirements, performance, and ease of use. We investigate the empirical influence of these key characteristics by employing a carefully designed experimental procedure and learning environment. Paper website: https://bimanual-imitation.github.io/},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/VZSGGXCK/Drolet et al. - 2024 - A Comparison of Imitation Learning Algorithms for Bimanual Manipulation.pdf;/Users/kevinwu/Zotero/storage/Z9HWRJQ2/2408.html}
}

@article{gallierLinearAlgebraComputer,
  title = {Linear {{Algebra}} for {{Computer Vision}}, {{Robotics}}, and {{Machine Learning}}},
  author = {Gallier, Jean and Quaintance, Jocelyn},
  langid = {english},
  file = {/Users/kevinwu/Zotero/storage/8VKFF2J7/Gallier and Quaintance - Linear Algebra for Computer Vision, Robotics, and Machine Learning.pdf}
}

@online{geistLearning3DRotations2024,
  title = {Learning with {{3D}} Rotations, a Hitchhiker's Guide to {{SO}}(3)},
  author = {Geist, A. René and Frey, Jonas and Zhobro, Mikel and Levina, Anna and Martius, Georg},
  date = {2024-06-19},
  eprint = {2404.11735},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.11735},
  url = {http://arxiv.org/abs/2404.11735},
  urldate = {2025-10-01},
  abstract = {Many settings in machine learning require the selection of a rotation representation. However, choosing a suitable representation from the many available options is challenging. This paper acts as a survey and guide through rotation representations. We walk through their properties that harm or benefit deep learning with gradient-based optimization. By consolidating insights from rotation-based learning, we provide a comprehensive overview of learning functions with rotation representations. We provide guidance on selecting representations based on whether rotations are in the model's input or output and whether the data primarily comprises small angles.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/3ZWUSA3P/Geist et al. - 2024 - Learning with 3D rotations, a hitchhiker's guide to SO(3).pdf;/Users/kevinwu/Zotero/storage/N6P6FXUE/2404.html}
}

@online{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014-06-10},
  eprint = {1406.2661},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1406.2661},
  url = {http://arxiv.org/abs/1406.2661},
  urldate = {2024-09-13},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/kevinwu/Zotero/storage/5B2KKTAJ/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf;/Users/kevinwu/Zotero/storage/S2PGWMDR/1406.html}
}

@article{hanoverAutonomousDroneRacing2024,
  title = {Autonomous {{Drone Racing}}: {{A Survey}}},
  shorttitle = {Autonomous {{Drone Racing}}},
  author = {Hanover, Drew and Loquercio, Antonio and Bauersfeld, Leonard and Romero, Angel and Penicka, Robert and Song, Yunlong and Cioffi, Giovanni and Kaufmann, Elia and Scaramuzza, Davide},
  date = {2024},
  journaltitle = {IEEE Transactions on Robotics},
  shortjournal = {IEEE Trans. Robot.},
  volume = {40},
  eprint = {2301.01755},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {3044--3067},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2024.3400838},
  url = {http://arxiv.org/abs/2301.01755},
  urldate = {2024-10-05},
  abstract = {Over the last decade, the use of autonomous drone systems for surveying, search and rescue, or last-mile delivery has increased exponentially. With the rise of these applications comes the need for highly robust, safety-critical algorithms which can operate drones in complex and uncertain environments. Additionally, flying fast enables drones to cover more ground which in turn increases productivity and further strengthens their use case. One proxy for developing algorithms used in high-speed navigation is the task of autonomous drone racing, where researchers program drones to fly through a sequence of gates and avoid obstacles as quickly as possible using onboard sensors and limited computational power. Speeds and accelerations exceed over 80 kph and 4 g respectively, raising significant challenges across perception, planning, control, and state estimation. To achieve maximum performance, systems require real-time algorithms that are robust to motion blur, high dynamic range, model uncertainties, aerodynamic disturbances, and often unpredictable opponents. This survey covers the progression of autonomous drone racing across model-based and learning-based approaches. We provide an overview of the field, its evolution over the years, and conclude with the biggest challenges and open questions to be faced in the future.},
  keywords = {Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/FP2IIEZB/Hanover et al. - 2024 - Autonomous Drone Racing A Survey.pdf;/Users/kevinwu/Zotero/storage/5ZUR2EUY/2301.html}
}

@online{heMaskedAutoencodersAre2021,
  title = {Masked {{Autoencoders Are Scalable Vision Learners}}},
  author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
  date = {2021-12-19},
  eprint = {2111.06377},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2111.06377},
  url = {http://arxiv.org/abs/2111.06377},
  urldate = {2024-10-15},
  abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/kevinwu/Zotero/storage/C5X2WHGJ/He et al. - 2021 - Masked Autoencoders Are Scalable Vision Learners.pdf;/Users/kevinwu/Zotero/storage/K42M8CHG/2111.html}
}

@online{heMomentumContrastUnsupervised2020,
  title = {Momentum {{Contrast}} for {{Unsupervised Visual Representation Learning}}},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  date = {2020-03-23},
  eprint = {1911.05722},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1911.05722},
  urldate = {2024-10-15},
  abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning [29] as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/kevinwu/Zotero/storage/S5YVJUVL/He et al. - 2020 - Momentum Contrast for Unsupervised Visual Representation Learning.pdf}
}

@online{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  date = {2020-12-16},
  eprint = {2006.11239},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2006.11239},
  url = {http://arxiv.org/abs/2006.11239},
  urldate = {2025-09-23},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/kevinwu/Zotero/storage/UBILNIBX/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf;/Users/kevinwu/Zotero/storage/2G6CMR4Y/2006.html}
}

@online{huangSpatiallyAnchoredTactile2025,
  title = {Spatially Anchored {{Tactile Awareness}} for {{Robust Dexterous Manipulation}}},
  author = {Huang, Jialei and Ye, Yang and Gong, Yuanqing and Zhu, Xuezhou and Gao, Yang and Zhang, Kaifeng},
  date = {2025-10-16},
  eprint = {2510.14647},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2510.14647},
  url = {http://arxiv.org/abs/2510.14647},
  urldate = {2025-11-09},
  abstract = {Dexterous manipulation requires precise geometric reasoning, yet existing visuo-tactile learning methods struggle with sub-millimeter precision tasks that are routine for traditional model-based approaches. We identify a key limitation: while tactile sensors provide rich contact information, current learning frameworks fail to effectively leverage both the perceptual richness of tactile signals and their spatial relationship with hand kinematics. We believe an ideal tactile representation should explicitly ground contact measurements in a stable reference frame while preserving detailed sensory information, enabling policies to not only detect contact occurrence but also precisely infer object geometry in the hand's coordinate system. We introduce SaTA (Spatially-anchored Tactile Awareness for dexterous manipulation), an end-to-end policy framework that explicitly anchors tactile features to the hand's kinematic frame through forward kinematics, enabling accurate geometric reasoning without requiring object models or explicit pose estimation. Our key insight is that spatially grounded tactile representations allow policies to not only detect contact occurrence but also precisely infer object geometry in the hand's coordinate system. We validate SaTA on challenging dexterous manipulation tasks, including bimanual USB-C mating in free space, a task demanding sub-millimeter alignment precision, as well as light bulb installation requiring precise thread engagement and rotational control, and card sliding that demands delicate force modulation and angular precision. These tasks represent significant challenges for learning-based methods due to their stringent precision requirements. Across multiple benchmarks, SaTA significantly outperforms strong visuo-tactile baselines, improving success rates by up to 30 percentage while reducing task completion times by 27 percentage.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/GXKXJMVD/Huang et al. - 2025 - Spatially anchored Tactile Awareness for Robust Dexterous Manipulation.pdf;/Users/kevinwu/Zotero/storage/53HFBPEI/2510.html}
}

@online{huLoRALowRankAdaptation2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  date = {2021-10-16},
  eprint = {2106.09685},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.09685},
  url = {http://arxiv.org/abs/2106.09685},
  urldate = {2025-09-11},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/kevinwu/Zotero/storage/ILDX4VCD/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf;/Users/kevinwu/Zotero/storage/Z3R72D4X/2106.html}
}

@article{kawaharazukaRealWorldRobotApplications2024,
  title = {Real-{{World Robot Applications}} of {{Foundation Models}}: {{A Review}}},
  shorttitle = {Real-{{World Robot Applications}} of {{Foundation Models}}},
  author = {Kawaharazuka, Kento and Matsushima, Tatsuya and Gambardella, Andrew and Guo, Jiaxian and Paxton, Chris and Zeng, Andy},
  date = {2024-09-16},
  journaltitle = {Advanced Robotics},
  shortjournal = {Advanced Robotics},
  volume = {38},
  number = {18},
  eprint = {2402.05741},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1232--1254},
  issn = {0169-1864, 1568-5535},
  doi = {10.1080/01691864.2024.2408593},
  url = {http://arxiv.org/abs/2402.05741},
  urldate = {2025-09-05},
  abstract = {Recent developments in foundation models, like Large Language Models (LLMs) and Vision-Language Models (VLMs), trained on extensive data, facilitate flexible application across different tasks and modalities. Their impact spans various fields, including healthcare, education, and robotics. This paper provides an overview of the practical application of foundation models in real-world robotics, with a primary emphasis on the replacement of specific components within existing robot systems. The summary encompasses the perspective of input-output relationships in foundation models, as well as their role in perception, motion planning, and control within the field of robotics. This paper concludes with a discussion of future challenges and implications for practical robot applications.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/Z684M4ES/Kawaharazuka et al. - 2024 - Real-World Robot Applications of Foundation Models A Review.pdf;/Users/kevinwu/Zotero/storage/ZIAN69CQ/2402.html}
}

@online{kingmaAutoEncodingVariationalBayes2022,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2022-12-10},
  eprint = {1312.6114},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1312.6114},
  url = {http://arxiv.org/abs/1312.6114},
  urldate = {2024-09-13},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/kevinwu/Zotero/storage/GI8KSC2Z/Kingma and Welling - 2022 - Auto-Encoding Variational Bayes.pdf;/Users/kevinwu/Zotero/storage/QPD4623E/1312.html}
}

@inproceedings{knoxInteractivelyShapingAgents2009,
  title = {Interactively Shaping Agents via Human Reinforcement: The {{TAMER}} Framework},
  shorttitle = {Interactively Shaping Agents via Human Reinforcement},
  booktitle = {Proceedings of the Fifth International Conference on {{Knowledge}} Capture},
  author = {Knox, W. Bradley and Stone, Peter},
  date = {2009-09},
  pages = {9--16},
  publisher = {ACM},
  location = {Redondo Beach California USA},
  doi = {10.1145/1597735.1597738},
  url = {https://dl.acm.org/doi/10.1145/1597735.1597738},
  urldate = {2025-02-05},
  abstract = {As computational learning agents move into domains that incur real costs (e.g., autonomous driving or financial investment), it will be necessary to learn good policies without numerous high-cost learning trials. One promising approach to reducing sample complexity of learning a task is knowledge transfer from humans to agents. Ideally, methods of transfer should be accessible to anyone with task knowledge, regardless of that person’s expertise in programming and AI. This paper focuses on allowing a human trainer to interactively shape an agent’s policy via reinforcement signals. Specifically, the paper introduces “Training an Agent Manually via Evaluative Reinforcement,” or tamer, a framework that enables such shaping. Differing from previous approaches to interactive shaping, a tamer agent models the human’s reinforcement and exploits its model by choosing actions expected to be most highly reinforced. Results from two domains demonstrate that lay users can train tamer agents without defining an environmental reward function (as in an MDP) and indicate that human training within the tamer framework can reduce sample complexity over autonomous learning algorithms.},
  eventtitle = {K-{{CAP}} '09: {{Fifth International Conference}} on {{Knowledge Capture}} 2009},
  isbn = {978-1-60558-658-8},
  langid = {english},
  file = {/Users/kevinwu/Zotero/storage/9WYYA8Q5/Knox and Stone - 2009 - Interactively shaping agents via human reinforcement the TAMER framework.pdf}
}

@online{kongAriaGen22025,
  title = {Aria {{Gen}} 2 {{Pilot Dataset}}},
  author = {Kong, Chen and Fort, James and Kang, Aria and Wittmer, Jonathan and Green, Simon and Shen, Tianwei and Zhao, Yipu and Peng, Cheng and Solaira, Gustavo and Berkovich, Andrew and Raina, Nikhil and Baiyya, Vijay and Oleinik, Evgeniy and Huang, Eric and Zhang, Fan and Straub, Julian and Schwesinger, Mark and Pesqueira, Luis and Pan, Xiaqing and Engel, Jakob Julian and Ren, Carl and Yan, Mingfei and Newcombe, Richard},
  date = {2025-10-17},
  eprint = {2510.16134},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2510.16134},
  url = {http://arxiv.org/abs/2510.16134},
  urldate = {2025-11-01},
  abstract = {The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset captured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely access, A2PD is released incrementally with ongoing dataset enhancements. The initial release features Dia'ane, our primary subject, who records her daily activities alongside friends, each equipped with Aria Gen 2 glasses. It encompasses five primary scenarios: cleaning, cooking, eating, playing, and outdoor walking. In each of the scenarios, we provide comprehensive raw sensor data and output data from various machine perception algorithms. These data illustrate the device's ability to perceive the wearer, the surrounding environment, and interactions between the wearer and the environment, while maintaining robust performance across diverse users and conditions. The A2PD is publicly available at projectaria.com, with open-source tools and usage examples provided in Project Aria Tools.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/2LN52D6J/Kong et al. - 2025 - Aria Gen 2 Pilot Dataset.pdf;/Users/kevinwu/Zotero/storage/23DQG7YQ/2510.html}
}

@online{labiosaReinforcementLearningClassical2025,
  title = {Reinforcement {{Learning Within}} the {{Classical Robotics Stack}}: {{A Case Study}} in {{Robot Soccer}}},
  shorttitle = {Reinforcement {{Learning Within}} the {{Classical Robotics Stack}}},
  author = {Labiosa, Adam and Wang, Zhihan and Agarwal, Siddhant and Cong, William and Hemkumar, Geethika and Harish, Abhinav Narayan and Hong, Benjamin and Kelle, Josh and Li, Chen and Li, Yuhao and Shao, Zisen and Stone, Peter and Hanna, Josiah P.},
  date = {2025-03-07},
  eprint = {2412.09417},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2412.09417},
  url = {http://arxiv.org/abs/2412.09417},
  urldate = {2025-04-03},
  abstract = {Robot decision-making in partially observable, real-time, dynamic, and multi-agent environments remains a difficult and unsolved challenge. Model-free reinforcement learning (RL) is a promising approach to learning decision-making in such domains, however, end-to-end RL in complex environments is often intractable. To address this challenge in the RoboCup Standard Platform League (SPL) domain, we developed a novel architecture integrating RL within a classical robotics stack, while employing a multi-fidelity sim2real approach and decomposing behavior into learned sub-behaviors with heuristic selection. Our architecture led to victory in the 2024 RoboCup SPL Challenge Shield Division. In this work, we fully describe our system's architecture and empirically analyze key design decisions that contributed to its success. Our approach demonstrates how RL-based behaviors can be integrated into complete robot behavior architectures.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/UCIRCPC4/Labiosa et al. - 2025 - Reinforcement Learning Within the Classical Robotics Stack A Case Study in Robot Soccer.pdf;/Users/kevinwu/Zotero/storage/Q6HGEM5T/2412.html}
}

@online{liangMakeAnAgentGeneralizablePolicy2024,
  title = {Make-{{An-Agent}}: {{A Generalizable Policy Network Generator}} with {{Behavior-Prompted Diffusion}}},
  shorttitle = {Make-{{An-Agent}}},
  author = {Liang, Yongyuan and Xu, Tingqiang and Hu, Kaizhe and Jiang, Guangqi and Huang, Furong and Xu, Huazhe},
  date = {2024-11-04},
  eprint = {2407.10973},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2407.10973},
  url = {http://arxiv.org/abs/2407.10973},
  urldate = {2025-02-17},
  abstract = {Can we generate a control policy for an agent using just one demonstration of desired behaviors as a prompt, as effortlessly as creating an image from a textual description? In this paper, we present Make-An-Agent, a novel policy parameter generator that leverages the power of conditional diffusion models for behavior-to-policy generation. Guided by behavior embeddings that encode trajectory information, our policy generator synthesizes latent parameter representations, which can then be decoded into policy networks. Trained on policy network checkpoints and their corresponding trajectories, our generation model demonstrates remarkable versatility and scalability on multiple tasks and has a strong generalization ability on unseen tasks to output well-performed policies with only few-shot demonstrations as inputs. We showcase its efficacy and efficiency on various domains and tasks, including varying objectives, behaviors, and even across different robot manipulators. Beyond simulation, we directly deploy policies generated by Make-An-Agent onto real-world robots on locomotion tasks. Project page: https://cheryyunl.github.io/make-an-agent/},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/kevinwu/Zotero/storage/P4NEYQ9B/Liang et al. - 2024 - Make-An-Agent A Generalizable Policy Network Generator with Behavior-Prompted Diffusion.pdf;/Users/kevinwu/Zotero/storage/J57753PV/2407.html}
}

@online{LifelongRobotLearning,
  title = {Lifelong {{Robot Learning}}},
  url = {https://www.ri.cmu.edu/publications/lifelong-robot-learning/},
  urldate = {2025-09-09},
  abstract = {Learning provides a useful tool for the automatic design of autonomous robots. Recent research on learning robot control has predominantly focused on learning single tasks that were studied in isolation. If robots encounter a multitude of control learning tasks over their entire lifetime there is an opportunity to transfer knowledge between them. In order to […]},
  langid = {american},
  organization = {Robotics Institute Carnegie Mellon University},
  file = {/Users/kevinwu/Zotero/storage/APVGM7KX/PDF.pdf;/Users/kevinwu/Zotero/storage/PAZZG8SA/lifelong-robot-learning.html}
}

@online{lipmanFlowMatchingGenerative2023,
  title = {Flow {{Matching}} for {{Generative Modeling}}},
  author = {Lipman, Yaron and Chen, Ricky T. Q. and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
  date = {2023-02-08},
  eprint = {2210.02747},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.02747},
  url = {http://arxiv.org/abs/2210.02747},
  urldate = {2025-09-05},
  abstract = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/kevinwu/Zotero/storage/8K82YELN/Lipman et al. - 2023 - Flow Matching for Generative Modeling.pdf;/Users/kevinwu/Zotero/storage/TCZV7C7K/2210.html}
}

@online{minS2M2ScalableStereo2025,
  title = {{{S2M2}}: {{Scalable Stereo Matching Model}} for {{Reliable Depth Estimation}}},
  shorttitle = {\{{{S}}\textbackslash textsuperscript\{2\}{{M}}\textbackslash textsuperscript\{2\}\}},
  author = {Min, Junhong and Jeon, Youngpil and Kim, Jimin and Choi, Minyong},
  date = {2025-10-11},
  eprint = {2507.13229},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2507.13229},
  url = {http://arxiv.org/abs/2507.13229},
  urldate = {2025-11-20},
  abstract = {The pursuit of a generalizable stereo matching model, capable of performing well across varying resolutions and disparity ranges without dataset-specific fine-tuning, has revealed a fundamental trade-off. Iterative local search methods achieve high scores on constrained benchmarks, but their core mechanism inherently limits the global consistency required for true generalization. However, global matching architectures, while theoretically more robust, have historically been rendered infeasible by prohibitive computational and memory costs. We resolve this dilemma with \{S\textbackslash textsuperscript\{2\}M\textbackslash textsuperscript\{2\}\}: a global matching architecture that achieves state-of-the-art accuracy and high efficiency without relying on cost volume filtering or deep refinement stacks. Our design integrates a multi-resolution transformer for robust long-range correspondence, trained with a novel loss function that concentrates probability on feasible matches. This approach enables a more robust joint estimation of disparity, occlusion, and confidence. \{S\textbackslash textsuperscript\{2\}M\textbackslash textsuperscript\{2\}\} establishes a new state of the art on Middlebury v3 and ETH3D benchmarks, significantly outperforming prior methods in most metrics while reconstructing high-quality details with competitive efficiency.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/VBMGC7N7/Min et al. - 2025 - Stextsuperscript 2 Mtextsuperscript 2 Scalable Stereo Matching Model for Reliable Depth Estima.pdf;/Users/kevinwu/Zotero/storage/TTBK5KZB/2507.html}
}

@online{mnihPlayingAtariDeep2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  date = {2013-12-19},
  eprint = {1312.5602},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1312.5602},
  url = {http://arxiv.org/abs/1312.5602},
  urldate = {2024-10-15},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/kevinwu/Zotero/storage/83UPZ5AI/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf;/Users/kevinwu/Zotero/storage/V3P48HSS/1312.html}
}

@online{panSPIDERScalablePhysicsInformed2025,
  title = {{{SPIDER}}: {{Scalable Physics-Informed Dexterous Retargeting}}},
  shorttitle = {{{SPIDER}}},
  author = {Pan, Chaoyi and Wang, Changhao and Qi, Haozhi and Liu, Zixi and Bharadhwaj, Homanga and Sharma, Akash and Wu, Tingfan and Shi, Guanya and Malik, Jitendra and Hogan, Francois},
  date = {2025-11-12},
  eprint = {2511.09484},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2511.09484},
  url = {http://arxiv.org/abs/2511.09484},
  urldate = {2025-11-19},
  abstract = {Learning dexterous and agile policy for humanoid and dexterous hand control requires large-scale demonstrations, but collecting robot-specific data is prohibitively expensive. In contrast, abundant human motion data is readily available from motion capture, videos, and virtual reality, which could help address the data scarcity problem. However, due to the embodiment gap and missing dynamic information like force and torque, these demonstrations cannot be directly executed on robots. To bridge this gap, we propose Scalable Physics-Informed DExterous Retargeting (SPIDER), a physics-based retargeting framework to transform and augment kinematic-only human demonstrations to dynamically feasible robot trajectories at scale. Our key insight is that human demonstrations should provide global task structure and objective, while large-scale physics-based sampling with curriculum-style virtual contact guidance should refine trajectories to ensure dynamical feasibility and correct contact sequences. SPIDER scales across diverse 9 humanoid/dexterous hand embodiments and 6 datasets, improving success rates by 18\% compared to standard sampling, while being 10X faster than reinforcement learning (RL) baselines, and enabling the generation of a 2.4M frames dynamic-feasible robot dataset for policy learning. As a universal physics-based retargeting method, SPIDER can work with diverse quality data and generate diverse and high-quality data to enable efficient policy learning with methods like RL.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/FE9GW63D/Pan et al. - 2025 - SPIDER Scalable Physics-Informed Dexterous Retargeting.pdf;/Users/kevinwu/Zotero/storage/HQ6M22GI/2511.html}
}

@online{qiControlorientedClusteringVisual2024,
  title = {Control-Oriented {{Clustering}} of {{Visual Latent Representation}}},
  author = {Qi, Han and Yin, Haocheng and Yang, Heng},
  date = {2024-10-08},
  eprint = {2410.05063},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2410.05063},
  url = {http://arxiv.org/abs/2410.05063},
  urldate = {2024-10-20},
  abstract = {We initiate a study of the geometry of the visual representation space -- the information channel from the vision encoder to the action decoder -- in an image-based control pipeline learned from behavior cloning. Inspired by the phenomenon of neural collapse (NC) in image classification, we investigate whether a similar law of clustering emerges in the visual representation space. Since image-based control is a regression task without explicitly defined classes, the central piece of the puzzle lies in determining according to what implicit classes the visual features cluster, if such a law exists. Focusing on image-based planar pushing, we posit the most important role of the visual representation in a control task is to convey a goal to the action decoder. We then classify training samples of expert demonstrations into eight "control-oriented" classes based on (a) the relative pose between the object and the target in the input or (b) the relative pose of the object induced by expert actions in the output, where one class corresponds to one relative pose orthant (REPO). Across four different instantiations of architecture, we report the prevalent emergence of control-oriented clustering in the visual representation space according to the eight REPOs. Beyond empirical observation, we show such a law of clustering can be leveraged as an algorithmic tool to improve test-time performance when training a policy with limited expert demonstrations. Particularly, we pretrain the vision encoder using NC as a regularization to encourage control-oriented clustering of the visual features. Surprisingly, such an NC-pretrained vision encoder, when finetuned end-to-end with the action decoder, boosts the test-time performance by 10\% to 35\% in the low-data regime. Real-world vision-based planar pushing experiments confirmed the surprising advantage of control-oriented visual representation pretraining.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/DIANHNAX/Qi et al. - 2024 - Control-oriented Clustering of Visual Latent Representation.pdf;/Users/kevinwu/Zotero/storage/GUVSRWKF/2410.html}
}

@article{romeroEmbodiedHandsModeling2017,
  title = {Embodied {{Hands}}: {{Modeling}} and {{Capturing Hands}} and {{Bodies Together}}},
  shorttitle = {Embodied {{Hands}}},
  author = {Romero, Javier and Tzionas, Dimitrios and Black, Michael J.},
  date = {2017-12-31},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {36},
  number = {6},
  eprint = {2201.02610},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1--17},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3130800.3130883},
  url = {http://arxiv.org/abs/2201.02610},
  urldate = {2025-09-16},
  abstract = {Humans move their hands and bodies together to communicate and solve tasks. Capturing and replicating such coordinated activity is critical for virtual characters that behave realistically. Surprisingly, most methods treat the 3D modeling and tracking of bodies and hands separately. Here we formulate a model of hands and bodies interacting together and fit it to full-body 4D sequences. When scanning or capturing the full body in 3D, hands are small and often partially occluded, making their shape and pose hard to recover. To cope with low-resolution, occlusion, and noise, we develop a new model called MANO (hand Model with Articulated and Non-rigid defOrmations). MANO is learned from around 1000 high-resolution 3D scans of hands of 31 subjects in a wide variety of hand poses. The model is realistic, low-dimensional, captures non-rigid shape changes with pose, is compatible with standard graphics packages, and can fit any human hand. MANO provides a compact mapping from hand poses to pose blend shape corrections and a linear manifold of pose synergies. We attach MANO to a standard parameterized 3D body shape model (SMPL), resulting in a fully articulated body and hand model (SMPL+H). We illustrate SMPL+H by fitting complex, natural, activities of subjects captured with a 4D scanner. The fitting is fully automatic and results in full body models that move naturally with detailed hand motions and a realism not seen before in full body performance capture. The models and data are freely available for research purposes in our website (http://mano.is.tue.mpg.de).},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {/Users/kevinwu/Zotero/storage/U49S2KN3/Romero et al. - 2017 - Embodied Hands Modeling and Capturing Hands and Bodies Together.pdf;/Users/kevinwu/Zotero/storage/BFVGDPW4/2201.html}
}

@article{rosenblattPerceptronProbabilisticModel1958,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain.},
  shorttitle = {The Perceptron},
  author = {Rosenblatt, F.},
  date = {1958},
  journaltitle = {Psychological Review},
  shortjournal = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/h0042519},
  url = {https://doi.apa.org/doi/10.1037/h0042519},
  urldate = {2025-09-09},
  langid = {english},
  file = {/Users/kevinwu/Zotero/storage/5MTB2CDU/Rosenblatt - 1958 - The perceptron A probabilistic model for information storage and organization in the brain..pdf}
}

@online{shahBUMBLEUnifyingReasoning2024,
  title = {{{BUMBLE}}: {{Unifying Reasoning}} and {{Acting}} with {{Vision-Language Models}} for {{Building-wide Mobile Manipulation}}},
  shorttitle = {{{BUMBLE}}},
  author = {Shah, Rutav and Yu, Albert and Zhu, Yifeng and Zhu, Yuke and Martín-Martín, Roberto},
  date = {2024-10-08},
  eprint = {2410.06237},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2410.06237},
  url = {http://arxiv.org/abs/2410.06237},
  urldate = {2025-04-03},
  abstract = {To operate at a building scale, service robots must perform very long-horizon mobile manipulation tasks by navigating to different rooms, accessing different floors, and interacting with a wide and unseen range of everyday objects. We refer to these tasks as Building-wide Mobile Manipulation. To tackle these inherently long-horizon tasks, we introduce BUMBLE, a unified Vision-Language Model (VLM)-based framework integrating open-world RGBD perception, a wide spectrum of gross-to-fine motor skills, and dual-layered memory. Our extensive evaluation (90+ hours) indicates that BUMBLE outperforms multiple baselines in long-horizon building-wide tasks that require sequencing up to 12 ground truth skills spanning 15 minutes per trial. BUMBLE achieves 47.1\% success rate averaged over 70 trials in different buildings, tasks, and scene layouts from different starting rooms and floors. Our user study demonstrates 22\% higher satisfaction with our method than state-of-the-art mobile manipulation methods. Finally, we demonstrate the potential of using increasingly-capable foundation models to push performance further. For more information, see https://robin-lab.cs.utexas.edu/BUMBLE/},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/69X3LN8T/Shah et al. - 2024 - BUMBLE Unifying Reasoning and Acting with Vision-Language Models for Building-wide Mobile Manipulat.pdf;/Users/kevinwu/Zotero/storage/7KQRCG3Z/2410.html}
}

@online{shahMimicDroidInContextLearning2025,
  title = {{{MimicDroid}}: {{In-Context Learning}} for {{Humanoid Robot Manipulation}} from {{Human Play Videos}}},
  shorttitle = {{{MimicDroid}}},
  author = {Shah, Rutav and Liu, Shuijing and Wang, Qi and Jiang, Zhenyu and Kumar, Sateesh and Seo, Mingyo and Martín-Martín, Roberto and Zhu, Yuke},
  date = {2025-09-11},
  eprint = {2509.09769},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2509.09769},
  url = {http://arxiv.org/abs/2509.09769},
  urldate = {2025-09-17},
  abstract = {We aim to enable humanoid robots to efficiently solve new manipulation tasks from a few video examples. In-context learning (ICL) is a promising framework for achieving this goal due to its test-time data efficiency and rapid adaptability. However, current ICL methods rely on labor-intensive teleoperated data for training, which restricts scalability. We propose using human play videos -- continuous, unlabeled videos of people interacting freely with their environment -- as a scalable and diverse training data source. We introduce MimicDroid, which enables humanoids to perform ICL using human play videos as the only training data. MimicDroid extracts trajectory pairs with similar manipulation behaviors and trains the policy to predict the actions of one trajectory conditioned on the other. Through this process, the model acquired ICL capabilities for adapting to novel objects and environments at test time. To bridge the embodiment gap, MimicDroid first retargets human wrist poses estimated from RGB videos to the humanoid, leveraging kinematic similarity. It also applies random patch masking during training to reduce overfitting to human-specific cues and improve robustness to visual differences. To evaluate few-shot learning for humanoids, we introduce an open-source simulation benchmark with increasing levels of generalization difficulty. MimicDroid outperformed state-of-the-art methods and achieved nearly twofold higher success rates in the real world. Additional materials can be found on: ut-austin-rpl.github.io/MimicDroid},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/EVQPMTAK/Shah et al. - 2025 - MimicDroid In-Context Learning for Humanoid Robot Manipulation from Human Play Videos.pdf;/Users/kevinwu/Zotero/storage/9E3U7SNH/2509.html}
}

@online{singhHandObjectInteractionPretraining2024,
  title = {Hand-{{Object Interaction Pretraining}} from {{Videos}}},
  author = {Singh, Himanshu Gaurav and Loquercio, Antonio and Sferrazza, Carmelo and Wu, Jane and Qi, Haozhi and Abbeel, Pieter and Malik, Jitendra},
  date = {2024-09-12},
  eprint = {2409.08273},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2409.08273},
  url = {http://arxiv.org/abs/2409.08273},
  urldate = {2024-10-12},
  abstract = {We present an approach to learn general robot manipulation priors from 3D hand-object interaction trajectories. We build a framework to use in-the-wild videos to generate sensorimotor robot trajectories. We do so by lifting both the human hand and the manipulated object in a shared 3D space and retargeting human motions to robot actions. Generative modeling on this data gives us a task-agnostic base policy. This policy captures a general yet flexible manipulation prior. We empirically demonstrate that finetuning this policy, with both reinforcement learning (RL) and behavior cloning (BC), enables sample-efficient adaptation to downstream tasks and simultaneously improves robustness and generalizability compared to prior approaches. Qualitative experiments are available at: \textbackslash url\{https://hgaurav2k.github.io/hop/\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/S68QE7NX/Singh et al. - 2024 - Hand-Object Interaction Pretraining from Videos.pdf;/Users/kevinwu/Zotero/storage/MX65L2IN/2409.html}
}

@inproceedings{sohnLearningStructuredOutput2015,
  title = {Learning {{Structured Output Representation}} Using {{Deep Conditional Generative Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
  date = {2015},
  volume = {28},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2015/hash/8d55a249e6baa5c06772297520da2051-Abstract.html},
  urldate = {2024-09-13},
  abstract = {Supervised deep learning has been successfully applied for many recognition problems in machine learning and computer vision. Although it can approximate a complex many-to-one function very well when large number of training data is provided, the lack of probabilistic inference of the current supervised deep learning methods makes it difficult to model a complex structured output representations. In this work, we develop a scalable deep conditional generative model for structured output variables using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient variational Bayes, and allows a fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build a robust structured prediction algorithms, such as recurrent prediction network architecture, input noise-injection and multi-scale prediction training methods. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic output representations using stochastic inference. Furthermore, the proposed schemes in training methods and architecture design were complimentary, which leads to achieve strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
  file = {/Users/kevinwu/Zotero/storage/A2GM32KN/Sohn et al. - 2015 - Learning Structured Output Representation using Deep Conditional Generative Models.pdf}
}

@article{spongRobotDynamicsControl,
  title = {Robot {{Dynamics}} and {{Control}}},
  author = {Spong, Mark W and Hutchinson, Seth and Vidyasagar, M},
  langid = {english},
  file = {/Users/kevinwu/Zotero/storage/YSSTRFPZ/Spong et al. - Robot Dynamics and Control.pdf}
}

@online{teamOctoOpenSourceGeneralist2024,
  title = {Octo: {{An Open-Source Generalist Robot Policy}}},
  shorttitle = {Octo},
  author = {Team, Octo Model and Ghosh, Dibya and Walke, Homer and Pertsch, Karl and Black, Kevin and Mees, Oier and Dasari, Sudeep and Hejna, Joey and Kreiman, Tobias and Xu, Charles and Luo, Jianlan and Tan, You Liang and Chen, Lawrence Yunliang and Sanketi, Pannag and Vuong, Quan and Xiao, Ted and Sadigh, Dorsa and Finn, Chelsea and Levine, Sergey},
  date = {2024-05-26},
  eprint = {2405.12213},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2405.12213},
  url = {http://arxiv.org/abs/2405.12213},
  urldate = {2024-10-14},
  abstract = {Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/P2CKSU9E/Team et al. - 2024 - Octo An Open-Source Generalist Robot Policy.pdf;/Users/kevinwu/Zotero/storage/L5TBXLU5/2405.html}
}

@article{teamSAM3D3Dfy,
  title = {{{SAM 3D}}: {{3Dfy Anything}} in {{Images}}},
  author = {Team, D and Chen, Xingyu and Chu, Fu-Jen and Gleize, Pierre and Liang, Kevin J and Sax, Alexander and Tang, Hao and Wang, Weiyao and Guo, Michelle and Hardin, Thibaut and Li, Xiang and Lin, Aohan and Liu, Jiawei and Ma, Ziqi and Sagar, Anushka and Song, Bowen and Wang, Xiaodong and Yang, Jianing and Zhang, Bowen and Dollár, Piotr and Gkioxari, Georgia and Malik, Jitendra},
  langid = {english},
  file = {/Users/kevinwu/Zotero/storage/F9TMSIUU/Team et al. - SAM 3D 3Dfy Anything in Images.pdf}
}

@online{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-01},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2024-08-28},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/kevinwu/Zotero/storage/X43IG47J/Vaswani et al. - 2023 - Attention Is All You Need.pdf}
}

@online{weiOmniDexGraspGeneralizableDexterous2025,
  title = {{{OmniDexGrasp}}: {{Generalizable Dexterous Grasping}} via {{Foundation Model}} and {{Force Feedback}}},
  shorttitle = {{{OmniDexGrasp}}},
  author = {Wei, Yi-Lin and Luo, Zhexi and Lin, Yuhao and Lin, Mu and Liang, Zhizhao and Chen, Shuoyu and Zheng, Wei-Shi},
  date = {2025-10-27},
  eprint = {2510.23119},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2510.23119},
  url = {http://arxiv.org/abs/2510.23119},
  urldate = {2025-10-28},
  abstract = {Enabling robots to dexterously grasp and manipulate objects based on human commands is a promising direction in robotics. However, existing approaches are challenging to generalize across diverse objects or tasks due to the limited scale of semantic dexterous grasp datasets. Foundation models offer a new way to enhance generalization, yet directly leveraging them to generate feasible robotic actions remains challenging due to the gap between abstract model knowledge and physical robot execution. To address these challenges, we propose OmniDexGrasp, a generalizable framework that achieves omni-capabilities in user prompting, dexterous embodiment, and grasping tasks by combining foundation models with the transfer and control strategies. OmniDexGrasp integrates three key modules: (i) foundation models are used to enhance generalization by generating human grasp images supporting omni-capability of user prompt and task; (ii) a human-image-to-robot-action transfer strategy converts human demonstrations into executable robot actions, enabling omni dexterous embodiment; (iii) force-aware adaptive grasp strategy ensures robust and stable grasp execution. Experiments in simulation and on real robots validate the effectiveness of OmniDexGrasp on diverse user prompts, grasp task and dexterous hands, and further results show its extensibility to dexterous manipulation tasks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/FSWD6VD8/Wei et al. - 2025 - OmniDexGrasp Generalizable Dexterous Grasping via Foundation Model and Force Feedback.pdf;/Users/kevinwu/Zotero/storage/BS3XM4J2/2510.html}
}

@online{wenFoundationStereoZeroShotStereo2025,
  title = {{{FoundationStereo}}: {{Zero-Shot Stereo Matching}}},
  shorttitle = {{{FoundationStereo}}},
  author = {Wen, Bowen and Trepte, Matthew and Aribido, Joseph and Kautz, Jan and Gallo, Orazio and Birchfield, Stan},
  date = {2025-04-04},
  eprint = {2501.09898},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2501.09898},
  url = {http://arxiv.org/abs/2501.09898},
  urldate = {2025-11-20},
  abstract = {Tremendous progress has been made in deep stereo matching to excel on benchmark datasets through per-domain fine-tuning. However, achieving strong zero-shot generalization - a hallmark of foundation models in other computer vision tasks - remains challenging for stereo matching. We introduce FoundationStereo, a foundation model for stereo depth estimation designed to achieve strong zero-shot generalization. To this end, we first construct a large-scale (1M stereo pairs) synthetic training dataset featuring large diversity and high photorealism, followed by an automatic self-curation pipeline to remove ambiguous samples. We then design a number of network architecture components to enhance scalability, including a side-tuning feature backbone that adapts rich monocular priors from vision foundation models to mitigate the sim-to-real gap, and long-range context reasoning for effective cost volume filtering. Together, these components lead to strong robustness and accuracy across domains, establishing a new standard in zero-shot stereo depth estimation. Project page: https://nvlabs.github.io/FoundationStereo/},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/459ZNM3V/Wen et al. - 2025 - FoundationStereo Zero-Shot Stereo Matching.pdf;/Users/kevinwu/Zotero/storage/DLJ5FE4F/2501.html}
}

@article{wolpertPerspectivesProblemsMotor2001,
  title = {Perspectives and Problems in Motor Learning},
  author = {Wolpert, Daniel M and Ghahramani, Zoubin and Flanagan, J.Randall},
  date = {2001-11},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {5},
  number = {11},
  pages = {487--494},
  issn = {13646613},
  doi = {10.1016/S1364-6613(00)01773-3},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661300017733},
  urldate = {2025-09-09},
  langid = {english},
  file = {/Users/kevinwu/Zotero/storage/FJKBJF77/Wolpert et al. - 2001 - Perspectives and problems in motor learning.pdf}
}

@online{wuEmergenetDigitalTwin2024,
  title = {Emergenet: {{A Digital Twin}} of {{Sequence Evolution}} for {{Scalable Emergence Risk Assessment}} of {{Animal Influenza A Strains}}},
  shorttitle = {Emergenet},
  author = {Wu, Kevin Yuanbo and Li, Jin and Esser-Kahn, Aaron and Chattopadhyay, Ishanu},
  date = {2024-11-26},
  eprint = {2411.17154},
  eprinttype = {arXiv},
  eprintclass = {q-bio},
  doi = {10.48550/arXiv.2411.17154},
  url = {http://arxiv.org/abs/2411.17154},
  urldate = {2025-02-12},
  abstract = {Despite having triggered devastating pandemics in the past, our ability to quantitatively assess the emergence potential of individual strains of animal influenza viruses remains limited. This study introduces Emergenet, a tool to infer a digital twin of sequence evolution to chart how new variants might emerge in the wild. Our predictions based on Emergenets built only using 220,151 Hemagglutinnin (HA) sequences consistently outperform WHO seasonal vaccine recommendations for H1N1/H3N2 subtypes over two decades (average match-improvement: 3.73 AAs, 28.40\textbackslash\%), and are at par with state-of-the-art approaches that use more detailed phenotypic annotations. Finally, our generative models are used to scalably calculate the current odds of emergence of animal strains not yet in human circulation, which strongly correlates with CDC's expert-assessed Influenza Risk Assessment Tool (IRAT) scores (Pearson's \$r = 0.721, p = 10\textasciicircum\{-4\}\$). A minimum five orders of magnitude speedup over CDC's assessment (seconds vs months) then enabled us to analyze 6,354 animal strains collected post-2020 to identify 35 strains with high emergence scores (\${$>$} 7.7\$). The Emergenet framework opens the door to preemptive pandemic mitigation through targeted inoculation of animal hosts before the first human infection.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Populations and Evolution,Statistics - Machine Learning},
  file = {/Users/kevinwu/Zotero/storage/4GAWNVWV/Wu et al. - 2024 - Emergenet A Digital Twin of Sequence Evolution for Scalable Emergence Risk Assessment of Animal Inf.pdf;/Users/kevinwu/Zotero/storage/G23RSAEW/2411.html}
}

@online{xuTactilebasedObjectRetrieval2024,
  title = {Tactile-Based {{Object Retrieval From Granular Media}}},
  author = {Xu, Jingxi and Jia, Yinsen and Yang, Dongxiao and Meng, Patrick and Zhu, Xinyue and Guo, Zihan and Song, Shuran and Ciocarlie, Matei},
  date = {2024-02-21},
  eprint = {2402.04536},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2402.04536},
  url = {http://arxiv.org/abs/2402.04536},
  urldate = {2024-10-29},
  abstract = {We introduce GEOTACT, a robotic manipulation method capable of retrieving objects buried in granular media. This is a challenging task due to the need to interact with granular media, and doing so based exclusively on tactile feedback, since a buried object can be completely hidden from vision. Tactile feedback is in itself challenging in this context, due to ubiquitous contact with the surrounding media, and the inherent noise level induced by the tactile readings. To address these challenges, we use a learning method trained end-to-end with simulated sensor noise. We show that our problem formulation leads to the natural emergence of learned pushing behaviors that the manipulator uses to reduce uncertainty and funnel the object to a stable grasp despite spurious and noisy tactile readings. We also introduce a training curriculum that enables learning these behaviors in simulation, followed by zero-shot transfer to real hardware. To the best of our knowledge, GEOTACT is the first method to reliably retrieve a number of different objects from a granular environment, doing so on real hardware and with integrated tactile sensing. Videos and additional information can be found at https://jxu.ai/geotact.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/UC8J3ZRR/Xu et al. - 2024 - Tactile-based Object Retrieval From Granular Media.pdf;/Users/kevinwu/Zotero/storage/U3B4A6IU/2402.html}
}

@online{yangEgoVLALearningVisionLanguageAction2025,
  title = {{{EgoVLA}}: {{Learning Vision-Language-Action Models}} from {{Egocentric Human Videos}}},
  shorttitle = {{{EgoVLA}}},
  author = {Yang, Ruihan and Yu, Qinxi and Wu, Yecheng and Yan, Rui and Li, Borui and Cheng, An-Chieh and Zou, Xueyan and Fang, Yunhao and Cheng, Xuxin and Qiu, Ri-Zhao and Yin, Hongxu and Liu, Sifei and Han, Song and Lu, Yao and Wang, Xiaolong},
  date = {2025-07-18},
  eprint = {2507.12440},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2507.12440},
  url = {http://arxiv.org/abs/2507.12440},
  urldate = {2025-09-16},
  abstract = {Real robot data collection for imitation learning has led to significant advancements in robotic manipulation. However, the requirement for robot hardware in the process fundamentally constrains the scale of the data. In this paper, we explore training Vision-Language-Action (VLA) models using egocentric human videos. The benefit of using human videos is not only for their scale but more importantly for the richness of scenes and tasks. With a VLA trained on human video that predicts human wrist and hand actions, we can perform Inverse Kinematics and retargeting to convert the human actions to robot actions. We fine-tune the model using a few robot manipulation demonstrations to obtain the robot policy, namely EgoVLA. We propose a simulation benchmark called Ego Humanoid Manipulation Benchmark, where we design diverse bimanual manipulation tasks with demonstrations. We fine-tune and evaluate EgoVLA with Ego Humanoid Manipulation Benchmark and show significant improvements over baselines and ablate the importance of human data. Videos can be found on our website: https://rchalyang.github.io/EgoVLA},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/9QB743M2/Yang et al. - 2025 - EgoVLA Learning Vision-Language-Action Models from Egocentric Human Videos.pdf;/Users/kevinwu/Zotero/storage/H6QEPCAX/2507.html}
}

@online{yangFoundationModelsDecision2023,
  title = {Foundation {{Models}} for {{Decision Making}}: {{Problems}}, {{Methods}}, and {{Opportunities}}},
  shorttitle = {Foundation {{Models}} for {{Decision Making}}},
  author = {Yang, Sherry and Nachum, Ofir and Du, Yilun and Wei, Jason and Abbeel, Pieter and Schuurmans, Dale},
  date = {2023-03-07},
  eprint = {2303.04129},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2303.04129},
  url = {http://arxiv.org/abs/2303.04129},
  urldate = {2024-10-22},
  abstract = {Foundation models pretrained on diverse data at scale have demonstrated extraordinary capabilities in a wide range of vision and language tasks. When such models are deployed in real world environments, they inevitably interface with other entities and agents. For example, language models are often used to interact with human beings through dialogue, and visual perception models are used to autonomously navigate neighborhood streets. In response to these developments, new paradigms are emerging for training foundation models to interact with other agents and perform long-term reasoning. These paradigms leverage the existence of ever-larger datasets curated for multimodal, multitask, and generalist interaction. Research at the intersection of foundation models and decision making holds tremendous promise for creating powerful new systems that can interact effectively across a diverse range of applications such as dialogue, autonomous driving, healthcare, education, and robotics. In this manuscript, we examine the scope of foundation models for decision making, and provide conceptual tools and technical background for understanding the problem space and exploring new research directions. We review recent approaches that ground foundation models in practical decision making applications through a variety of methods such as prompting, conditional generative modeling, planning, optimal control, and reinforcement learning, and discuss common challenges and open problems in the field.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/kevinwu/Zotero/storage/PZJGKJNY/Yang et al. - 2023 - Foundation Models for Decision Making Problems, Methods, and Opportunities.pdf;/Users/kevinwu/Zotero/storage/83672LQV/2303.html}
}

@online{zhaoLearningFineGrainedBimanual2023,
  title = {Learning {{Fine-Grained Bimanual Manipulation}} with {{Low-Cost Hardware}}},
  author = {Zhao, Tony Z. and Kumar, Vikash and Levine, Sergey and Finn, Chelsea},
  date = {2023-04-23},
  eprint = {2304.13705},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2304.13705},
  url = {http://arxiv.org/abs/2304.13705},
  urldate = {2024-10-14},
  abstract = {Fine manipulation tasks, such as threading cable ties or slotting a battery, are notoriously difficult for robots because they require precision, careful coordination of contact forces, and closed-loop visual feedback. Performing these tasks typically requires high-end robots, accurate sensors, or careful calibration, which can be expensive and difficult to set up. Can learning enable low-cost and imprecise hardware to perform these fine manipulation tasks? We present a low-cost system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface. Imitation learning, however, presents its own challenges, particularly in high-precision domains: errors in the policy can compound over time, and human demonstrations can be non-stationary. To address these challenges, we develop a simple yet novel algorithm, Action Chunking with Transformers (ACT), which learns a generative model over action sequences. ACT allows the robot to learn 6 difficult tasks in the real world, such as opening a translucent condiment cup and slotting a battery with 80-90\% success, with only 10 minutes worth of demonstrations. Project website: https://tonyzhaozh.github.io/aloha/},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/EDC8RM9Z/Zhao et al. - 2023 - Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware.pdf;/Users/kevinwu/Zotero/storage/4FXH7N66/2304.html}
}

@online{zhuVisionbasedManipulationSingle2024,
  title = {Vision-Based {{Manipulation}} from {{Single Human Video}} with {{Open-World Object Graphs}}},
  author = {Zhu, Yifeng and Lim, Arisrei and Stone, Peter and Zhu, Yuke},
  date = {2024-05-30},
  eprint = {2405.20321},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.20321},
  url = {http://arxiv.org/abs/2405.20321},
  urldate = {2025-04-03},
  abstract = {We present an object-centric approach to empower robots to learn vision-based manipulation skills from human videos. We investigate the problem of imitating robot manipulation from a single human video in the open-world setting, where a robot must learn to manipulate novel objects from one video demonstration. We introduce ORION, an algorithm that tackles the problem by extracting an object-centric manipulation plan from a single RGB-D video and deriving a policy that conditions on the extracted plan. Our method enables the robot to learn from videos captured by daily mobile devices such as an iPad and generalize the policies to deployment environments with varying visual backgrounds, camera angles, spatial layouts, and novel object instances. We systematically evaluate our method on both short-horizon and long-horizon tasks, demonstrating the efficacy of ORION in learning from a single human video in the open world. Videos can be found in the project website https://ut-austin-rpl.github.io/ORION-release.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/kevinwu/Zotero/storage/EGBSWBDV/Zhu et al. - 2024 - Vision-based Manipulation from Single Human Video with Open-World Object Graphs.pdf;/Users/kevinwu/Zotero/storage/JCHGK5ST/2405.html}
}
