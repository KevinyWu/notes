@online{alakuijalaLearningRewardFunctions2023,
  title = {Learning {{Reward Functions}} for {{Robotic Manipulation}} by {{Observing Humans}}},
  author = {Alakuijala, Minttu and Dulac-Arnold, Gabriel and Mairal, Julien and Ponce, Jean and Schmid, Cordelia},
  date = {2023-03-07},
  eprint = {2211.09019},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2211.09019},
  url = {http://arxiv.org/abs/2211.09019},
  urldate = {2024-10-14},
  abstract = {Observing a human demonstrator manipulate objects provides a rich, scalable and inexpensive source of data for learning robotic policies. However, transferring skills from human videos to a robotic manipulator poses several challenges, not least a difference in action and observation spaces. In this work, we use unlabeled videos of humans solving a wide range of manipulation tasks to learn a task-agnostic reward function for robotic manipulation policies. Thanks to the diversity of this training data, the learned reward function sufficiently generalizes to image observations from a previously unseen robot embodiment and environment to provide a meaningful prior for directed exploration in reinforcement learning. We propose two methods for scoring states relative to a goal image: through direct temporal regression, and through distances in an embedding space obtained with time-contrastive learning. By conditioning the function on a goal image, we are able to reuse one model across a variety of tasks. Unlike prior work on leveraging human videos to teach robots, our method, Human Offline Learned Distances (HOLD) requires neither a priori data from the robot environment, nor a set of task-specific human demonstrations, nor a predefined notion of correspondence across morphologies, yet it is able to accelerate training of several manipulation tasks on a simulated robot arm compared to using only a sparse reward obtained from task completion.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/MNVD9F69/Alakuijala et al. - 2023 - Learning Reward Functions for Robotic Manipulation by Observing Humans.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/4TKP6TW5/2211.html}
}

@online{andrychowiczHindsightExperienceReplay2018,
  title = {Hindsight {{Experience Replay}}},
  author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
  date = {2018-02-23},
  eprint = {1707.01495},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1707.01495},
  url = {http://arxiv.org/abs/1707.01495},
  urldate = {2024-10-29},
  abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/PHHCL756/Andrychowicz et al. - 2018 - Hindsight Experience Replay.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/U7EI3KPE/1707.html}
}

@online{bajcsyLearningVisionbasedPursuitEvasion2023,
  title = {Learning {{Vision-based Pursuit-Evasion Robot Policies}}},
  author = {Bajcsy, Andrea and Loquercio, Antonio and Kumar, Ashish and Malik, Jitendra},
  date = {2023-08-30},
  eprint = {2308.16185},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.16185},
  url = {http://arxiv.org/abs/2308.16185},
  urldate = {2024-10-06},
  abstract = {Learning strategic robot behavior -- like that required in pursuit-evasion interactions -- under real-world constraints is extremely challenging. It requires exploiting the dynamics of the interaction, and planning through both physical state and latent intent uncertainty. In this paper, we transform this intractable problem into a supervised learning problem, where a fully-observable robot policy generates supervision for a partially-observable one. We find that the quality of the supervision signal for the partially-observable pursuer policy depends on two key factors: the balance of diversity and optimality of the evader's behavior and the strength of the modeling assumptions in the fully-observable policy. We deploy our policy on a physical quadruped robot with an RGB-D camera on pursuit-evasion interactions in the wild. Despite all the challenges, the sensing constraints bring about creativity: the robot is pushed to gather information when uncertain, predict intent from noisy measurements, and anticipate in order to intercept. Project webpage: https://abajcsy.github.io/vision-based-pursuit/},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/7GZK3ZCL/Bajcsy et al. - 2023 - Learning Vision-based Pursuit-Evasion Robot Policies.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/W4BANNFN/2308.html}
}

@online{belkhaleDataQualityImitation2023,
  title = {Data {{Quality}} in {{Imitation Learning}}},
  author = {Belkhale, Suneel and Cui, Yuchen and Sadigh, Dorsa},
  date = {2023-06-04},
  url = {https://arxiv.org/abs/2306.02437v1},
  urldate = {2024-09-27},
  abstract = {In supervised learning, the question of data quality and curation has been over-shadowed in recent years by increasingly more powerful and expressive models that can ingest internet-scale data. However, in offline learning for robotics, we simply lack internet scale data, and so high quality datasets are a necessity. This is especially true in imitation learning (IL), a sample efficient paradigm for robot learning using expert demonstrations. Policies learned through IL suffer from state distribution shift at test time due to compounding errors in action prediction, which leads to unseen states that the policy cannot recover from. Instead of designing new algorithms to address distribution shift, an alternative perspective is to develop new ways of assessing and curating datasets. There is growing evidence that the same IL algorithms can have substantially different performance across different datasets. This calls for a formalism for defining metrics of "data quality" that can further be leveraged for data curation. In this work, we take the first step toward formalizing data quality for imitation learning through the lens of distribution shift: a high quality dataset encourages the policy to stay in distribution at test time. We propose two fundamental properties that shape the quality of a dataset: i) action divergence: the mismatch between the expert and learned policy at certain states; and ii) transition diversity: the noise present in the system for a given state and action. We investigate the combined effect of these two key properties in imitation learning theoretically, and we empirically analyze models trained on a variety of different data sources. We show that state diversity is not always beneficial, and we demonstrate how action divergence and transition diversity interact in practice.},
  langid = {english},
  organization = {arXiv.org},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/GCQHCPJ8/Belkhale et al. - 2023 - Data Quality in Imitation Learning.pdf}
}

@online{belkhaleHYDRAHybridRobot2023,
  title = {{{HYDRA}}: {{Hybrid Robot Actions}} for {{Imitation Learning}}},
  shorttitle = {{{HYDRA}}},
  author = {Belkhale, Suneel and Cui, Yuchen and Sadigh, Dorsa},
  date = {2023-06-29},
  url = {https://arxiv.org/abs/2306.17237v2},
  urldate = {2024-09-27},
  abstract = {Imitation Learning (IL) is a sample efficient paradigm for robot learning using expert demonstrations. However, policies learned through IL suffer from state distribution shift at test time, due to compounding errors in action prediction which lead to previously unseen states. Choosing an action representation for the policy that minimizes this distribution shift is critical in imitation learning. Prior work propose using temporal action abstractions to reduce compounding errors, but they often sacrifice policy dexterity or require domain-specific knowledge. To address these trade-offs, we introduce HYDRA, a method that leverages a hybrid action space with two levels of action abstractions: sparse high-level waypoints and dense low-level actions. HYDRA dynamically switches between action abstractions at test time to enable both coarse and fine-grained control of a robot. In addition, HYDRA employs action relabeling to increase the consistency of actions in the dataset, further reducing distribution shift. HYDRA outperforms prior imitation learning methods by 30-40\% on seven challenging simulation and real world environments, involving long-horizon tasks in the real world like making coffee and toasting bread. Videos are found on our website: https://tinyurl.com/3mc6793z},
  langid = {english},
  organization = {arXiv.org},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/3NEWRFB8/Belkhale et al. - 2023 - HYDRA Hybrid Robot Actions for Imitation Learning.pdf}
}

@online{caronEmergingPropertiesSelfSupervised2021,
  title = {Emerging {{Properties}} in {{Self-Supervised Vision Transformers}}},
  author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  date = {2021-05-24},
  eprint = {2104.14294},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2104.14294},
  url = {http://arxiv.org/abs/2104.14294},
  urldate = {2024-10-15},
  abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/P5FCGE5U/Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Transformers.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/QUAHIMIC/2104.html}
}

@online{chane-saneGoalConditionedReinforcementLearning2021,
  title = {Goal-{{Conditioned Reinforcement Learning}} with {{Imagined Subgoals}}},
  author = {Chane-Sane, Elliot and Schmid, Cordelia and Laptev, Ivan},
  date = {2021-07-01},
  eprint = {2107.00541},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2107.00541},
  url = {http://arxiv.org/abs/2107.00541},
  urldate = {2024-10-29},
  abstract = {Goal-conditioned reinforcement learning endows an agent with a large variety of skills, but it often struggles to solve tasks that require more temporally extended reasoning. In this work, we propose to incorporate imagined subgoals into policy learning to facilitate learning of complex tasks. Imagined subgoals are predicted by a separate high-level policy, which is trained simultaneously with the policy and its critic. This high-level policy predicts intermediate states halfway to the goal using the value function as a reachability metric. We don't require the policy to reach these subgoals explicitly. Instead, we use them to define a prior policy, and incorporate this prior into a KL-constrained policy iteration scheme to speed up and regularize learning. Imagined subgoals are used during policy learning, but not during test time, where we only apply the learned policy. We evaluate our approach on complex robotic navigation and manipulation tasks and show that it outperforms existing methods by a large margin.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/GQX4XT2Q/Chane-Sane et al. - 2021 - Goal-Conditioned Reinforcement Learning with Imagined Subgoals.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/Z5VQURVJ/2107.html}
}

@online{chenBigSelfSupervisedModels2020,
  title = {Big {{Self-Supervised Models}} Are {{Strong Semi-Supervised Learners}}},
  author = {Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey},
  date = {2020-10-26},
  eprint = {2006.10029},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2006.10029},
  url = {http://arxiv.org/abs/2006.10029},
  urldate = {2024-10-15},
  abstract = {One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\% ImageNet top-1 accuracy with just 1\% of the labels (\$\textbackslash le\$13 labeled images per class) using ResNet-50, a \$10\textbackslash times\$ improvement in label efficiency over the previous state-of-the-art. With 10\% of labels, ResNet-50 trained with our method achieves 77.5\% top-1 accuracy, outperforming standard supervised training with all of the labels.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/BNKGNNTI/Chen et al. - 2020 - Big Self-Supervised Models are Strong Semi-Supervised Learners.pdf}
}

@online{chenSimpleFrameworkContrastive2020,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  date = {2020-07-01},
  eprint = {2002.05709},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2002.05709},
  url = {http://arxiv.org/abs/2002.05709},
  urldate = {2024-10-15},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/HSEV7AJL/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Visual Representations.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/ZMA87PQU/2002.html}
}

@online{cuiDynaMoDomainDynamics2024,
  title = {{{DynaMo}}: {{In-Domain Dynamics Pretraining}} for {{Visuo-Motor Control}}},
  shorttitle = {{{DynaMo}}},
  author = {Cui, Zichen Jeff and Pan, Hengkai and Iyer, Aadhithya and Haldar, Siddhant and Pinto, Lerrel},
  date = {2024-09-18},
  eprint = {2409.12192},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2409.12192},
  url = {http://arxiv.org/abs/2409.12192},
  urldate = {2024-10-20},
  abstract = {Imitation learning has proven to be a powerful tool for training complex visuomotor policies. However, current methods often require hundreds to thousands of expert demonstrations to handle high-dimensional visual observations. A key reason for this poor data efficiency is that visual representations are predominantly either pretrained on out-of-domain data or trained directly through a behavior cloning objective. In this work, we present DynaMo, a new in-domain, self-supervised method for learning visual representations. Given a set of expert demonstrations, we jointly learn a latent inverse dynamics model and a forward dynamics model over a sequence of image embeddings, predicting the next frame in latent space, without augmentations, contrastive sampling, or access to ground truth actions. Importantly, DynaMo does not require any out-of-domain data such as Internet datasets or cross-embodied datasets. On a suite of six simulated and real environments, we show that representations learned with DynaMo significantly improve downstream imitation learning performance over prior self-supervised learning objectives, and pretrained representations. Gains from using DynaMo hold across policy classes such as Behavior Transformer, Diffusion Policy, MLP, and nearest neighbors. Finally, we ablate over key components of DynaMo and measure its impact on downstream policy performance. Robot videos are best viewed at https://dynamo-ssl.github.io},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/ARQ2RZ67/Cui et al. - 2024 - DynaMo In-Domain Dynamics Pretraining for Visuo-Motor Control.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/CNKVFCX6/2409.html}
}

@online{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  eprint = {2010.11929},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.11929},
  url = {http://arxiv.org/abs/2010.11929},
  urldate = {2024-09-08},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/VGD5R35D/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/3ICKKPZU/2010.html}
}

@online{douTactileAugmentedRadianceFields2024,
  title = {Tactile-{{Augmented Radiance Fields}}},
  author = {Dou, Yiming and Yang, Fengyu and Liu, Yi and Loquercio, Antonio and Owens, Andrew},
  date = {2024-05-07},
  eprint = {2405.04534},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2405.04534},
  url = {http://arxiv.org/abs/2405.04534},
  urldate = {2024-10-12},
  abstract = {We present a scene representation, which we call a tactile-augmented radiance field (TaRF), that brings vision and touch into a shared 3D space. This representation can be used to estimate the visual and tactile signals for a given 3D position within a scene. We capture a scene's TaRF from a collection of photos and sparsely sampled touch probes. Our approach makes use of two insights: (i) common vision-based touch sensors are built on ordinary cameras and thus can be registered to images using methods from multi-view geometry, and (ii) visually and structurally similar regions of a scene share the same tactile features. We use these insights to register touch signals to a captured visual scene, and to train a conditional diffusion model that, provided with an RGB-D image rendered from a neural radiance field, generates its corresponding tactile signal. To evaluate our approach, we collect a dataset of TaRFs. This dataset contains more touch samples than previous real-world datasets, and it provides spatially aligned visual signals for each captured touch signal. We demonstrate the accuracy of our cross-modal generative model and the utility of the captured visual-tactile data on several downstream tasks. Project page: https://dou-yiming.github.io/TaRF},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/T5H7VHTX/Dou et al. - 2024 - Tactile-Augmented Radiance Fields.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/MUAPQGJJ/2405.html}
}

@online{droletComparisonImitationLearning2024,
  title = {A {{Comparison}} of {{Imitation Learning Algorithms}} for {{Bimanual Manipulation}}},
  author = {Drolet, Michael and Stepputtis, Simon and Kailas, Siva and Jain, Ajinkya and Peters, Jan and Schaal, Stefan and Amor, Heni Ben},
  date = {2024-08-24},
  eprint = {2408.06536},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2408.06536},
  url = {http://arxiv.org/abs/2408.06536},
  urldate = {2024-09-26},
  abstract = {Amidst the wide popularity of imitation learning algorithms in robotics, their properties regarding hyperparameter sensitivity, ease of training, data efficiency, and performance have not been well-studied in high-precision industry-inspired environments. In this work, we demonstrate the limitations and benefits of prominent imitation learning approaches and analyze their capabilities regarding these properties. We evaluate each algorithm on a complex bimanual manipulation task involving an over-constrained dynamics system in a setting involving multiple contacts between the manipulated object and the environment. While we find that imitation learning is well suited to solve such complex tasks, not all algorithms are equal in terms of handling environmental and hyperparameter perturbations, training requirements, performance, and ease of use. We investigate the empirical influence of these key characteristics by employing a carefully designed experimental procedure and learning environment. Paper website: https://bimanual-imitation.github.io/},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/VZSGGXCK/Drolet et al. - 2024 - A Comparison of Imitation Learning Algorithms for Bimanual Manipulation.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/Z9HWRJQ2/2408.html}
}

@online{eysenbachContrastiveLearningGoalConditioned2023,
  title = {Contrastive {{Learning}} as {{Goal-Conditioned Reinforcement Learning}}},
  author = {Eysenbach, Benjamin and Zhang, Tianjun and Salakhutdinov, Ruslan and Levine, Sergey},
  date = {2023-02-17},
  eprint = {2206.07568},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2206.07568},
  url = {http://arxiv.org/abs/2206.07568},
  urldate = {2024-10-28},
  abstract = {In reinforcement learning (RL), it is easier to solve a task if given a good representation. While deep RL should automatically acquire such good representations, prior work often finds that learning representations in an end-to-end fashion is unstable and instead equip RL algorithms with additional representation learning parts (e.g., auxiliary losses, data augmentation). How can we design RL algorithms that directly acquire good representations? In this paper, instead of adding representation learning parts to an existing RL algorithm, we show (contrastive) representation learning methods can be cast as RL algorithms in their own right. To do this, we build upon prior work and apply contrastive representation learning to action-labeled trajectories, in such a way that the (inner product of) learned representations exactly corresponds to a goal-conditioned value function. We use this idea to reinterpret a prior RL method as performing contrastive learning, and then use the idea to propose a much simpler method that achieves similar performance. Across a range of goal-conditioned RL tasks, we demonstrate that contrastive RL methods achieve higher success rates than prior non-contrastive methods, including in the offline RL setting. We also show that contrastive RL outperforms prior methods on image-based tasks, without using data augmentation or auxiliary objectives.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/WJKEIQYF/Eysenbach et al. - 2023 - Contrastive Learning as Goal-Conditioned Reinforcement Learning.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/8DE4GJST/2206.html}
}

@online{eysenbachInferenceInterpolationContrastive2024,
  title = {Inference via {{Interpolation}}: {{Contrastive Representations Provably Enable Planning}} and {{Inference}}},
  shorttitle = {Inference via {{Interpolation}}},
  author = {Eysenbach, Benjamin and Myers, Vivek and Salakhutdinov, Ruslan and Levine, Sergey},
  date = {2024-06-18},
  eprint = {2403.04082},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2403.04082},
  url = {http://arxiv.org/abs/2403.04082},
  urldate = {2024-10-29},
  abstract = {Given time series data, how can we answer questions like "what will happen in the future?" and "how did we get here?" These sorts of probabilistic inference questions are challenging when observations are high-dimensional. In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of contrastive learning to time series data. Prior work already shows that the representations learned by contrastive learning encode a probability ratio. By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian. Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-dimensional matrix. In one special case, inferring intermediate representations will be equivalent to interpolating between the learned representations. We validate our theory using numerical simulations on tasks up to 46-dimensions.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/2R6AU66U/Eysenbach et al. - 2024 - Inference via Interpolation Contrastive Representations Provably Enable Planning and Inference.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/KMLR6RPG/2403.html}
}

@online{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014-06-10},
  eprint = {1406.2661},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1406.2661},
  url = {http://arxiv.org/abs/1406.2661},
  urldate = {2024-09-13},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/5B2KKTAJ/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/S2PGWMDR/1406.html}
}

@article{hanoverAutonomousDroneRacing2024,
  title = {Autonomous {{Drone Racing}}: {{A Survey}}},
  shorttitle = {Autonomous {{Drone Racing}}},
  author = {Hanover, Drew and Loquercio, Antonio and Bauersfeld, Leonard and Romero, Angel and Penicka, Robert and Song, Yunlong and Cioffi, Giovanni and Kaufmann, Elia and Scaramuzza, Davide},
  date = {2024},
  journaltitle = {IEEE Transactions on Robotics},
  shortjournal = {IEEE Trans. Robot.},
  volume = {40},
  eprint = {2301.01755},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {3044--3067},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2024.3400838},
  url = {http://arxiv.org/abs/2301.01755},
  urldate = {2024-10-05},
  abstract = {Over the last decade, the use of autonomous drone systems for surveying, search and rescue, or last-mile delivery has increased exponentially. With the rise of these applications comes the need for highly robust, safety-critical algorithms which can operate drones in complex and uncertain environments. Additionally, flying fast enables drones to cover more ground which in turn increases productivity and further strengthens their use case. One proxy for developing algorithms used in high-speed navigation is the task of autonomous drone racing, where researchers program drones to fly through a sequence of gates and avoid obstacles as quickly as possible using onboard sensors and limited computational power. Speeds and accelerations exceed over 80 kph and 4 g respectively, raising significant challenges across perception, planning, control, and state estimation. To achieve maximum performance, systems require real-time algorithms that are robust to motion blur, high dynamic range, model uncertainties, aerodynamic disturbances, and often unpredictable opponents. This survey covers the progression of autonomous drone racing across model-based and learning-based approaches. We provide an overview of the field, its evolution over the years, and conclude with the biggest challenges and open questions to be faced in the future.},
  keywords = {Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/FP2IIEZB/Hanover et al. - 2024 - Autonomous Drone Racing A Survey.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/5ZUR2EUY/2301.html}
}

@online{hatchGHILGlueHierarchicalControl2024,
  title = {{{GHIL-Glue}}: {{Hierarchical Control}} with {{Filtered Subgoal Images}}},
  shorttitle = {{{GHIL-Glue}}},
  author = {Hatch, Kyle B. and Balakrishna, Ashwin and Mees, Oier and Nair, Suraj and Park, Seohong and Wulfe, Blake and Itkina, Masha and Eysenbach, Benjamin and Levine, Sergey and Kollar, Thomas and Burchfiel, Benjamin},
  date = {2024-10-26},
  eprint = {2410.20018},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2410.20018},
  url = {http://arxiv.org/abs/2410.20018},
  urldate = {2024-10-29},
  abstract = {Image and video generative models that are pre-trained on Internet-scale data can greatly increase the generalization capacity of robot learning systems. These models can function as high-level planners, generating intermediate subgoals for low-level goal-conditioned policies to reach. However, the performance of these systems can be greatly bottlenecked by the interface between generative models and low-level controllers. For example, generative models may predict photorealistic yet physically infeasible frames that confuse low-level policies. Low-level policies may also be sensitive to subtle visual artifacts in generated goal images. This paper addresses these two facets of generalization, providing an interface to effectively "glue together" language-conditioned image or video prediction models with low-level goal-conditioned policies. Our method, Generative Hierarchical Imitation Learning-Glue (GHIL-Glue), filters out subgoals that do not lead to task progress and improves the robustness of goal-conditioned policies to generated subgoals with harmful visual artifacts. We find in extensive experiments in both simulated and real environments that GHIL-Glue achieves a 25\% improvement across several hierarchical models that leverage generative subgoals, achieving a new state-of-the-art on the CALVIN simulation benchmark for policies using observations from a single RGB camera. GHIL-Glue also outperforms other generalist robot policies across 3/4 language-conditioned manipulation tasks testing zero-shot generalization in physical experiments.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/4BPPS9RX/Hatch et al. - 2024 - GHIL-Glue Hierarchical Control with Filtered Subgoal Images.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/JZXVVHGX/2410.html}
}

@online{heMaskedAutoencodersAre2021,
  title = {Masked {{Autoencoders Are Scalable Vision Learners}}},
  author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
  date = {2021-12-19},
  eprint = {2111.06377},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2111.06377},
  url = {http://arxiv.org/abs/2111.06377},
  urldate = {2024-10-15},
  abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/C5X2WHGJ/He et al. - 2021 - Masked Autoencoders Are Scalable Vision Learners.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/K42M8CHG/2111.html}
}

@online{heMomentumContrastUnsupervised2020,
  title = {Momentum {{Contrast}} for {{Unsupervised Visual Representation Learning}}},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  date = {2020-03-23},
  eprint = {1911.05722},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1911.05722},
  urldate = {2024-10-15},
  abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning [29] as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/S5YVJUVL/He et al. - 2020 - Momentum Contrast for Unsupervised Visual Representation Learning.pdf}
}

@online{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  date = {2020-12-16},
  eprint = {2006.11239},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2006.11239},
  url = {http://arxiv.org/abs/2006.11239},
  urldate = {2024-09-13},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/PEEAJNY4/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/8UXN7TUV/2006.html}
}

@online{holladayPlanningMultistageForceful2021,
  title = {Planning for {{Multi-stage Forceful Manipulation}}},
  author = {Holladay, Rachel and Lozano-Pérez, Tomás and Rodriguez, Alberto},
  date = {2021-01-07},
  url = {https://arxiv.org/abs/2101.02679v2},
  urldate = {2024-10-06},
  abstract = {Multi-stage forceful manipulation tasks, such as twisting a nut on a bolt, require reasoning over interlocking constraints over discrete as well as continuous choices. The robot must choose a sequence of discrete actions, or strategy, such as whether to pick up an object, and the continuous parameters of each of those actions, such as how to grasp the object. In forceful manipulation tasks, the force requirements substantially impact the choices of both strategy and parameters. To enable planning and executing forceful manipulation, we augment an existing task and motion planner with controllers that exert wrenches and constraints that explicitly consider torque and frictional limits. In two domains, opening a childproof bottle and twisting a nut, we demonstrate how the system considers a combinatorial number of strategies and how choosing actions that are robust to parameter variations impacts the choice of strategy.},
  langid = {english},
  organization = {arXiv.org},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/UNDUGYQP/Holladay et al. - 2021 - Planning for Multi-stage Forceful Manipulation.pdf}
}

@online{huangDiffusionRewardLearning2024,
  title = {Diffusion {{Reward}}: {{Learning Rewards}} via {{Conditional Video Diffusion}}},
  shorttitle = {Diffusion {{Reward}}},
  author = {Huang, Tao and Jiang, Guangqi and Ze, Yanjie and Xu, Huazhe},
  date = {2024-08-09},
  eprint = {2312.14134},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2312.14134},
  url = {http://arxiv.org/abs/2312.14134},
  urldate = {2024-10-14},
  abstract = {Learning rewards from expert videos offers an affordable and effective solution to specify the intended behaviors for reinforcement learning (RL) tasks. In this work, we propose Diffusion Reward, a novel framework that learns rewards from expert videos via conditional video diffusion models for solving complex visual RL problems. Our key insight is that lower generative diversity is exhibited when conditioning diffusion on expert trajectories. Diffusion Reward is accordingly formalized by the negative of conditional entropy that encourages productive exploration of expert behaviors. We show the efficacy of our method over robotic manipulation tasks in both simulation platforms and the real world with visual input. Moreover, Diffusion Reward can even solve unseen tasks successfully and effectively, largely surpassing baseline methods. Project page and code: https://diffusion-reward.github.io.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/G87XU3QY/Huang et al. - 2024 - Diffusion Reward Learning Rewards via Conditional Video Diffusion.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/C4AUPHVS/2312.html}
}

@online{kingmaAutoEncodingVariationalBayes2022,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2022-12-10},
  eprint = {1312.6114},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1312.6114},
  url = {http://arxiv.org/abs/1312.6114},
  urldate = {2024-09-13},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/GI8KSC2Z/Kingma and Welling - 2022 - Auto-Encoding Variational Bayes.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/QPD4623E/1312.html}
}

@inproceedings{knoxInteractivelyShapingAgents2009,
  title = {Interactively Shaping Agents via Human Reinforcement: The {{TAMER}} Framework},
  shorttitle = {Interactively Shaping Agents via Human Reinforcement},
  booktitle = {Proceedings of the Fifth International Conference on {{Knowledge}} Capture},
  author = {Knox, W. Bradley and Stone, Peter},
  date = {2009-09},
  pages = {9--16},
  publisher = {ACM},
  location = {Redondo Beach California USA},
  doi = {10.1145/1597735.1597738},
  url = {https://dl.acm.org/doi/10.1145/1597735.1597738},
  urldate = {2025-02-05},
  abstract = {As computational learning agents move into domains that incur real costs (e.g., autonomous driving or financial investment), it will be necessary to learn good policies without numerous high-cost learning trials. One promising approach to reducing sample complexity of learning a task is knowledge transfer from humans to agents. Ideally, methods of transfer should be accessible to anyone with task knowledge, regardless of that person’s expertise in programming and AI. This paper focuses on allowing a human trainer to interactively shape an agent’s policy via reinforcement signals. Specifically, the paper introduces “Training an Agent Manually via Evaluative Reinforcement,” or tamer, a framework that enables such shaping. Differing from previous approaches to interactive shaping, a tamer agent models the human’s reinforcement and exploits its model by choosing actions expected to be most highly reinforced. Results from two domains demonstrate that lay users can train tamer agents without defining an environmental reward function (as in an MDP) and indicate that human training within the tamer framework can reduce sample complexity over autonomous learning algorithms.},
  eventtitle = {K-{{CAP}} '09: {{Fifth International Conference}} on {{Knowledge Capture}} 2009},
  isbn = {978-1-60558-658-8},
  langid = {english},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/9WYYA8Q5/Knox and Stone - 2009 - Interactively shaping agents via human reinforcement the TAMER framework.pdf}
}

@online{kumarPracticeMakesPerfect2024,
  title = {Practice {{Makes Perfect}}: {{Planning}} to {{Learn Skill Parameter Policies}}},
  shorttitle = {Practice {{Makes Perfect}}},
  author = {Kumar, Nishanth and Silver, Tom and McClinton, Willie and Zhao, Linfeng and Proulx, Stephen and Lozano-Pérez, Tomás and Kaelbling, Leslie Pack and Barry, Jennifer},
  date = {2024-02-22},
  url = {https://arxiv.org/abs/2402.15025v2},
  urldate = {2024-09-25},
  abstract = {One promising approach towards effective robot decision making in complex, long-horizon tasks is to sequence together parameterized skills. We consider a setting where a robot is initially equipped with (1) a library of parameterized skills, (2) an AI planner for sequencing together the skills given a goal, and (3) a very general prior distribution for selecting skill parameters. Once deployed, the robot should rapidly and autonomously learn to improve its performance by specializing its skill parameter selection policy to the particular objects, goals, and constraints in its environment. In this work, we focus on the active learning problem of choosing which skills to practice to maximize expected future task success. We propose that the robot should estimate the competence of each skill, extrapolate the competence (asking: "how much would the competence improve through practice?"), and situate the skill in the task distribution through competence-aware planning. This approach is implemented within a fully autonomous system where the robot repeatedly plans, practices, and learns without any environment resets. Through experiments in simulation, we find that our approach learns effective parameter policies more sample-efficiently than several baselines. Experiments in the real-world demonstrate our approach's ability to handle noise from perception and control and improve the robot's ability to solve two long-horizon mobile-manipulation tasks after a few hours of autonomous practice. Project website: http://ees.csail.mit.edu},
  langid = {english},
  organization = {arXiv.org},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/DCQRAFWD/Kumar et al. - 2024 - Practice Makes Perfect Planning to Learn Skill Parameter Policies.pdf}
}

@online{linReinforcementLearningGroundTruth2019,
  title = {Reinforcement {{Learning}} without {{Ground-Truth State}}},
  author = {Lin, Xingyu and Baweja, Harjatin Singh and Held, David},
  date = {2019-07-15},
  eprint = {1905.07866},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1905.07866},
  url = {http://arxiv.org/abs/1905.07866},
  urldate = {2024-10-29},
  abstract = {To perform robot manipulation tasks, a low-dimensional state of the environment typically needs to be estimated. However, designing a state estimator can sometimes be difficult, especially in environments with deformable objects. An alternative is to learn an end-to-end policy that maps directly from high-dimensional sensor inputs to actions. However, if this policy is trained with reinforcement learning, then without a state estimator, it is hard to specify a reward function based on high-dimensional observations. To meet this challenge, we propose a simple indicator reward function for goal-conditioned reinforcement learning: we only give a positive reward when the robot's observation exactly matches a target goal observation. We show that by relabeling the original goal with the achieved goal to obtain positive rewards (Andrychowicz et al., 2017), we can learn with the indicator reward function even in continuous state spaces. We propose two methods to further speed up convergence with indicator rewards: reward balancing and reward filtering. We show comparable performance between our method and an oracle which uses the ground-truth state for computing rewards. We show that our method can perform complex tasks in continuous state spaces such as rope manipulation from RGB-D images, without knowledge of the ground-truth state.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/J6M4LE9H/Lin et al. - 2019 - Reinforcement Learning without Ground-Truth State.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/TFQ64QP3/1905.html}
}

@online{liuSelfImprovingAutonomousUnderwater2024,
  title = {Self-{{Improving Autonomous Underwater Manipulation}}},
  author = {Liu, Ruoshi and Ha, Huy and Hou, Mengxue and Song, Shuran and Vondrick, Carl},
  date = {2024-10-24},
  eprint = {2410.18969},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2410.18969},
  url = {http://arxiv.org/abs/2410.18969},
  urldate = {2024-10-27},
  abstract = {Underwater robotic manipulation faces significant challenges due to complex fluid dynamics and unstructured environments, causing most manipulation systems to rely heavily on human teleoperation. In this paper, we introduce AquaBot, a fully autonomous manipulation system that combines behavior cloning from human demonstrations with self-learning optimization to improve beyond human teleoperation performance. With extensive real-world experiments, we demonstrate AquaBot's versatility across diverse manipulation tasks, including object grasping, trash sorting, and rescue retrieval. Our real-world experiments show that AquaBot's self-optimized policy outperforms a human operator by 41\% in speed. AquaBot represents a promising step towards autonomous and self-improving underwater manipulation systems. We open-source both hardware and software implementation details.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/B84VZKA5/Liu et al. - 2024 - Self-Improving Autonomous Underwater Manipulation.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/UCA2IW97/2410.html}
}

@online{liuSingleGoalAll2024,
  title = {A {{Single Goal}} Is {{All You Need}}: {{Skills}} and {{Exploration Emerge}} from {{Contrastive RL}} without {{Rewards}}, {{Demonstrations}}, or {{Subgoals}}},
  shorttitle = {A {{Single Goal}} Is {{All You Need}}},
  author = {Liu, Grace and Tang, Michael and Eysenbach, Benjamin},
  date = {2024-08-11},
  eprint = {2408.05804},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2408.05804},
  url = {http://arxiv.org/abs/2408.05804},
  urldate = {2024-10-07},
  abstract = {In this paper, we present empirical evidence of skills and directed exploration emerging from a simple RL algorithm long before any successful trials are observed. For example, in a manipulation task, the agent is given a single observation of the goal state and learns skills, first for moving its end-effector, then for pushing the block, and finally for picking up and placing the block. These skills emerge before the agent has ever successfully placed the block at the goal location and without the aid of any reward functions, demonstrations, or manually-specified distance metrics. Once the agent has learned to reach the goal state reliably, exploration is reduced. Implementing our method involves a simple modification of prior work and does not require density estimates, ensembles, or any additional hyperparameters. Intuitively, the proposed method seems like it should be terrible at exploration, and we lack a clear theoretical understanding of why it works so effectively, though our experiments provide some hints.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/BC82PDW6/Liu et al. - 2024 - A Single Goal is All You Need Skills and Exploration Emerge from Contrastive RL without Rewards, De.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/EGHJP2CV/Liu et al. - 2024 - A Single Goal is All You Need Skills and Exploration Emerge from Contrastive RL without Rewards, De.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/7A8VTL3V/2408.html;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/QMW57Y78/2408.html}
}

@online{maVIPUniversalVisual2023,
  title = {{{VIP}}: {{Towards Universal Visual Reward}} and {{Representation}} via {{Value-Implicit Pre-Training}}},
  shorttitle = {{{VIP}}},
  author = {Ma, Yecheng Jason and Sodhani, Shagun and Jayaraman, Dinesh and Bastani, Osbert and Kumar, Vikash and Zhang, Amy},
  date = {2023-03-07},
  eprint = {2210.00030},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2210.00030},
  url = {http://arxiv.org/abs/2210.00030},
  urldate = {2024-10-14},
  abstract = {Reward and representation learning are two long-standing challenges for learning an expanding set of robot manipulation skills from sensory observations. Given the inherent cost and scarcity of in-domain, task-specific robot data, learning from large, diverse, offline human videos has emerged as a promising path towards acquiring a generally useful visual representation for control; however, how these human videos can be used for general-purpose reward learning remains an open question. We introduce \$\textbackslash textbf\{V\}\$alue-\$\textbackslash textbf\{I\}\$mplicit \$\textbackslash textbf\{P\}\$re-training (VIP), a self-supervised pre-trained visual representation capable of generating dense and smooth reward functions for unseen robotic tasks. VIP casts representation learning from human videos as an offline goal-conditioned reinforcement learning problem and derives a self-supervised dual goal-conditioned value-function objective that does not depend on actions, enabling pre-training on unlabeled human videos. Theoretically, VIP can be understood as a novel implicit time contrastive objective that generates a temporally smooth embedding, enabling the value function to be implicitly defined via the embedding distance, which can then be used to construct the reward for any goal-image specified downstream task. Trained on large-scale Ego4D human videos and without any fine-tuning on in-domain, task-specific data, VIP's frozen representation can provide dense visual reward for an extensive set of simulated and \$\textbackslash textbf\{real-robot\}\$ tasks, enabling diverse reward-based visual control methods and significantly outperforming all prior pre-trained representations. Notably, VIP can enable simple, \$\textbackslash textbf\{few-shot\}\$ offline RL on a suite of real-world robot tasks with as few as 20 trajectories.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/ME67B54J/Ma et al. - 2023 - VIP Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/FH9EQD8B/2210.html}
}

@online{mnihPlayingAtariDeep2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  date = {2013-12-19},
  eprint = {1312.5602},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1312.5602},
  url = {http://arxiv.org/abs/1312.5602},
  urldate = {2024-10-15},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/83UPZ5AI/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/V3P48HSS/1312.html}
}

@online{myersLearningTemporalDistances2024,
  title = {Learning {{Temporal Distances}}: {{Contrastive Successor Features Can Provide}} a {{Metric Structure}} for {{Decision-Making}}},
  shorttitle = {Learning {{Temporal Distances}}},
  author = {Myers, Vivek and Zheng, Chongyi and Dragan, Anca and Levine, Sergey and Eysenbach, Benjamin},
  date = {2024-06-24},
  eprint = {2406.17098},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2406.17098},
  url = {http://arxiv.org/abs/2406.17098},
  urldate = {2024-10-29},
  abstract = {Temporal distances lie at the heart of many algorithms for planning, control, and reinforcement learning that involve reaching goals, allowing one to estimate the transit time between two states. However, prior attempts to define such temporal distances in stochastic settings have been stymied by an important limitation: these prior approaches do not satisfy the triangle inequality. This is not merely a definitional concern, but translates to an inability to generalize and find shortest paths. In this paper, we build on prior work in contrastive learning and quasimetrics to show how successor features learned by contrastive learning (after a change of variables) form a temporal distance that does satisfy the triangle inequality, even in stochastic settings. Importantly, this temporal distance is computationally efficient to estimate, even in high-dimensional and stochastic settings. Experiments in controlled settings and benchmark suites demonstrate that an RL algorithm based on these new temporal distances exhibits combinatorial generalization (i.e., "stitching") and can sometimes learn more quickly than prior methods, including those based on quasimetrics.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/NV6UAYBN/Myers et al. - 2024 - Learning Temporal Distances Contrastive Successor Features Can Provide a Metric Structure for Decis.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/36Z56KH5/2406.html}
}

@online{nasirianyPIVOTIterativeVisual2024,
  title = {{{PIVOT}}: {{Iterative Visual Prompting Elicits Actionable Knowledge}} for {{VLMs}}},
  shorttitle = {{{PIVOT}}},
  author = {Nasiriany, Soroush and Xia, Fei and Yu, Wenhao and Xiao, Ted and Liang, Jacky and Dasgupta, Ishita and Xie, Annie and Driess, Danny and Wahid, Ayzaan and Xu, Zhuo and Vuong, Quan and Zhang, Tingnan and Lee, Tsang-Wei Edward and Lee, Kuang-Huei and Xu, Peng and Kirmani, Sean and Zhu, Yuke and Zeng, Andy and Hausman, Karol and Heess, Nicolas and Finn, Chelsea and Levine, Sergey and Ichter, Brian},
  date = {2024-02-12},
  eprint = {2402.07872},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2402.07872},
  url = {http://arxiv.org/abs/2402.07872},
  urldate = {2024-10-29},
  abstract = {Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data? In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available answer. We investigate PIVOT on real-world robotic navigation, real-world manipulation from images, instruction following in simulation, and additional spatial inference tasks such as localization. We find, perhaps surprisingly, that our approach enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities. Although current performance is far from perfect, our work highlights potentials and limitations of this new regime and shows a promising approach for Internet-Scale VLMs in robotic and spatial reasoning domains. Website: pivot-prompt.github.io and HuggingFace: https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/7Y59IFTZ/Nasiriany et al. - 2024 - PIVOT Iterative Visual Prompting Elicits Actionable Knowledge for VLMs.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/AXSFJUNB/2402.html}
}

@online{openaiAsymmetricSelfplayAutomatic2021,
  title = {Asymmetric Self-Play for Automatic Goal Discovery in Robotic Manipulation},
  author = {OpenAI, OpenAI and Plappert, Matthias and Sampedro, Raul and Xu, Tao and Akkaya, Ilge and Kosaraju, Vineet and Welinder, Peter and D'Sa, Ruben and Petron, Arthur and Pinto, Henrique P. d O. and Paino, Alex and Noh, Hyeonwoo and Weng, Lilian and Yuan, Qiming and Chu, Casey and Zaremba, Wojciech},
  date = {2021-01-13},
  eprint = {2101.04882},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2101.04882},
  url = {http://arxiv.org/abs/2101.04882},
  urldate = {2024-10-29},
  abstract = {We train a single, goal-conditioned policy that can solve many robotic manipulation tasks, including tasks with previously unseen goals and objects. We rely on asymmetric self-play for goal discovery, where two agents, Alice and Bob, play a game. Alice is asked to propose challenging goals and Bob aims to solve them. We show that this method can discover highly diverse and complex goals without any human priors. Bob can be trained with only sparse rewards, because the interaction between Alice and Bob results in a natural curriculum and Bob can learn from Alice's trajectory when relabeled as a goal-conditioned demonstration. Finally, our method scales, resulting in a single policy that can generalize to many unseen tasks such as setting a table, stacking blocks, and solving simple puzzles. Videos of a learned policy is available at https://robotics-self-play.github.io.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/HIIMCWKE/OpenAI et al. - 2021 - Asymmetric self-play for automatic goal discovery in robotic manipulation.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/GE62AMTP/2101.html}
}

@online{pathakCuriositydrivenExplorationSelfsupervised2017,
  title = {Curiosity-Driven {{Exploration}} by {{Self-supervised Prediction}}},
  author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
  date = {2017-05-15},
  eprint = {1705.05363},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1705.05363},
  url = {http://arxiv.org/abs/1705.05363},
  urldate = {2024-10-29},
  abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/F6F3N4BB/Pathak et al. - 2017 - Curiosity-driven Exploration by Self-supervised Prediction.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/FT943TB3/1705.html}
}

@online{qiControlorientedClusteringVisual2024,
  title = {Control-Oriented {{Clustering}} of {{Visual Latent Representation}}},
  author = {Qi, Han and Yin, Haocheng and Yang, Heng},
  date = {2024-10-08},
  eprint = {2410.05063},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2410.05063},
  url = {http://arxiv.org/abs/2410.05063},
  urldate = {2024-10-20},
  abstract = {We initiate a study of the geometry of the visual representation space -- the information channel from the vision encoder to the action decoder -- in an image-based control pipeline learned from behavior cloning. Inspired by the phenomenon of neural collapse (NC) in image classification, we investigate whether a similar law of clustering emerges in the visual representation space. Since image-based control is a regression task without explicitly defined classes, the central piece of the puzzle lies in determining according to what implicit classes the visual features cluster, if such a law exists. Focusing on image-based planar pushing, we posit the most important role of the visual representation in a control task is to convey a goal to the action decoder. We then classify training samples of expert demonstrations into eight "control-oriented" classes based on (a) the relative pose between the object and the target in the input or (b) the relative pose of the object induced by expert actions in the output, where one class corresponds to one relative pose orthant (REPO). Across four different instantiations of architecture, we report the prevalent emergence of control-oriented clustering in the visual representation space according to the eight REPOs. Beyond empirical observation, we show such a law of clustering can be leveraged as an algorithmic tool to improve test-time performance when training a policy with limited expert demonstrations. Particularly, we pretrain the vision encoder using NC as a regularization to encourage control-oriented clustering of the visual features. Surprisingly, such an NC-pretrained vision encoder, when finetuned end-to-end with the action decoder, boosts the test-time performance by 10\% to 35\% in the low-data regime. Real-world vision-based planar pushing experiments confirmed the surprising advantage of control-oriented visual representation pretraining.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/DIANHNAX/Qi et al. - 2024 - Control-oriented Clustering of Visual Latent Representation.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/GUVSRWKF/2410.html}
}

@online{shahRapidExplorationOpenWorld2023,
  title = {Rapid {{Exploration}} for {{Open-World Navigation}} with {{Latent Goal Models}}},
  author = {Shah, Dhruv and Eysenbach, Benjamin and Kahn, Gregory and Rhinehart, Nicholas and Levine, Sergey},
  date = {2023-10-11},
  eprint = {2104.05859},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2104.05859},
  url = {http://arxiv.org/abs/2104.05859},
  urldate = {2024-10-29},
  abstract = {We describe a robotic learning system for autonomous exploration and navigation in diverse, open-world environments. At the core of our method is a learned latent variable model of distances and actions, along with a non-parametric topological memory of images. We use an information bottleneck to regularize the learned policy, giving us (i) a compact visual representation of goals, (ii) improved generalization capabilities, and (iii) a mechanism for sampling feasible goals for exploration. Trained on a large offline dataset of prior experience, the model acquires a representation of visual goals that is robust to task-irrelevant distractors. We demonstrate our method on a mobile ground robot in open-world exploration scenarios. Given an image of a goal that is up to 80 meters away, our method leverages its representation to explore and discover the goal in under 20 minutes, even amidst previously-unseen obstacles and weather conditions. Please check out the project website for videos of our experiments and information about the real-world dataset used at https://sites.google.com/view/recon-robot.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/FBH3UXXK/Shah et al. - 2023 - Rapid Exploration for Open-World Navigation with Latent Goal Models.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/52UMKTJX/2104.html}
}

@online{singhHandObjectInteractionPretraining2024,
  title = {Hand-{{Object Interaction Pretraining}} from {{Videos}}},
  author = {Singh, Himanshu Gaurav and Loquercio, Antonio and Sferrazza, Carmelo and Wu, Jane and Qi, Haozhi and Abbeel, Pieter and Malik, Jitendra},
  date = {2024-09-12},
  eprint = {2409.08273},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2409.08273},
  url = {http://arxiv.org/abs/2409.08273},
  urldate = {2024-10-12},
  abstract = {We present an approach to learn general robot manipulation priors from 3D hand-object interaction trajectories. We build a framework to use in-the-wild videos to generate sensorimotor robot trajectories. We do so by lifting both the human hand and the manipulated object in a shared 3D space and retargeting human motions to robot actions. Generative modeling on this data gives us a task-agnostic base policy. This policy captures a general yet flexible manipulation prior. We empirically demonstrate that finetuning this policy, with both reinforcement learning (RL) and behavior cloning (BC), enables sample-efficient adaptation to downstream tasks and simultaneously improves robustness and generalizability compared to prior approaches. Qualitative experiments are available at: \textbackslash url\{https://hgaurav2k.github.io/hop/\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/S68QE7NX/Singh et al. - 2024 - Hand-Object Interaction Pretraining from Videos.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/MX65L2IN/2409.html}
}

@inproceedings{sohnLearningStructuredOutput2015,
  title = {Learning {{Structured Output Representation}} Using {{Deep Conditional Generative Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
  date = {2015},
  volume = {28},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2015/hash/8d55a249e6baa5c06772297520da2051-Abstract.html},
  urldate = {2024-09-13},
  abstract = {Supervised deep learning has been successfully applied for many recognition problems in machine learning and computer vision. Although it can approximate a complex many-to-one function very well when large number of training data is provided, the lack of probabilistic inference of the current supervised deep learning methods makes it difficult to model a complex structured output representations. In this work, we develop a scalable deep conditional generative model for structured output variables using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient variational Bayes, and allows a fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build a robust structured prediction algorithms, such as recurrent prediction network architecture, input noise-injection and multi-scale prediction training methods. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic output representations using stochastic inference. Furthermore, the proposed schemes in training methods and architecture design were complimentary, which leads to achieve strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/A2GM32KN/Sohn et al. - 2015 - Learning Structured Output Representation using Deep Conditional Generative Models.pdf}
}

@article{spongRobotDynamicsControl,
  title = {Robot {{Dynamics}} and {{Control}}},
  author = {Spong, Mark W and Hutchinson, Seth and Vidyasagar, M},
  langid = {english},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/YSSTRFPZ/Spong et al. - Robot Dynamics and Control.pdf}
}

@online{teamOctoOpenSourceGeneralist2024,
  title = {Octo: {{An Open-Source Generalist Robot Policy}}},
  shorttitle = {Octo},
  author = {Team, Octo Model and Ghosh, Dibya and Walke, Homer and Pertsch, Karl and Black, Kevin and Mees, Oier and Dasari, Sudeep and Hejna, Joey and Kreiman, Tobias and Xu, Charles and Luo, Jianlan and Tan, You Liang and Chen, Lawrence Yunliang and Sanketi, Pannag and Vuong, Quan and Xiao, Ted and Sadigh, Dorsa and Finn, Chelsea and Levine, Sergey},
  date = {2024-05-26},
  eprint = {2405.12213},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2405.12213},
  url = {http://arxiv.org/abs/2405.12213},
  urldate = {2024-10-14},
  abstract = {Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/P2CKSU9E/Team et al. - 2024 - Octo An Open-Source Generalist Robot Policy.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/L5TBXLU5/2405.html}
}

@online{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-01},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2024-08-28},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/X43IG47J/Vaswani et al. - 2023 - Attention Is All You Need.pdf}
}

@online{xuTactilebasedObjectRetrieval2024,
  title = {Tactile-Based {{Object Retrieval From Granular Media}}},
  author = {Xu, Jingxi and Jia, Yinsen and Yang, Dongxiao and Meng, Patrick and Zhu, Xinyue and Guo, Zihan and Song, Shuran and Ciocarlie, Matei},
  date = {2024-02-21},
  eprint = {2402.04536},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2402.04536},
  url = {http://arxiv.org/abs/2402.04536},
  urldate = {2024-10-29},
  abstract = {We introduce GEOTACT, a robotic manipulation method capable of retrieving objects buried in granular media. This is a challenging task due to the need to interact with granular media, and doing so based exclusively on tactile feedback, since a buried object can be completely hidden from vision. Tactile feedback is in itself challenging in this context, due to ubiquitous contact with the surrounding media, and the inherent noise level induced by the tactile readings. To address these challenges, we use a learning method trained end-to-end with simulated sensor noise. We show that our problem formulation leads to the natural emergence of learned pushing behaviors that the manipulator uses to reduce uncertainty and funnel the object to a stable grasp despite spurious and noisy tactile readings. We also introduce a training curriculum that enables learning these behaviors in simulation, followed by zero-shot transfer to real hardware. To the best of our knowledge, GEOTACT is the first method to reliably retrieve a number of different objects from a granular environment, doing so on real hardware and with integrated tactile sensing. Videos and additional information can be found at https://jxu.ai/geotact.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/UC8J3ZRR/Xu et al. - 2024 - Tactile-based Object Retrieval From Granular Media.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/U3B4A6IU/2402.html}
}

@online{yangFoundationModelsDecision2023,
  title = {Foundation {{Models}} for {{Decision Making}}: {{Problems}}, {{Methods}}, and {{Opportunities}}},
  shorttitle = {Foundation {{Models}} for {{Decision Making}}},
  author = {Yang, Sherry and Nachum, Ofir and Du, Yilun and Wei, Jason and Abbeel, Pieter and Schuurmans, Dale},
  date = {2023-03-07},
  eprint = {2303.04129},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2303.04129},
  url = {http://arxiv.org/abs/2303.04129},
  urldate = {2024-10-22},
  abstract = {Foundation models pretrained on diverse data at scale have demonstrated extraordinary capabilities in a wide range of vision and language tasks. When such models are deployed in real world environments, they inevitably interface with other entities and agents. For example, language models are often used to interact with human beings through dialogue, and visual perception models are used to autonomously navigate neighborhood streets. In response to these developments, new paradigms are emerging for training foundation models to interact with other agents and perform long-term reasoning. These paradigms leverage the existence of ever-larger datasets curated for multimodal, multitask, and generalist interaction. Research at the intersection of foundation models and decision making holds tremendous promise for creating powerful new systems that can interact effectively across a diverse range of applications such as dialogue, autonomous driving, healthcare, education, and robotics. In this manuscript, we examine the scope of foundation models for decision making, and provide conceptual tools and technical background for understanding the problem space and exploring new research directions. We review recent approaches that ground foundation models in practical decision making applications through a variety of methods such as prompting, conditional generative modeling, planning, optimal control, and reinforcement learning, and discuss common challenges and open problems in the field.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/PZJGKJNY/Yang et al. - 2023 - Foundation Models for Decision Making Problems, Methods, and Opportunities.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/83672LQV/2303.html}
}

@online{yangRank2RewardLearningShaped2024,
  title = {{{Rank2Reward}}: {{Learning Shaped Reward Functions}} from {{Passive Video}}},
  shorttitle = {{{Rank2Reward}}},
  author = {Yang, Daniel and Tjia, Davin and Berg, Jacob and Damen, Dima and Agrawal, Pulkit and Gupta, Abhishek},
  date = {2024-04-23},
  eprint = {2404.14735},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2404.14735},
  url = {http://arxiv.org/abs/2404.14735},
  urldate = {2024-10-14},
  abstract = {Teaching robots novel skills with demonstrations via human-in-the-loop data collection techniques like kinesthetic teaching or teleoperation puts a heavy burden on human supervisors. In contrast to this paradigm, it is often significantly easier to provide raw, action-free visual data of tasks being performed. Moreover, this data can even be mined from video datasets or the web. Ideally, this data can serve to guide robot learning for new tasks in novel environments, informing both "what" to do and "how" to do it. A powerful way to encode both the "what" and the "how" is to infer a well-shaped reward function for reinforcement learning. The challenge is determining how to ground visual demonstration inputs into a well-shaped and informative reward function. We propose a technique Rank2Reward for learning behaviors from videos of tasks being performed without access to any low-level states and actions. We do so by leveraging the videos to learn a reward function that measures incremental "progress" through a task by learning how to temporally rank the video frames in a demonstration. By inferring an appropriate ranking, the reward function is able to guide reinforcement learning by indicating when task progress is being made. This ranking function can be integrated into an adversarial imitation learning scheme resulting in an algorithm that can learn behaviors without exploiting the learned reward function. We demonstrate the effectiveness of Rank2Reward at learning behaviors from raw video on a number of tabletop manipulation tasks in both simulations and on a real-world robotic arm. We also demonstrate how Rank2Reward can be easily extended to be applicable to web-scale video datasets.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/RQQQQWFI/Yang et al. - 2024 - Rank2Reward Learning Shaped Reward Functions from Passive Video.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/EPBSPV6H/2404.html}
}

@online{yaratsMasteringVisualContinuous2021,
  title = {Mastering {{Visual Continuous Control}}: {{Improved Data-Augmented Reinforcement Learning}}},
  shorttitle = {Mastering {{Visual Continuous Control}}},
  author = {Yarats, Denis and Fergus, Rob and Lazaric, Alessandro and Pinto, Lerrel},
  date = {2021-07-20},
  eprint = {2107.09645},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2107.09645},
  url = {http://arxiv.org/abs/2107.09645},
  urldate = {2024-10-14},
  abstract = {We present DrQ-v2, a model-free reinforcement learning (RL) algorithm for visual continuous control. DrQ-v2 builds on DrQ, an off-policy actor-critic approach that uses data augmentation to learn directly from pixels. We introduce several improvements that yield state-of-the-art results on the DeepMind Control Suite. Notably, DrQ-v2 is able to solve complex humanoid locomotion tasks directly from pixel observations, previously unattained by model-free RL. DrQ-v2 is conceptually simple, easy to implement, and provides significantly better computational footprint compared to prior work, with the majority of tasks taking just 8 hours to train on a single GPU. Finally, we publicly release DrQ-v2's implementation to provide RL practitioners with a strong and computationally efficient baseline.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/6CMF9333/Yarats et al. - 2021 - Mastering Visual Continuous Control Improved Data-Augmented Reinforcement Learning.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/D2WYQZME/2107.html}
}

@online{zhaoLearningFineGrainedBimanual2023,
  title = {Learning {{Fine-Grained Bimanual Manipulation}} with {{Low-Cost Hardware}}},
  author = {Zhao, Tony Z. and Kumar, Vikash and Levine, Sergey and Finn, Chelsea},
  date = {2023-04-23},
  eprint = {2304.13705},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2304.13705},
  url = {http://arxiv.org/abs/2304.13705},
  urldate = {2024-10-14},
  abstract = {Fine manipulation tasks, such as threading cable ties or slotting a battery, are notoriously difficult for robots because they require precision, careful coordination of contact forces, and closed-loop visual feedback. Performing these tasks typically requires high-end robots, accurate sensors, or careful calibration, which can be expensive and difficult to set up. Can learning enable low-cost and imprecise hardware to perform these fine manipulation tasks? We present a low-cost system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface. Imitation learning, however, presents its own challenges, particularly in high-precision domains: errors in the policy can compound over time, and human demonstrations can be non-stationary. To address these challenges, we develop a simple yet novel algorithm, Action Chunking with Transformers (ACT), which learns a generative model over action sequences. ACT allows the robot to learn 6 difficult tasks in the real world, such as opening a translucent condiment cup and slotting a battery with 80-90\% success, with only 10 minutes worth of demonstrations. Project website: https://tonyzhaozh.github.io/aloha/},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/EDC8RM9Z/Zhao et al. - 2023 - Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/4FXH7N66/2304.html}
}

@online{zhengStabilizingContrastiveRL2024,
  title = {Stabilizing {{Contrastive RL}}: {{Techniques}} for {{Robotic Goal Reaching}} from {{Offline Data}}},
  shorttitle = {Stabilizing {{Contrastive RL}}},
  author = {Zheng, Chongyi and Eysenbach, Benjamin and Walke, Homer and Yin, Patrick and Fang, Kuan and Salakhutdinov, Ruslan and Levine, Sergey},
  date = {2024-02-26},
  eprint = {2306.03346},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2306.03346},
  url = {http://arxiv.org/abs/2306.03346},
  urldate = {2024-10-30},
  abstract = {Robotic systems that rely primarily on self-supervised learning have the potential to decrease the amount of human annotation and engineering effort required to learn control strategies. In the same way that prior robotic systems have leveraged self-supervised techniques from computer vision (CV) and natural language processing (NLP), our work builds on prior work showing that the reinforcement learning (RL) itself can be cast as a self-supervised problem: learning to reach any goal without human-specified rewards or labels. Despite the seeming appeal, little (if any) prior work has demonstrated how self-supervised RL methods can be practically deployed on robotic systems. By first studying a challenging simulated version of this task, we discover design decisions about architectures and hyperparameters that increase the success rate by \$2 \textbackslash times\$. These findings lay the groundwork for our main result: we demonstrate that a self-supervised RL algorithm based on contrastive learning can solve real-world, image-based robotic manipulation tasks, with tasks being specified by a single goal image provided after training.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/65UL94L7/Zheng et al. - 2024 - Stabilizing Contrastive RL Techniques for Robotic Goal Reaching from Offline Data.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/ZUBN68CZ/2306.html}
}
