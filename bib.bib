@online{bajcsyLearningVisionbasedPursuitEvasion2023,
  title = {Learning {{Vision-based Pursuit-Evasion Robot Policies}}},
  author = {Bajcsy, Andrea and Loquercio, Antonio and Kumar, Ashish and Malik, Jitendra},
  date = {2023-08-30},
  eprint = {2308.16185},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.16185},
  url = {http://arxiv.org/abs/2308.16185},
  urldate = {2024-10-06},
  abstract = {Learning strategic robot behavior -- like that required in pursuit-evasion interactions -- under real-world constraints is extremely challenging. It requires exploiting the dynamics of the interaction, and planning through both physical state and latent intent uncertainty. In this paper, we transform this intractable problem into a supervised learning problem, where a fully-observable robot policy generates supervision for a partially-observable one. We find that the quality of the supervision signal for the partially-observable pursuer policy depends on two key factors: the balance of diversity and optimality of the evader's behavior and the strength of the modeling assumptions in the fully-observable policy. We deploy our policy on a physical quadruped robot with an RGB-D camera on pursuit-evasion interactions in the wild. Despite all the challenges, the sensing constraints bring about creativity: the robot is pushed to gather information when uncertain, predict intent from noisy measurements, and anticipate in order to intercept. Project webpage: https://abajcsy.github.io/vision-based-pursuit/},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/7GZK3ZCL/Bajcsy et al. - 2023 - Learning Vision-based Pursuit-Evasion Robot Policies.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/W4BANNFN/2308.html}
}

@online{belkhaleDataQualityImitation2023,
  title = {Data {{Quality}} in {{Imitation Learning}}},
  author = {Belkhale, Suneel and Cui, Yuchen and Sadigh, Dorsa},
  date = {2023-06-04},
  url = {https://arxiv.org/abs/2306.02437v1},
  urldate = {2024-09-27},
  abstract = {In supervised learning, the question of data quality and curation has been over-shadowed in recent years by increasingly more powerful and expressive models that can ingest internet-scale data. However, in offline learning for robotics, we simply lack internet scale data, and so high quality datasets are a necessity. This is especially true in imitation learning (IL), a sample efficient paradigm for robot learning using expert demonstrations. Policies learned through IL suffer from state distribution shift at test time due to compounding errors in action prediction, which leads to unseen states that the policy cannot recover from. Instead of designing new algorithms to address distribution shift, an alternative perspective is to develop new ways of assessing and curating datasets. There is growing evidence that the same IL algorithms can have substantially different performance across different datasets. This calls for a formalism for defining metrics of "data quality" that can further be leveraged for data curation. In this work, we take the first step toward formalizing data quality for imitation learning through the lens of distribution shift: a high quality dataset encourages the policy to stay in distribution at test time. We propose two fundamental properties that shape the quality of a dataset: i) action divergence: the mismatch between the expert and learned policy at certain states; and ii) transition diversity: the noise present in the system for a given state and action. We investigate the combined effect of these two key properties in imitation learning theoretically, and we empirically analyze models trained on a variety of different data sources. We show that state diversity is not always beneficial, and we demonstrate how action divergence and transition diversity interact in practice.},
  langid = {english},
  organization = {arXiv.org},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/GCQHCPJ8/Belkhale et al. - 2023 - Data Quality in Imitation Learning.pdf}
}

@online{belkhaleHYDRAHybridRobot2023,
  title = {{{HYDRA}}: {{Hybrid Robot Actions}} for {{Imitation Learning}}},
  shorttitle = {{{HYDRA}}},
  author = {Belkhale, Suneel and Cui, Yuchen and Sadigh, Dorsa},
  date = {2023-06-29},
  url = {https://arxiv.org/abs/2306.17237v2},
  urldate = {2024-09-27},
  abstract = {Imitation Learning (IL) is a sample efficient paradigm for robot learning using expert demonstrations. However, policies learned through IL suffer from state distribution shift at test time, due to compounding errors in action prediction which lead to previously unseen states. Choosing an action representation for the policy that minimizes this distribution shift is critical in imitation learning. Prior work propose using temporal action abstractions to reduce compounding errors, but they often sacrifice policy dexterity or require domain-specific knowledge. To address these trade-offs, we introduce HYDRA, a method that leverages a hybrid action space with two levels of action abstractions: sparse high-level waypoints and dense low-level actions. HYDRA dynamically switches between action abstractions at test time to enable both coarse and fine-grained control of a robot. In addition, HYDRA employs action relabeling to increase the consistency of actions in the dataset, further reducing distribution shift. HYDRA outperforms prior imitation learning methods by 30-40\% on seven challenging simulation and real world environments, involving long-horizon tasks in the real world like making coffee and toasting bread. Videos are found on our website: https://tinyurl.com/3mc6793z},
  langid = {english},
  organization = {arXiv.org},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/3NEWRFB8/Belkhale et al. - 2023 - HYDRA Hybrid Robot Actions for Imitation Learning.pdf}
}

@online{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  eprint = {2010.11929},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.11929},
  url = {http://arxiv.org/abs/2010.11929},
  urldate = {2024-09-08},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/VGD5R35D/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/3ICKKPZU/2010.html}
}

@online{droletComparisonImitationLearning2024,
  title = {A {{Comparison}} of {{Imitation Learning Algorithms}} for {{Bimanual Manipulation}}},
  author = {Drolet, Michael and Stepputtis, Simon and Kailas, Siva and Jain, Ajinkya and Peters, Jan and Schaal, Stefan and Amor, Heni Ben},
  date = {2024-08-24},
  eprint = {2408.06536},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2408.06536},
  url = {http://arxiv.org/abs/2408.06536},
  urldate = {2024-09-26},
  abstract = {Amidst the wide popularity of imitation learning algorithms in robotics, their properties regarding hyperparameter sensitivity, ease of training, data efficiency, and performance have not been well-studied in high-precision industry-inspired environments. In this work, we demonstrate the limitations and benefits of prominent imitation learning approaches and analyze their capabilities regarding these properties. We evaluate each algorithm on a complex bimanual manipulation task involving an over-constrained dynamics system in a setting involving multiple contacts between the manipulated object and the environment. While we find that imitation learning is well suited to solve such complex tasks, not all algorithms are equal in terms of handling environmental and hyperparameter perturbations, training requirements, performance, and ease of use. We investigate the empirical influence of these key characteristics by employing a carefully designed experimental procedure and learning environment. Paper website: https://bimanual-imitation.github.io/},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/VZSGGXCK/Drolet et al. - 2024 - A Comparison of Imitation Learning Algorithms for Bimanual Manipulation.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/Z9HWRJQ2/2408.html}
}

@online{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014-06-10},
  eprint = {1406.2661},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1406.2661},
  url = {http://arxiv.org/abs/1406.2661},
  urldate = {2024-09-13},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/5B2KKTAJ/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/S2PGWMDR/1406.html}
}

@article{hanoverAutonomousDroneRacing2024,
  title = {Autonomous {{Drone Racing}}: {{A Survey}}},
  shorttitle = {Autonomous {{Drone Racing}}},
  author = {Hanover, Drew and Loquercio, Antonio and Bauersfeld, Leonard and Romero, Angel and Penicka, Robert and Song, Yunlong and Cioffi, Giovanni and Kaufmann, Elia and Scaramuzza, Davide},
  date = {2024},
  journaltitle = {IEEE Transactions on Robotics},
  shortjournal = {IEEE Trans. Robot.},
  volume = {40},
  eprint = {2301.01755},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {3044--3067},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2024.3400838},
  url = {http://arxiv.org/abs/2301.01755},
  urldate = {2024-10-05},
  abstract = {Over the last decade, the use of autonomous drone systems for surveying, search and rescue, or last-mile delivery has increased exponentially. With the rise of these applications comes the need for highly robust, safety-critical algorithms which can operate drones in complex and uncertain environments. Additionally, flying fast enables drones to cover more ground which in turn increases productivity and further strengthens their use case. One proxy for developing algorithms used in high-speed navigation is the task of autonomous drone racing, where researchers program drones to fly through a sequence of gates and avoid obstacles as quickly as possible using onboard sensors and limited computational power. Speeds and accelerations exceed over 80 kph and 4 g respectively, raising significant challenges across perception, planning, control, and state estimation. To achieve maximum performance, systems require real-time algorithms that are robust to motion blur, high dynamic range, model uncertainties, aerodynamic disturbances, and often unpredictable opponents. This survey covers the progression of autonomous drone racing across model-based and learning-based approaches. We provide an overview of the field, its evolution over the years, and conclude with the biggest challenges and open questions to be faced in the future.},
  keywords = {Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/FP2IIEZB/Hanover et al. - 2024 - Autonomous Drone Racing A Survey.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/5ZUR2EUY/2301.html}
}

@online{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  date = {2020-12-16},
  eprint = {2006.11239},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2006.11239},
  url = {http://arxiv.org/abs/2006.11239},
  urldate = {2024-09-13},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/PEEAJNY4/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/8UXN7TUV/2006.html}
}

@online{holladayPlanningMultistageForceful2021,
  title = {Planning for {{Multi-stage Forceful Manipulation}}},
  author = {Holladay, Rachel and Lozano-Pérez, Tomás and Rodriguez, Alberto},
  date = {2021-01-07},
  url = {https://arxiv.org/abs/2101.02679v2},
  urldate = {2024-10-06},
  abstract = {Multi-stage forceful manipulation tasks, such as twisting a nut on a bolt, require reasoning over interlocking constraints over discrete as well as continuous choices. The robot must choose a sequence of discrete actions, or strategy, such as whether to pick up an object, and the continuous parameters of each of those actions, such as how to grasp the object. In forceful manipulation tasks, the force requirements substantially impact the choices of both strategy and parameters. To enable planning and executing forceful manipulation, we augment an existing task and motion planner with controllers that exert wrenches and constraints that explicitly consider torque and frictional limits. In two domains, opening a childproof bottle and twisting a nut, we demonstrate how the system considers a combinatorial number of strategies and how choosing actions that are robust to parameter variations impacts the choice of strategy.},
  langid = {english},
  organization = {arXiv.org},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/UNDUGYQP/Holladay et al. - 2021 - Planning for Multi-stage Forceful Manipulation.pdf}
}

@online{kingmaAutoEncodingVariationalBayes2022,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2022-12-10},
  eprint = {1312.6114},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1312.6114},
  url = {http://arxiv.org/abs/1312.6114},
  urldate = {2024-09-13},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/GI8KSC2Z/Kingma and Welling - 2022 - Auto-Encoding Variational Bayes.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/QPD4623E/1312.html}
}

@online{kumarPracticeMakesPerfect2024,
  title = {Practice {{Makes Perfect}}: {{Planning}} to {{Learn Skill Parameter Policies}}},
  shorttitle = {Practice {{Makes Perfect}}},
  author = {Kumar, Nishanth and Silver, Tom and McClinton, Willie and Zhao, Linfeng and Proulx, Stephen and Lozano-Pérez, Tomás and Kaelbling, Leslie Pack and Barry, Jennifer},
  date = {2024-02-22},
  url = {https://arxiv.org/abs/2402.15025v2},
  urldate = {2024-09-25},
  abstract = {One promising approach towards effective robot decision making in complex, long-horizon tasks is to sequence together parameterized skills. We consider a setting where a robot is initially equipped with (1) a library of parameterized skills, (2) an AI planner for sequencing together the skills given a goal, and (3) a very general prior distribution for selecting skill parameters. Once deployed, the robot should rapidly and autonomously learn to improve its performance by specializing its skill parameter selection policy to the particular objects, goals, and constraints in its environment. In this work, we focus on the active learning problem of choosing which skills to practice to maximize expected future task success. We propose that the robot should estimate the competence of each skill, extrapolate the competence (asking: "how much would the competence improve through practice?"), and situate the skill in the task distribution through competence-aware planning. This approach is implemented within a fully autonomous system where the robot repeatedly plans, practices, and learns without any environment resets. Through experiments in simulation, we find that our approach learns effective parameter policies more sample-efficiently than several baselines. Experiments in the real-world demonstrate our approach's ability to handle noise from perception and control and improve the robot's ability to solve two long-horizon mobile-manipulation tasks after a few hours of autonomous practice. Project website: http://ees.csail.mit.edu},
  langid = {english},
  organization = {arXiv.org},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/DCQRAFWD/Kumar et al. - 2024 - Practice Makes Perfect Planning to Learn Skill Parameter Policies.pdf}
}

@online{liuSingleGoalAll2024,
  title = {A {{Single Goal}} Is {{All You Need}}: {{Skills}} and {{Exploration Emerge}} from {{Contrastive RL}} without {{Rewards}}, {{Demonstrations}}, or {{Subgoals}}},
  shorttitle = {A {{Single Goal}} Is {{All You Need}}},
  author = {Liu, Grace and Tang, Michael and Eysenbach, Benjamin},
  date = {2024-08-11},
  eprint = {2408.05804},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2408.05804},
  url = {http://arxiv.org/abs/2408.05804},
  urldate = {2024-10-07},
  abstract = {In this paper, we present empirical evidence of skills and directed exploration emerging from a simple RL algorithm long before any successful trials are observed. For example, in a manipulation task, the agent is given a single observation of the goal state and learns skills, first for moving its end-effector, then for pushing the block, and finally for picking up and placing the block. These skills emerge before the agent has ever successfully placed the block at the goal location and without the aid of any reward functions, demonstrations, or manually-specified distance metrics. Once the agent has learned to reach the goal state reliably, exploration is reduced. Implementing our method involves a simple modification of prior work and does not require density estimates, ensembles, or any additional hyperparameters. Intuitively, the proposed method seems like it should be terrible at exploration, and we lack a clear theoretical understanding of why it works so effectively, though our experiments provide some hints.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/EGHJP2CV/Liu et al. - 2024 - A Single Goal is All You Need Skills and Exploration Emerge from Contrastive RL without Rewards, De.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/7A8VTL3V/2408.html}
}

@inproceedings{sohnLearningStructuredOutput2015,
  title = {Learning {{Structured Output Representation}} Using {{Deep Conditional Generative Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
  date = {2015},
  volume = {28},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2015/hash/8d55a249e6baa5c06772297520da2051-Abstract.html},
  urldate = {2024-09-13},
  abstract = {Supervised deep learning has been successfully applied for many recognition problems in machine learning and computer vision. Although it can approximate a complex many-to-one function very well when large number of training data is provided, the lack of probabilistic inference of the current supervised deep learning methods makes it difficult to model a complex structured output representations. In this work, we develop a scalable deep conditional generative model for structured output variables using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient variational Bayes, and allows a fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build a robust structured prediction algorithms, such as recurrent prediction network architecture, input noise-injection and multi-scale prediction training methods. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic output representations using stochastic inference. Furthermore, the proposed schemes in training methods and architecture design were complimentary, which leads to achieve strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/A2GM32KN/Sohn et al. - 2015 - Learning Structured Output Representation using Deep Conditional Generative Models.pdf}
}

@online{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-01},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2024-08-28},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/X43IG47J/Vaswani et al. - 2023 - Attention Is All You Need.pdf}
}
