@online{alakuijalaLearningRewardFunctions2023,
  title = {Learning {{Reward Functions}} for {{Robotic Manipulation}} by {{Observing Humans}}},
  author = {Alakuijala, Minttu and Dulac-Arnold, Gabriel and Mairal, Julien and Ponce, Jean and Schmid, Cordelia},
  date = {2023-03-07},
  eprint = {2211.09019},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2211.09019},
  url = {http://arxiv.org/abs/2211.09019},
  urldate = {2024-10-14},
  abstract = {Observing a human demonstrator manipulate objects provides a rich, scalable and inexpensive source of data for learning robotic policies. However, transferring skills from human videos to a robotic manipulator poses several challenges, not least a difference in action and observation spaces. In this work, we use unlabeled videos of humans solving a wide range of manipulation tasks to learn a task-agnostic reward function for robotic manipulation policies. Thanks to the diversity of this training data, the learned reward function sufficiently generalizes to image observations from a previously unseen robot embodiment and environment to provide a meaningful prior for directed exploration in reinforcement learning. We propose two methods for scoring states relative to a goal image: through direct temporal regression, and through distances in an embedding space obtained with time-contrastive learning. By conditioning the function on a goal image, we are able to reuse one model across a variety of tasks. Unlike prior work on leveraging human videos to teach robots, our method, Human Offline Learned Distances (HOLD) requires neither a priori data from the robot environment, nor a set of task-specific human demonstrations, nor a predefined notion of correspondence across morphologies, yet it is able to accelerate training of several manipulation tasks on a simulated robot arm compared to using only a sparse reward obtained from task completion.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/MNVD9F69/Alakuijala et al. - 2023 - Learning Reward Functions for Robotic Manipulation by Observing Humans.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/4TKP6TW5/2211.html}
}

@online{bajcsyLearningVisionbasedPursuitEvasion2023,
  title = {Learning {{Vision-based Pursuit-Evasion Robot Policies}}},
  author = {Bajcsy, Andrea and Loquercio, Antonio and Kumar, Ashish and Malik, Jitendra},
  date = {2023-08-30},
  eprint = {2308.16185},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.16185},
  url = {http://arxiv.org/abs/2308.16185},
  urldate = {2024-10-06},
  abstract = {Learning strategic robot behavior -- like that required in pursuit-evasion interactions -- under real-world constraints is extremely challenging. It requires exploiting the dynamics of the interaction, and planning through both physical state and latent intent uncertainty. In this paper, we transform this intractable problem into a supervised learning problem, where a fully-observable robot policy generates supervision for a partially-observable one. We find that the quality of the supervision signal for the partially-observable pursuer policy depends on two key factors: the balance of diversity and optimality of the evader's behavior and the strength of the modeling assumptions in the fully-observable policy. We deploy our policy on a physical quadruped robot with an RGB-D camera on pursuit-evasion interactions in the wild. Despite all the challenges, the sensing constraints bring about creativity: the robot is pushed to gather information when uncertain, predict intent from noisy measurements, and anticipate in order to intercept. Project webpage: https://abajcsy.github.io/vision-based-pursuit/},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/7GZK3ZCL/Bajcsy et al. - 2023 - Learning Vision-based Pursuit-Evasion Robot Policies.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/W4BANNFN/2308.html}
}

@online{belkhaleDataQualityImitation2023,
  title = {Data {{Quality}} in {{Imitation Learning}}},
  author = {Belkhale, Suneel and Cui, Yuchen and Sadigh, Dorsa},
  date = {2023-06-04},
  url = {https://arxiv.org/abs/2306.02437v1},
  urldate = {2024-09-27},
  abstract = {In supervised learning, the question of data quality and curation has been over-shadowed in recent years by increasingly more powerful and expressive models that can ingest internet-scale data. However, in offline learning for robotics, we simply lack internet scale data, and so high quality datasets are a necessity. This is especially true in imitation learning (IL), a sample efficient paradigm for robot learning using expert demonstrations. Policies learned through IL suffer from state distribution shift at test time due to compounding errors in action prediction, which leads to unseen states that the policy cannot recover from. Instead of designing new algorithms to address distribution shift, an alternative perspective is to develop new ways of assessing and curating datasets. There is growing evidence that the same IL algorithms can have substantially different performance across different datasets. This calls for a formalism for defining metrics of "data quality" that can further be leveraged for data curation. In this work, we take the first step toward formalizing data quality for imitation learning through the lens of distribution shift: a high quality dataset encourages the policy to stay in distribution at test time. We propose two fundamental properties that shape the quality of a dataset: i) action divergence: the mismatch between the expert and learned policy at certain states; and ii) transition diversity: the noise present in the system for a given state and action. We investigate the combined effect of these two key properties in imitation learning theoretically, and we empirically analyze models trained on a variety of different data sources. We show that state diversity is not always beneficial, and we demonstrate how action divergence and transition diversity interact in practice.},
  langid = {english},
  organization = {arXiv.org},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/GCQHCPJ8/Belkhale et al. - 2023 - Data Quality in Imitation Learning.pdf}
}

@online{belkhaleHYDRAHybridRobot2023,
  title = {{{HYDRA}}: {{Hybrid Robot Actions}} for {{Imitation Learning}}},
  shorttitle = {{{HYDRA}}},
  author = {Belkhale, Suneel and Cui, Yuchen and Sadigh, Dorsa},
  date = {2023-06-29},
  url = {https://arxiv.org/abs/2306.17237v2},
  urldate = {2024-09-27},
  abstract = {Imitation Learning (IL) is a sample efficient paradigm for robot learning using expert demonstrations. However, policies learned through IL suffer from state distribution shift at test time, due to compounding errors in action prediction which lead to previously unseen states. Choosing an action representation for the policy that minimizes this distribution shift is critical in imitation learning. Prior work propose using temporal action abstractions to reduce compounding errors, but they often sacrifice policy dexterity or require domain-specific knowledge. To address these trade-offs, we introduce HYDRA, a method that leverages a hybrid action space with two levels of action abstractions: sparse high-level waypoints and dense low-level actions. HYDRA dynamically switches between action abstractions at test time to enable both coarse and fine-grained control of a robot. In addition, HYDRA employs action relabeling to increase the consistency of actions in the dataset, further reducing distribution shift. HYDRA outperforms prior imitation learning methods by 30-40\% on seven challenging simulation and real world environments, involving long-horizon tasks in the real world like making coffee and toasting bread. Videos are found on our website: https://tinyurl.com/3mc6793z},
  langid = {english},
  organization = {arXiv.org},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/3NEWRFB8/Belkhale et al. - 2023 - HYDRA Hybrid Robot Actions for Imitation Learning.pdf}
}

@online{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  eprint = {2010.11929},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.11929},
  url = {http://arxiv.org/abs/2010.11929},
  urldate = {2024-09-08},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/VGD5R35D/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/3ICKKPZU/2010.html}
}

@online{douTactileAugmentedRadianceFields2024,
  title = {Tactile-{{Augmented Radiance Fields}}},
  author = {Dou, Yiming and Yang, Fengyu and Liu, Yi and Loquercio, Antonio and Owens, Andrew},
  date = {2024-05-07},
  eprint = {2405.04534},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2405.04534},
  url = {http://arxiv.org/abs/2405.04534},
  urldate = {2024-10-12},
  abstract = {We present a scene representation, which we call a tactile-augmented radiance field (TaRF), that brings vision and touch into a shared 3D space. This representation can be used to estimate the visual and tactile signals for a given 3D position within a scene. We capture a scene's TaRF from a collection of photos and sparsely sampled touch probes. Our approach makes use of two insights: (i) common vision-based touch sensors are built on ordinary cameras and thus can be registered to images using methods from multi-view geometry, and (ii) visually and structurally similar regions of a scene share the same tactile features. We use these insights to register touch signals to a captured visual scene, and to train a conditional diffusion model that, provided with an RGB-D image rendered from a neural radiance field, generates its corresponding tactile signal. To evaluate our approach, we collect a dataset of TaRFs. This dataset contains more touch samples than previous real-world datasets, and it provides spatially aligned visual signals for each captured touch signal. We demonstrate the accuracy of our cross-modal generative model and the utility of the captured visual-tactile data on several downstream tasks. Project page: https://dou-yiming.github.io/TaRF},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/T5H7VHTX/Dou et al. - 2024 - Tactile-Augmented Radiance Fields.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/MUAPQGJJ/2405.html}
}

@online{droletComparisonImitationLearning2024,
  title = {A {{Comparison}} of {{Imitation Learning Algorithms}} for {{Bimanual Manipulation}}},
  author = {Drolet, Michael and Stepputtis, Simon and Kailas, Siva and Jain, Ajinkya and Peters, Jan and Schaal, Stefan and Amor, Heni Ben},
  date = {2024-08-24},
  eprint = {2408.06536},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2408.06536},
  url = {http://arxiv.org/abs/2408.06536},
  urldate = {2024-09-26},
  abstract = {Amidst the wide popularity of imitation learning algorithms in robotics, their properties regarding hyperparameter sensitivity, ease of training, data efficiency, and performance have not been well-studied in high-precision industry-inspired environments. In this work, we demonstrate the limitations and benefits of prominent imitation learning approaches and analyze their capabilities regarding these properties. We evaluate each algorithm on a complex bimanual manipulation task involving an over-constrained dynamics system in a setting involving multiple contacts between the manipulated object and the environment. While we find that imitation learning is well suited to solve such complex tasks, not all algorithms are equal in terms of handling environmental and hyperparameter perturbations, training requirements, performance, and ease of use. We investigate the empirical influence of these key characteristics by employing a carefully designed experimental procedure and learning environment. Paper website: https://bimanual-imitation.github.io/},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/VZSGGXCK/Drolet et al. - 2024 - A Comparison of Imitation Learning Algorithms for Bimanual Manipulation.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/Z9HWRJQ2/2408.html}
}

@online{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014-06-10},
  eprint = {1406.2661},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1406.2661},
  url = {http://arxiv.org/abs/1406.2661},
  urldate = {2024-09-13},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/5B2KKTAJ/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/S2PGWMDR/1406.html}
}

@article{hanoverAutonomousDroneRacing2024,
  title = {Autonomous {{Drone Racing}}: {{A Survey}}},
  shorttitle = {Autonomous {{Drone Racing}}},
  author = {Hanover, Drew and Loquercio, Antonio and Bauersfeld, Leonard and Romero, Angel and Penicka, Robert and Song, Yunlong and Cioffi, Giovanni and Kaufmann, Elia and Scaramuzza, Davide},
  date = {2024},
  journaltitle = {IEEE Transactions on Robotics},
  shortjournal = {IEEE Trans. Robot.},
  volume = {40},
  eprint = {2301.01755},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {3044--3067},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2024.3400838},
  url = {http://arxiv.org/abs/2301.01755},
  urldate = {2024-10-05},
  abstract = {Over the last decade, the use of autonomous drone systems for surveying, search and rescue, or last-mile delivery has increased exponentially. With the rise of these applications comes the need for highly robust, safety-critical algorithms which can operate drones in complex and uncertain environments. Additionally, flying fast enables drones to cover more ground which in turn increases productivity and further strengthens their use case. One proxy for developing algorithms used in high-speed navigation is the task of autonomous drone racing, where researchers program drones to fly through a sequence of gates and avoid obstacles as quickly as possible using onboard sensors and limited computational power. Speeds and accelerations exceed over 80 kph and 4 g respectively, raising significant challenges across perception, planning, control, and state estimation. To achieve maximum performance, systems require real-time algorithms that are robust to motion blur, high dynamic range, model uncertainties, aerodynamic disturbances, and often unpredictable opponents. This survey covers the progression of autonomous drone racing across model-based and learning-based approaches. We provide an overview of the field, its evolution over the years, and conclude with the biggest challenges and open questions to be faced in the future.},
  keywords = {Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/FP2IIEZB/Hanover et al. - 2024 - Autonomous Drone Racing A Survey.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/5ZUR2EUY/2301.html}
}

@online{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  date = {2020-12-16},
  eprint = {2006.11239},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2006.11239},
  url = {http://arxiv.org/abs/2006.11239},
  urldate = {2024-09-13},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/PEEAJNY4/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/8UXN7TUV/2006.html}
}

@online{holladayPlanningMultistageForceful2021,
  title = {Planning for {{Multi-stage Forceful Manipulation}}},
  author = {Holladay, Rachel and Lozano-Pérez, Tomás and Rodriguez, Alberto},
  date = {2021-01-07},
  url = {https://arxiv.org/abs/2101.02679v2},
  urldate = {2024-10-06},
  abstract = {Multi-stage forceful manipulation tasks, such as twisting a nut on a bolt, require reasoning over interlocking constraints over discrete as well as continuous choices. The robot must choose a sequence of discrete actions, or strategy, such as whether to pick up an object, and the continuous parameters of each of those actions, such as how to grasp the object. In forceful manipulation tasks, the force requirements substantially impact the choices of both strategy and parameters. To enable planning and executing forceful manipulation, we augment an existing task and motion planner with controllers that exert wrenches and constraints that explicitly consider torque and frictional limits. In two domains, opening a childproof bottle and twisting a nut, we demonstrate how the system considers a combinatorial number of strategies and how choosing actions that are robust to parameter variations impacts the choice of strategy.},
  langid = {english},
  organization = {arXiv.org},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/UNDUGYQP/Holladay et al. - 2021 - Planning for Multi-stage Forceful Manipulation.pdf}
}

@online{huangDiffusionRewardLearning2024,
  title = {Diffusion {{Reward}}: {{Learning Rewards}} via {{Conditional Video Diffusion}}},
  shorttitle = {Diffusion {{Reward}}},
  author = {Huang, Tao and Jiang, Guangqi and Ze, Yanjie and Xu, Huazhe},
  date = {2024-08-09},
  eprint = {2312.14134},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2312.14134},
  url = {http://arxiv.org/abs/2312.14134},
  urldate = {2024-10-14},
  abstract = {Learning rewards from expert videos offers an affordable and effective solution to specify the intended behaviors for reinforcement learning (RL) tasks. In this work, we propose Diffusion Reward, a novel framework that learns rewards from expert videos via conditional video diffusion models for solving complex visual RL problems. Our key insight is that lower generative diversity is exhibited when conditioning diffusion on expert trajectories. Diffusion Reward is accordingly formalized by the negative of conditional entropy that encourages productive exploration of expert behaviors. We show the efficacy of our method over robotic manipulation tasks in both simulation platforms and the real world with visual input. Moreover, Diffusion Reward can even solve unseen tasks successfully and effectively, largely surpassing baseline methods. Project page and code: https://diffusion-reward.github.io.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/G87XU3QY/Huang et al. - 2024 - Diffusion Reward Learning Rewards via Conditional Video Diffusion.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/C4AUPHVS/2312.html}
}

@online{kingmaAutoEncodingVariationalBayes2022,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2022-12-10},
  eprint = {1312.6114},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1312.6114},
  url = {http://arxiv.org/abs/1312.6114},
  urldate = {2024-09-13},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/GI8KSC2Z/Kingma and Welling - 2022 - Auto-Encoding Variational Bayes.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/QPD4623E/1312.html}
}

@online{kumarPracticeMakesPerfect2024,
  title = {Practice {{Makes Perfect}}: {{Planning}} to {{Learn Skill Parameter Policies}}},
  shorttitle = {Practice {{Makes Perfect}}},
  author = {Kumar, Nishanth and Silver, Tom and McClinton, Willie and Zhao, Linfeng and Proulx, Stephen and Lozano-Pérez, Tomás and Kaelbling, Leslie Pack and Barry, Jennifer},
  date = {2024-02-22},
  url = {https://arxiv.org/abs/2402.15025v2},
  urldate = {2024-09-25},
  abstract = {One promising approach towards effective robot decision making in complex, long-horizon tasks is to sequence together parameterized skills. We consider a setting where a robot is initially equipped with (1) a library of parameterized skills, (2) an AI planner for sequencing together the skills given a goal, and (3) a very general prior distribution for selecting skill parameters. Once deployed, the robot should rapidly and autonomously learn to improve its performance by specializing its skill parameter selection policy to the particular objects, goals, and constraints in its environment. In this work, we focus on the active learning problem of choosing which skills to practice to maximize expected future task success. We propose that the robot should estimate the competence of each skill, extrapolate the competence (asking: "how much would the competence improve through practice?"), and situate the skill in the task distribution through competence-aware planning. This approach is implemented within a fully autonomous system where the robot repeatedly plans, practices, and learns without any environment resets. Through experiments in simulation, we find that our approach learns effective parameter policies more sample-efficiently than several baselines. Experiments in the real-world demonstrate our approach's ability to handle noise from perception and control and improve the robot's ability to solve two long-horizon mobile-manipulation tasks after a few hours of autonomous practice. Project website: http://ees.csail.mit.edu},
  langid = {english},
  organization = {arXiv.org},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/DCQRAFWD/Kumar et al. - 2024 - Practice Makes Perfect Planning to Learn Skill Parameter Policies.pdf}
}

@online{liuSingleGoalAll2024,
  title = {A {{Single Goal}} Is {{All You Need}}: {{Skills}} and {{Exploration Emerge}} from {{Contrastive RL}} without {{Rewards}}, {{Demonstrations}}, or {{Subgoals}}},
  shorttitle = {A {{Single Goal}} Is {{All You Need}}},
  author = {Liu, Grace and Tang, Michael and Eysenbach, Benjamin},
  date = {2024-08-11},
  eprint = {2408.05804},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2408.05804},
  url = {http://arxiv.org/abs/2408.05804},
  urldate = {2024-10-07},
  abstract = {In this paper, we present empirical evidence of skills and directed exploration emerging from a simple RL algorithm long before any successful trials are observed. For example, in a manipulation task, the agent is given a single observation of the goal state and learns skills, first for moving its end-effector, then for pushing the block, and finally for picking up and placing the block. These skills emerge before the agent has ever successfully placed the block at the goal location and without the aid of any reward functions, demonstrations, or manually-specified distance metrics. Once the agent has learned to reach the goal state reliably, exploration is reduced. Implementing our method involves a simple modification of prior work and does not require density estimates, ensembles, or any additional hyperparameters. Intuitively, the proposed method seems like it should be terrible at exploration, and we lack a clear theoretical understanding of why it works so effectively, though our experiments provide some hints.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/EGHJP2CV/Liu et al. - 2024 - A Single Goal is All You Need Skills and Exploration Emerge from Contrastive RL without Rewards, De.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/7A8VTL3V/2408.html}
}

@online{maVIPUniversalVisual2023,
  title = {{{VIP}}: {{Towards Universal Visual Reward}} and {{Representation}} via {{Value-Implicit Pre-Training}}},
  shorttitle = {{{VIP}}},
  author = {Ma, Yecheng Jason and Sodhani, Shagun and Jayaraman, Dinesh and Bastani, Osbert and Kumar, Vikash and Zhang, Amy},
  date = {2023-03-07},
  eprint = {2210.00030},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2210.00030},
  url = {http://arxiv.org/abs/2210.00030},
  urldate = {2024-10-14},
  abstract = {Reward and representation learning are two long-standing challenges for learning an expanding set of robot manipulation skills from sensory observations. Given the inherent cost and scarcity of in-domain, task-specific robot data, learning from large, diverse, offline human videos has emerged as a promising path towards acquiring a generally useful visual representation for control; however, how these human videos can be used for general-purpose reward learning remains an open question. We introduce \$\textbackslash textbf\{V\}\$alue-\$\textbackslash textbf\{I\}\$mplicit \$\textbackslash textbf\{P\}\$re-training (VIP), a self-supervised pre-trained visual representation capable of generating dense and smooth reward functions for unseen robotic tasks. VIP casts representation learning from human videos as an offline goal-conditioned reinforcement learning problem and derives a self-supervised dual goal-conditioned value-function objective that does not depend on actions, enabling pre-training on unlabeled human videos. Theoretically, VIP can be understood as a novel implicit time contrastive objective that generates a temporally smooth embedding, enabling the value function to be implicitly defined via the embedding distance, which can then be used to construct the reward for any goal-image specified downstream task. Trained on large-scale Ego4D human videos and without any fine-tuning on in-domain, task-specific data, VIP's frozen representation can provide dense visual reward for an extensive set of simulated and \$\textbackslash textbf\{real-robot\}\$ tasks, enabling diverse reward-based visual control methods and significantly outperforming all prior pre-trained representations. Notably, VIP can enable simple, \$\textbackslash textbf\{few-shot\}\$ offline RL on a suite of real-world robot tasks with as few as 20 trajectories.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/ME67B54J/Ma et al. - 2023 - VIP Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/FH9EQD8B/2210.html}
}

@online{singhHandObjectInteractionPretraining2024,
  title = {Hand-{{Object Interaction Pretraining}} from {{Videos}}},
  author = {Singh, Himanshu Gaurav and Loquercio, Antonio and Sferrazza, Carmelo and Wu, Jane and Qi, Haozhi and Abbeel, Pieter and Malik, Jitendra},
  date = {2024-09-12},
  eprint = {2409.08273},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2409.08273},
  url = {http://arxiv.org/abs/2409.08273},
  urldate = {2024-10-12},
  abstract = {We present an approach to learn general robot manipulation priors from 3D hand-object interaction trajectories. We build a framework to use in-the-wild videos to generate sensorimotor robot trajectories. We do so by lifting both the human hand and the manipulated object in a shared 3D space and retargeting human motions to robot actions. Generative modeling on this data gives us a task-agnostic base policy. This policy captures a general yet flexible manipulation prior. We empirically demonstrate that finetuning this policy, with both reinforcement learning (RL) and behavior cloning (BC), enables sample-efficient adaptation to downstream tasks and simultaneously improves robustness and generalizability compared to prior approaches. Qualitative experiments are available at: \textbackslash url\{https://hgaurav2k.github.io/hop/\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/S68QE7NX/Singh et al. - 2024 - Hand-Object Interaction Pretraining from Videos.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/MX65L2IN/2409.html}
}

@inproceedings{sohnLearningStructuredOutput2015,
  title = {Learning {{Structured Output Representation}} Using {{Deep Conditional Generative Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
  date = {2015},
  volume = {28},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2015/hash/8d55a249e6baa5c06772297520da2051-Abstract.html},
  urldate = {2024-09-13},
  abstract = {Supervised deep learning has been successfully applied for many recognition problems in machine learning and computer vision. Although it can approximate a complex many-to-one function very well when large number of training data is provided, the lack of probabilistic inference of the current supervised deep learning methods makes it difficult to model a complex structured output representations. In this work, we develop a scalable deep conditional generative model for structured output variables using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient variational Bayes, and allows a fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build a robust structured prediction algorithms, such as recurrent prediction network architecture, input noise-injection and multi-scale prediction training methods. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic output representations using stochastic inference. Furthermore, the proposed schemes in training methods and architecture design were complimentary, which leads to achieve strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/A2GM32KN/Sohn et al. - 2015 - Learning Structured Output Representation using Deep Conditional Generative Models.pdf}
}

@online{teamOctoOpenSourceGeneralist2024,
  title = {Octo: {{An Open-Source Generalist Robot Policy}}},
  shorttitle = {Octo},
  author = {Team, Octo Model and Ghosh, Dibya and Walke, Homer and Pertsch, Karl and Black, Kevin and Mees, Oier and Dasari, Sudeep and Hejna, Joey and Kreiman, Tobias and Xu, Charles and Luo, Jianlan and Tan, You Liang and Chen, Lawrence Yunliang and Sanketi, Pannag and Vuong, Quan and Xiao, Ted and Sadigh, Dorsa and Finn, Chelsea and Levine, Sergey},
  date = {2024-05-26},
  eprint = {2405.12213},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2405.12213},
  url = {http://arxiv.org/abs/2405.12213},
  urldate = {2024-10-14},
  abstract = {Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/P2CKSU9E/Team et al. - 2024 - Octo An Open-Source Generalist Robot Policy.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/L5TBXLU5/2405.html}
}

@online{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-01},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2024-08-28},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/X43IG47J/Vaswani et al. - 2023 - Attention Is All You Need.pdf}
}

@online{yangRank2RewardLearningShaped2024,
  title = {{{Rank2Reward}}: {{Learning Shaped Reward Functions}} from {{Passive Video}}},
  shorttitle = {{{Rank2Reward}}},
  author = {Yang, Daniel and Tjia, Davin and Berg, Jacob and Damen, Dima and Agrawal, Pulkit and Gupta, Abhishek},
  date = {2024-04-23},
  eprint = {2404.14735},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2404.14735},
  url = {http://arxiv.org/abs/2404.14735},
  urldate = {2024-10-14},
  abstract = {Teaching robots novel skills with demonstrations via human-in-the-loop data collection techniques like kinesthetic teaching or teleoperation puts a heavy burden on human supervisors. In contrast to this paradigm, it is often significantly easier to provide raw, action-free visual data of tasks being performed. Moreover, this data can even be mined from video datasets or the web. Ideally, this data can serve to guide robot learning for new tasks in novel environments, informing both "what" to do and "how" to do it. A powerful way to encode both the "what" and the "how" is to infer a well-shaped reward function for reinforcement learning. The challenge is determining how to ground visual demonstration inputs into a well-shaped and informative reward function. We propose a technique Rank2Reward for learning behaviors from videos of tasks being performed without access to any low-level states and actions. We do so by leveraging the videos to learn a reward function that measures incremental "progress" through a task by learning how to temporally rank the video frames in a demonstration. By inferring an appropriate ranking, the reward function is able to guide reinforcement learning by indicating when task progress is being made. This ranking function can be integrated into an adversarial imitation learning scheme resulting in an algorithm that can learn behaviors without exploiting the learned reward function. We demonstrate the effectiveness of Rank2Reward at learning behaviors from raw video on a number of tabletop manipulation tasks in both simulations and on a real-world robotic arm. We also demonstrate how Rank2Reward can be easily extended to be applicable to web-scale video datasets.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/RQQQQWFI/Yang et al. - 2024 - Rank2Reward Learning Shaped Reward Functions from Passive Video.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/EPBSPV6H/2404.html}
}

@online{yaratsMasteringVisualContinuous2021,
  title = {Mastering {{Visual Continuous Control}}: {{Improved Data-Augmented Reinforcement Learning}}},
  shorttitle = {Mastering {{Visual Continuous Control}}},
  author = {Yarats, Denis and Fergus, Rob and Lazaric, Alessandro and Pinto, Lerrel},
  date = {2021-07-20},
  eprint = {2107.09645},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2107.09645},
  url = {http://arxiv.org/abs/2107.09645},
  urldate = {2024-10-14},
  abstract = {We present DrQ-v2, a model-free reinforcement learning (RL) algorithm for visual continuous control. DrQ-v2 builds on DrQ, an off-policy actor-critic approach that uses data augmentation to learn directly from pixels. We introduce several improvements that yield state-of-the-art results on the DeepMind Control Suite. Notably, DrQ-v2 is able to solve complex humanoid locomotion tasks directly from pixel observations, previously unattained by model-free RL. DrQ-v2 is conceptually simple, easy to implement, and provides significantly better computational footprint compared to prior work, with the majority of tasks taking just 8 hours to train on a single GPU. Finally, we publicly release DrQ-v2's implementation to provide RL practitioners with a strong and computationally efficient baseline.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/6CMF9333/Yarats et al. - 2021 - Mastering Visual Continuous Control Improved Data-Augmented Reinforcement Learning.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/D2WYQZME/2107.html}
}

@online{zhaoLearningFineGrainedBimanual2023,
  title = {Learning {{Fine-Grained Bimanual Manipulation}} with {{Low-Cost Hardware}}},
  author = {Zhao, Tony Z. and Kumar, Vikash and Levine, Sergey and Finn, Chelsea},
  date = {2023-04-23},
  eprint = {2304.13705},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2304.13705},
  url = {http://arxiv.org/abs/2304.13705},
  urldate = {2024-10-14},
  abstract = {Fine manipulation tasks, such as threading cable ties or slotting a battery, are notoriously difficult for robots because they require precision, careful coordination of contact forces, and closed-loop visual feedback. Performing these tasks typically requires high-end robots, accurate sensors, or careful calibration, which can be expensive and difficult to set up. Can learning enable low-cost and imprecise hardware to perform these fine manipulation tasks? We present a low-cost system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface. Imitation learning, however, presents its own challenges, particularly in high-precision domains: errors in the policy can compound over time, and human demonstrations can be non-stationary. To address these challenges, we develop a simple yet novel algorithm, Action Chunking with Transformers (ACT), which learns a generative model over action sequences. ACT allows the robot to learn 6 difficult tasks in the real world, such as opening a translucent condiment cup and slotting a battery with 80-90\% success, with only 10 minutes worth of demonstrations. Project website: https://tonyzhaozh.github.io/aloha/},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/kevinywu/snap/zotero-snap/common/Zotero/storage/EDC8RM9Z/Zhao et al. - 2023 - Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware.pdf;/home/kevinywu/snap/zotero-snap/common/Zotero/storage/4FXH7N66/2304.html}
}
